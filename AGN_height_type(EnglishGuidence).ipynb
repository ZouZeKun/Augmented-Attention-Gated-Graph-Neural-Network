{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:06.703975Z",
     "iopub.status.busy": "2024-07-01T06:44:06.703523Z",
     "iopub.status.idle": "2024-07-01T06:44:10.490282Z",
     "shell.execute_reply": "2024-07-01T06:44:10.489363Z",
     "shell.execute_reply.started": "2024-07-01T06:44:06.703939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_pressure_file = './simulate_pressure.csv'\n",
    "water_head_file = './simulate_head.csv'\n",
    "\n",
    "Adj_file = './AdjacencyMatrix781.csv'\n",
    "node_heights = './node_heights.csv'\n",
    "node_types = './node_types.csv'\n",
    "\n",
    "model_file = 'GGNN_33_step12_height_type.pkl'\n",
    "log_file = 'log_step12_height_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.493093Z",
     "iopub.status.busy": "2024-07-01T06:44:10.492238Z",
     "iopub.status.idle": "2024-07-01T06:44:10.512350Z",
     "shell.execute_reply": "2024-07-01T06:44:10.511660Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.493061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='AGN_Pressure_LTown_step12 Parameters')\n",
    "parser.add_argument('--num_step'     , type=int,   default=105120,help='Total number of steps')\n",
    "parser.add_argument('--num_hp'       , type=int,   default=12,  help='history steps equal to predict steps')\n",
    "parser.add_argument('--num_vertex'   , type=int,   default=781, help='number of nodes')\n",
    "parser.add_argument('--hidden_dim'   , type=int,   default=12,  help='hidden_dim')\n",
    "\n",
    "parser.add_argument('--d'            , type=int,   default=781, help='number of nodes')\n",
    "parser.add_argument('--bn_decay'     , type=float, default=0.1, help='batch normalization decay')\n",
    "parser.add_argument('--iter_step'    , type=int,   default=3 ,  help='iter_step')\n",
    "parser.add_argument('--train_ratio'  , type=float, default=0.7, help='train set ratio')\n",
    "parser.add_argument('--val_ratio'    , type=float, default=0.2, help='validation set ratio')\n",
    "parser.add_argument('--test_ratio'   , type=float, default=0.1, help='test set ratio')\n",
    "parser.add_argument('--l1_lambda'    , type=float, default=0.01,help='L1 norm')\n",
    "parser.add_argument('--batch_size'   , type=int,   default=32,  help='batch size')\n",
    "parser.add_argument('--max_epoch'    , type=int,   default=10,  help='max epoch')\n",
    "parser.add_argument('--patience'     , type=int,   default=10,  help='early stop patience')\n",
    "parser.add_argument('--gamma'        , type=float, default=0.8, help='learning rate decay rate')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3,help='learning rate') \n",
    "parser.add_argument('--decay_epoch'  , type=int,   default=10,  help='decay epoch')\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.514133Z",
     "iopub.status.busy": "2024-07-01T06:44:10.513692Z",
     "iopub.status.idle": "2024-07-01T06:44:10.766052Z",
     "shell.execute_reply": "2024-07-01T06:44:10.765231Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.514104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read command line arguments\n",
    "def log_string(log, string):\n",
    "    log.write(string + '\\n')\n",
    "    log.flush() \n",
    "    print(string)\n",
    "\n",
    "# statistic model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.767806Z",
     "iopub.status.busy": "2024-07-01T06:44:10.767266Z",
     "iopub.status.idle": "2024-07-01T06:44:10.775094Z",
     "shell.execute_reply": "2024-07-01T06:44:10.774558Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.767774Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_mask(args, num_masked, labels):\n",
    "    ''' \n",
    "    Generate a two-step mask: \n",
    "    a one-step masking tensor for masking only sensors, \n",
    "    a two-step masking tensor for continuing to mask sensors\n",
    "\n",
    "    1. sensor only: masked_tensor1 / masking method: mask1\n",
    "    2. mask the sensor again: masked_tensor2 / masking method: mask2\n",
    "    '''\n",
    "    # '111' '226' '300' is the PRV outlet --> fixed pressure\n",
    "    fixed_nodes = ['111', '226', '300', '781']\n",
    "    sensor_name = ['1','4','31','54','105','114','163','188','215','229',\n",
    "    '288','296','330','340','408','413','427','456','467',\n",
    "    '493','504','514','517','547','611','634','642','677',\n",
    "    '720','724','738','750','767']\n",
    "\n",
    "    sensor_name.extend(fixed_nodes)\n",
    "\n",
    "    # Reduce the selection probability of areas with few sensors \n",
    "    # 1:3:29 --> 87:29:1\n",
    "    weights = [     29 if i in ['1', '4', '31'] \n",
    "               else 1 if i == '215' \n",
    "               else 87 for i in sensor_name]\n",
    "\n",
    "    # Make sure fixed_sensors have a weight of 0 to indicate that they will not be masked\n",
    "    for sensor in fixed_nodes:\n",
    "        weights[sensor_name.index(sensor)] = 0\n",
    "\n",
    "    # Converts sensor_index to an integer\n",
    "    sensor_index = [int(i)-1 for i in sensor_name]\n",
    " \n",
    "    # Create a mask with the same shape as the input tensor\n",
    "    mask1 = torch.zeros(labels.shape)\n",
    "    mask1[:,:,sensor_index] = 1\n",
    "    \n",
    "    # The index is randomly selected in sensor_index to mask\n",
    "    masked_indices = random.choices(sensor_index, weights=weights, k=num_masked)\n",
    "    \n",
    "    # Create a second mask\n",
    "    mask2 = mask1.clone()\n",
    "    mask2[:, :, masked_indices] = 0\n",
    "\n",
    "    mask1 = mask1.to(args.device)\n",
    "    mask2 = mask2.to(args.device)\n",
    "    \n",
    "    # Apply the mask to the input tensor\n",
    "    masked_tensor1 = labels * mask1\n",
    "    masked_tensor2 = labels * mask2\n",
    "    masked_tensor1 = torch.tensor(masked_tensor1, dtype=torch.float32)\n",
    "    masked_tensor2 = torch.tensor(masked_tensor2, dtype=torch.float32)\n",
    "\n",
    "    return masked_tensor1, masked_tensor2, mask1, mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.777649Z",
     "iopub.status.busy": "2024-07-01T06:44:10.777316Z",
     "iopub.status.idle": "2024-07-01T06:44:10.814006Z",
     "shell.execute_reply": "2024-07-01T06:44:10.813406Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.777623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide the time series data into multiple training samples\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, water_pressure_file, water_head_file, dataset_type='train'):\n",
    "        self.args = args\n",
    "        self.water_head_file = water_head_file\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "        # Calculate the number of time steps for the training set, verification set, and test set\n",
    "        train_steps = round(args.train_ratio * args.num_step)\n",
    "        test_steps = round(args.test_ratio * args.num_step)\n",
    "        val_steps = args.num_step - train_steps - test_steps\n",
    "\n",
    "        fixed_nodes = ['111', '226', '300', '781']\n",
    "        sensor_name = ['1','4','31','54','105','114','163','188','215','229',\n",
    "                       '288','296','330','340','408','413','427','456','467',\n",
    "                       '493','504','514','517','547','611','634','642','677',\n",
    "                       '720','724','738','750','767']\n",
    "        sensor_name.extend(fixed_nodes)\n",
    "        sensor_index = [int(i)-1 for i in sensor_name]\n",
    "        \n",
    "        # Read water head and pressure data \n",
    "        pressure = pd.read_csv(water_pressure_file, header=0, index_col=None)\n",
    "        pressure = torch.from_numpy(pressure.values)\n",
    "        std_pressure = pressure[sensor_index].std()     # Avoid information leaks\n",
    "        mean_pressure  = pressure[sensor_index].mean()  # Avoid information leaks\n",
    "        water_pressure =  (pressure - mean_pressure) / std_pressure \n",
    "\n",
    "        head = pd.read_csv(water_head_file, header=0, index_col=None)\n",
    "        head = torch.from_numpy(head.values)\n",
    "        std_head = head[sensor_index].std()   # Avoid information leaks\n",
    "        mean_head = head[sensor_index].mean() # Avoid information leaks\n",
    "        water_head = (head - mean_head) / std_head\n",
    "\n",
    "        # Split the data set\n",
    "        self.train_head = water_head[: train_steps]\n",
    "        self.train_pressure = water_pressure[: train_steps]\n",
    "        self.val_head = water_head[train_steps: train_steps + val_steps]\n",
    "        self.val_pressure = water_pressure[train_steps: train_steps + val_steps]\n",
    "        self.test_head = water_head[-test_steps:]\n",
    "        self.test_pressure = water_pressure[-test_steps:]\n",
    "\n",
    "        # Divide the time series data into multiple training samples\n",
    "        self.trainX, self.trainY = self.seq2instance(args, self.train_head, self.train_pressure)\n",
    "        self.valX, self.valY = self.seq2instance(args, self.val_head, self.val_pressure)\n",
    "        self.testX, self.testY = self.seq2instance(args, self.test_head, self.test_pressure)\n",
    "    \n",
    "    # Slide window to generate sample\n",
    "    def seq2instance(self, args, data1, data2):\n",
    "        num_step, dims = data1.shape\n",
    "        num_sample = num_step - args.num_hp - args.num_hp + 1\n",
    "        x = torch.zeros(num_sample, args.num_hp, dims)\n",
    "        y = torch.zeros(num_sample, args.num_hp, dims)\n",
    "        for i in range(num_sample):\n",
    "            ''' \n",
    "            head = pressure + height\n",
    "            \n",
    "            Although this paper uses two kinds of data to improve the meaning of the data\n",
    "            But (x[i] --> data1) or (y[i] --> data2)\n",
    "            You can get basically the same results\n",
    "            But the effect will be slightly lower in the range of 0.5m by about 2%\n",
    "            \n",
    "            '''\n",
    "            x[i] = data2[i: i + args.num_hp]\n",
    "            y[i] = data1[i + args.num_hp: i + args.num_hp + args.num_hp]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset_type == 'train':\n",
    "            return len(self.trainX)\n",
    "        elif self.dataset_type == 'val':\n",
    "            return len(self.valX)\n",
    "        elif self.dataset_type == 'test':\n",
    "            return len(self.testX)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset_type == 'train':\n",
    "            return self.trainX[idx].to(self.args.device), self.trainY[idx].to(self.args.device)\n",
    "        elif self.dataset_type == 'val':\n",
    "            return self.valX[idx].to(self.args.device), self.valY[idx].to(self.args.device)\n",
    "        elif self.dataset_type == 'test':\n",
    "            return self.testX[idx].to(self.args.device), self.testY[idx].to(self.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.815317Z",
     "iopub.status.busy": "2024-07-01T06:44:10.814992Z",
     "iopub.status.idle": "2024-07-01T06:44:10.821200Z",
     "shell.execute_reply": "2024-07-01T06:44:10.820820Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.815291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "    def forward(self, pred, target):\n",
    "        mask = target != 0   \n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss * mask   \n",
    "        loss = torch.sum(loss)/torch.sum(mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.831657Z",
     "iopub.status.busy": "2024-07-01T06:44:10.831462Z",
     "iopub.status.idle": "2024-07-01T06:44:10.840052Z",
     "shell.execute_reply": "2024-07-01T06:44:10.839483Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.831643Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the weights of the model\n",
    "''' \n",
    "xavier_normal initialization is better than kaiming_normal initialization\n",
    "'''\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.822053Z",
     "iopub.status.busy": "2024-07-01T06:44:10.821883Z",
     "iopub.status.idle": "2024-07-01T06:44:10.830882Z",
     "shell.execute_reply": "2024-07-01T06:44:10.830513Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.822039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entities should not be multiplied unnecessarily.\n",
    "\n",
    "class GGNN(nn.Module):\n",
    "    def __init__(self, num_hp, num_vertex, hidden_dim, batch_size, K, d, iter_step, bn_decay):\n",
    "        super(GGNN, self).__init__()\n",
    "        self.num_hp = num_hp            \n",
    "        self.num_vertex = num_vertex    \n",
    "        self.hidden_dim = hidden_dim     \n",
    "        self.batch_size = batch_size\n",
    "        self.iter_step = iter_step    \n",
    "\n",
    "        self.W_z = AttLayer(num_vertex, hidden_dim, batch_size, d)   # update gate\n",
    "        self.W_r = AttLayer(num_vertex, hidden_dim, batch_size, d)   # reset gate\n",
    "        self.W   = AttLayer(num_vertex, hidden_dim, batch_size, d)   # candidate hidden state\n",
    "\n",
    "        self.W_height = AttLayer(num_vertex, hidden_dim, batch_size, d)\n",
    "        self.W_type   = AttLayer(num_vertex, hidden_dim, batch_size, d)\n",
    "\n",
    "        self.FC_height = nn.Linear(num_hp + num_hp, num_vertex)  \n",
    "        self.FC_type   = nn.Linear(num_hp + num_hp, num_vertex)\n",
    "\n",
    "        self.FC1 = nn.Linear(num_hp + num_vertex, num_vertex)\n",
    "        self.FC2 = nn.Linear(num_hp + num_vertex, num_vertex)\n",
    "\n",
    "        self.output_layer = nn.Linear(num_vertex, num_hp)\n",
    "\n",
    "        self.BatchNorm = nn.BatchNorm1d(num_vertex)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X, Adj, node_height, node_type):\n",
    "        \"\"\"\n",
    "        Adj : adjacency matrix (num_nodes, num_nodes)\n",
    "        X   : input features   (batch_size, num_hp, num_nodes)\n",
    "        node height ：  add\n",
    "        node type   :   multiply\n",
    "        \"\"\"\n",
    "        h = torch.zeros(self.num_vertex, self.num_vertex).to(X.device) \n",
    "        Adj = Adj.unsqueeze(0).expand(X.shape[0], -1, -1)  \n",
    "\n",
    "        for _ in range(self.iter_step):\n",
    "            ''' \n",
    "            Do not arbitrarily increase the number of loops, it will run very slowly.\n",
    "            ''' \n",
    "\n",
    "            a = torch.matmul(Adj, h)  # Aggregate neighbor information to the hidden layer\n",
    "\n",
    "            # hidden state update\n",
    "            concat = torch.cat((X, a), dim=1) \n",
    "            concat = concat.permute(0, 2, 1)  \n",
    "            concat = self.FC1(concat)  \n",
    "            concat = self.BatchNorm(concat)\n",
    "\n",
    "            attn_z = self.W_z(concat, concat, concat) # reset gate: self-Atten\n",
    "            attn_r = self.W_r(concat, concat, concat) # update gate: self-Atten\n",
    "            attn_z = torch.sigmoid(attn_z)  # This parameter can be replaced by the constant 0.5\n",
    "            attn_r = torch.sigmoid(attn_r)  # This parameter can be replaced by the constant 0.5\n",
    "\n",
    "            concat_r = torch.cat((X, attn_r * a), dim=1) \n",
    "            concat_r = concat_r.permute(0, 2, 1)\n",
    "            concat_r = self.FC2(concat_r)  \n",
    "\n",
    "            h_tilde = self.W(concat_r, concat_r, concat_r)  # candidate hidden state: self-Atten\n",
    "            h_tilde = torch.tanh(h_tilde)\n",
    "\n",
    "            h = (1 - attn_z) * a + attn_z * h_tilde  # gate control\n",
    "            \n",
    "        # Secondary polymerization (elevation)\n",
    "        concat_height = torch.cat((X, node_height), dim=1) \n",
    "        concat_height = concat_height.permute(0, 2, 1)  \n",
    "        concat_height = self.FC_height(concat_height)  \n",
    "        concat_height = self.W_height(concat_height, h, concat_height) # Atten from h\n",
    "        concat_height = self.BatchNorm(concat_height)\n",
    "        # Secondary polymerization (type)\n",
    "        concat_type = torch.cat((X, node_type), dim=1) \n",
    "        concat_type = concat_type.permute(0, 2, 1)  \n",
    "        concat_type = self.FC_type(concat_type)  \n",
    "        concat_type = self.W_type(concat_type, h, concat_type) # Atten from h\n",
    "        concat_type = self.BatchNorm(concat_type)\n",
    "        \n",
    "        # feature fusion\n",
    "        h = torch.add(h, concat_height) \n",
    "        h = torch.matmul(h, concat_type) \n",
    "\n",
    "        output = self.output_layer(h)    \n",
    "        output = output.permute(0, 2, 1)\n",
    "        output = self.tanh(output) \n",
    "\n",
    "        return output\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "    def __init__(self, num_vertex, hidden_dim, batch_size, d):\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.FC_query = nn.Linear(num_vertex, hidden_dim)\n",
    "        self.FC_key   = nn.Linear(num_vertex, hidden_dim)\n",
    "        self.FC_value = nn.Linear(num_vertex, hidden_dim)\n",
    "\n",
    "        self.FC_ = nn.Linear(hidden_dim, num_vertex)  \n",
    "\n",
    "        # It is very important to prevent overfitting\n",
    "        # adjust this parameter if the effect is not good\n",
    "        self.Dropout = nn.Dropout(0.8)  \n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        ''' \n",
    "        this model is simplified from the multi-headed attention mechanism\n",
    "        multi-headed attention mechanism has no advantage in this case\n",
    "        '''\n",
    "\n",
    "        query = self.FC_query(input1)\n",
    "        key   = self.FC_key(input2)\n",
    "        value = self.FC_value(input3)\n",
    "\n",
    "        attention = torch.matmul(query, key.transpose(1, 2))\n",
    "        attention /= (self.d ** 0.5) \n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        X = torch.matmul(attention, value)\n",
    "        X = self.Dropout(X)\n",
    "\n",
    "        output = self.FC_(X) \n",
    "\n",
    "        del query, key, value, attention, X\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.841147Z",
     "iopub.status.busy": "2024-07-01T06:44:10.840933Z",
     "iopub.status.idle": "2024-07-01T06:44:10.855016Z",
     "shell.execute_reply": "2024-07-01T06:44:10.854539Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.841127Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args, std_pressure, mean_pressure, model, model_file, log, loss_criterion, optimizer, scheduler):\n",
    "\n",
    "    train_dataset = CustomDataset(args, water_pressure_file, water_head_file, dataset_type='train')\n",
    "    val_dataset = CustomDataset(args, water_pressure_file, water_head_file, dataset_type='val')\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    wait = 0\n",
    "    val_loss_min = float('inf')\n",
    "    best_model_wts = None\n",
    "    train_total_loss = []\n",
    "    val_total_loss = []\n",
    "\n",
    "    for epoch in range(args.max_epoch):\n",
    "        if wait >= args.patience:\n",
    "            log_string(log, f'early stop at epoch: {epoch:04d}')\n",
    "        i = 0\n",
    "\n",
    "        start_train = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            X, Y = batch\n",
    "\n",
    "            # Check and fill dimensions\n",
    "            target_size = args.batch_size\n",
    "            pad_size = target_size - X.size(0)\n",
    "            if pad_size > 0:\n",
    "                X = F.pad(X, (0, 0, 0, 0, pad_size, 0))\n",
    "                Y = F.pad(Y, (0, 0, 0, 0, pad_size, 0))\n",
    "\n",
    "            # Cascade masking \n",
    "            # (1) Only 33 sensor data; (2): Continue mask at 33 sensor positions)\n",
    "            num_masked = 33 - epoch * 3 + 1   \n",
    "            _, X2, _, mask2 = apply_mask(args, num_masked, X)\n",
    "            Y2 = Y * mask2\n",
    "            node_heights_input = node_heights * mask2\n",
    "            node_types_input = node_types * mask2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(X2, Adj, node_heights_input, node_types_input)\n",
    "            loss_batch = loss_criterion(pred, Y2)\n",
    "\n",
    "            # L1 norm (L2 norm is not suitable for this model)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss_batch += args.l1_lambda * l1_norm\n",
    "\n",
    "            train_loss += float(loss_batch) * args.batch_size\n",
    "            loss_batch.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                log_string(log, f'Training batch: {i+1} in epoch:{epoch+1}, training batch loss:{loss_batch:.4f}')\n",
    "                \n",
    "            del X2, mask2, Y2, loss_batch, node_heights_input, node_types_input\n",
    "\n",
    "        train_loss = train_loss * std_pressure / (105120 * args.train_ratio)\n",
    "\n",
    "        train_total_loss.append(train_loss)\n",
    "        end_train = time.time()\n",
    "\n",
    "        # val loss\n",
    "        j = 0\n",
    "        start_val = time.time()\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                X, Y= batch\n",
    "\n",
    "                # Check and fill dimensions\n",
    "                target_size = args.batch_size\n",
    "                pad_size = target_size - X.size(0)\n",
    "                if pad_size > 0:\n",
    "                    X = F.pad(X, (0, 0, 0, 0, pad_size, 0))\n",
    "                    Y = F.pad(Y, (0, 0, 0, 0, pad_size, 0))\n",
    "\n",
    "                ## Cascade masking\n",
    "                num_masked = 33 - epoch * 3 + 1\n",
    "                X1, _, mask1, _ = apply_mask(args, num_masked, X)\n",
    "                Y1 = Y * mask1\n",
    "                node_heights_input = node_heights * mask1\n",
    "                node_types_input = node_types * mask1\n",
    "                pred = model(X1, Adj, node_heights_input, node_types_input)\n",
    "                \n",
    "                # Losses are calculated only for nodes with masks\n",
    "                loss_batch = loss_criterion(pred, Y1) \n",
    "                val_loss += loss_batch * args.batch_size\n",
    "                j += 1\n",
    "\n",
    "                del X1,  mask1, Y1, loss_batch, node_heights_input, node_types_input\n",
    "\n",
    "        val_loss /= 105120 * args.val_ratio\n",
    "        val_total_loss.append(val_loss)\n",
    "        end_val = time.time()\n",
    "\n",
    "        log_string(\n",
    "            log,\n",
    "            '%s | epoch: %04d/%d, training time: %.1fs, inference time: %.1fs' %\n",
    "            (datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch + 1, args.max_epoch, end_train - start_train, end_val - start_val))\n",
    "        log_string(\n",
    "            log, f'train loss: {train_loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss <= val_loss_min:\n",
    "            log_string(\n",
    "                log,\n",
    "                f'val loss decrease from {val_loss_min:.4f} to {val_loss:.4f}, saving model to {model_file}')\n",
    "            wait = 0\n",
    "            val_loss_min = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, model_file)\n",
    "    log_string(log, f'Training and validation are completed, and model has been stored as {model_file}')\n",
    "    \n",
    "    return train_total_loss, val_total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.855825Z",
     "iopub.status.busy": "2024-07-01T06:44:10.855658Z",
     "iopub.status.idle": "2024-07-01T06:44:10.862692Z",
     "shell.execute_reply": "2024-07-01T06:44:10.862191Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.855811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(args, std_head, mean_head, model_file, log):\n",
    "    log_string(log, '**** testing model ****')\n",
    "    log_string(log, 'loading model from %s' % model_file)\n",
    "\n",
    "    model = torch.load(model_file)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = CustomDataset(args, water_pressure_file, water_head_file, dataset_type='test')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # test model\n",
    "    model = torch.load(model_file)\n",
    "    log_string(log, 'model restored!')\n",
    "    log_string(log, 'evaluating...')\n",
    "\n",
    "    Pred_1 = torch.Tensor().to(args.device)\n",
    "    Pred_2 = torch.Tensor().to(args.device)\n",
    "    Pred_3 = torch.Tensor().to(args.device)\n",
    "    Pred_4 = torch.Tensor().to(args.device)\n",
    "    Pred_5 = torch.Tensor().to(args.device)\n",
    "    Pred_6 = torch.Tensor().to(args.device)\n",
    "    Pred_7 = torch.Tensor().to(args.device)\n",
    "    Pred_8 = torch.Tensor().to(args.device)\n",
    "    Pred_9 = torch.Tensor().to(args.device)\n",
    "    Pred_10 = torch.Tensor().to(args.device)\n",
    "    Pred_11 = torch.Tensor().to(args.device)\n",
    "    Pred_12 = torch.Tensor().to(args.device)\n",
    "\n",
    "    Label_1 = torch.Tensor().to(args.device)\n",
    "    Label_2 = torch.Tensor().to(args.device)\n",
    "    Label_3 = torch.Tensor().to(args.device)\n",
    "    Label_4 = torch.Tensor().to(args.device)\n",
    "    Label_5 = torch.Tensor().to(args.device)\n",
    "    Label_6 = torch.Tensor().to(args.device)\n",
    "    Label_7 = torch.Tensor().to(args.device)\n",
    "    Label_8 = torch.Tensor().to(args.device)\n",
    "    Label_9 = torch.Tensor().to(args.device)\n",
    "    Label_10 = torch.Tensor().to(args.device)\n",
    "    Label_11 = torch.Tensor().to(args.device)\n",
    "    Label_12 = torch.Tensor().to(args.device)\n",
    "\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch in test_dataloader:\n",
    "            X, Y = batch\n",
    "\n",
    "            target_size = args.batch_size\n",
    "            pad_size = target_size - X.size(0)\n",
    "            if pad_size > 0:\n",
    "                X = F.pad(X, (0, 0, 0, 0, pad_size, 0))\n",
    "                Y = F.pad(Y, (0, 0, 0, 0, pad_size, 0))\n",
    "\n",
    "            num_masked = 0\n",
    "            X1, _, _, _,  = apply_mask(args, num_masked, X)  \n",
    "            # Only X1 is available to avoid information leakage\n",
    "            pred = model(X1, Adj, node_heights, node_types) \n",
    "            loss_batch = pred * std_head + mean_head\n",
    "            Y = Y * std_head + mean_head\n",
    "            \n",
    "            # MAE/MAPE\n",
    "            mae  = torch.mean(torch.abs(loss_batch - Y))\n",
    "            mape = torch.mean(torch.abs(loss_batch - Y) / Y)*100\n",
    "            mae_list.append(mae.item())\n",
    "            mape_list.append(mape.item())\n",
    "\n",
    "            loss_batch_ = torch.mean(loss_batch, dim=0)\n",
    "            testPred_1  = loss_batch_[0]\n",
    "            testPred_2  = loss_batch_[1]\n",
    "            testPred_3  = loss_batch_[2]\n",
    "            testPred_4  = loss_batch_[3]\n",
    "            testPred_5  = loss_batch_[4]\n",
    "            testPred_6  = loss_batch_[5]\n",
    "            testPred_7  = loss_batch_[6]\n",
    "            testPred_8  = loss_batch_[7]\n",
    "            testPred_9  = loss_batch_[8]\n",
    "            testPred_10 = loss_batch_[9]\n",
    "            testPred_11 = loss_batch_[10]\n",
    "            testPred_12 = loss_batch_[11]\n",
    "            testPred_1 = testPred_1.unsqueeze(0)\n",
    "            testPred_2 = testPred_2.unsqueeze(0)\n",
    "            testPred_3 = testPred_3.unsqueeze(0)\n",
    "            testPred_4 = testPred_4.unsqueeze(0)\n",
    "            testPred_5 = testPred_5.unsqueeze(0)\n",
    "            testPred_6 = testPred_6.unsqueeze(0)\n",
    "            testPred_7 = testPred_7.unsqueeze(0)\n",
    "            testPred_8 = testPred_8.unsqueeze(0)\n",
    "            testPred_9 = testPred_9.unsqueeze(0)\n",
    "            testPred_10 = testPred_10.unsqueeze(0)\n",
    "            testPred_11 = testPred_11.unsqueeze(0)\n",
    "            testPred_12 = testPred_12.unsqueeze(0)\n",
    "            Pred_1 = torch.cat((Pred_1, testPred_1), dim=0)\n",
    "            Pred_2 = torch.cat((Pred_2, testPred_2), dim=0)\n",
    "            Pred_3 = torch.cat((Pred_3, testPred_3), dim=0)\n",
    "            Pred_4 = torch.cat((Pred_4, testPred_4), dim=0)\n",
    "            Pred_5 = torch.cat((Pred_5, testPred_5), dim=0) \n",
    "            Pred_6 = torch.cat((Pred_6, testPred_6), dim=0)\n",
    "            Pred_7 = torch.cat((Pred_7, testPred_7), dim=0)\n",
    "            Pred_8 = torch.cat((Pred_8, testPred_8), dim=0)\n",
    "            Pred_9 = torch.cat((Pred_9, testPred_9), dim=0)\n",
    "            Pred_10 = torch.cat((Pred_10, testPred_10), dim=0)\n",
    "            Pred_11 = torch.cat((Pred_11, testPred_11), dim=0)\n",
    "            Pred_12 = torch.cat((Pred_12, testPred_12), dim=0)\n",
    "            \n",
    "            Y_ = torch.mean(Y, dim=0)\n",
    "            testLabel_1  = Y_[0] \n",
    "            testLabel_2  = Y_[1] \n",
    "            testLabel_3  = Y_[2] \n",
    "            testLabel_4  = Y_[3] \n",
    "            testLabel_5  = Y_[4] \n",
    "            testLabel_6  = Y_[5] \n",
    "            testLabel_7  = Y_[6] \n",
    "            testLabel_8  = Y_[7] \n",
    "            testLabel_9  = Y_[8] \n",
    "            testLabel_10 = Y_[9] \n",
    "            testLabel_11 = Y_[10]\n",
    "            testLabel_12 = Y_[11] \n",
    "            testLabel_1 = testLabel_1.unsqueeze(0)\n",
    "            testLabel_2 = testLabel_2.unsqueeze(0)\n",
    "            testLabel_3 = testLabel_3.unsqueeze(0)\n",
    "            testLabel_4 = testLabel_4.unsqueeze(0)\n",
    "            testLabel_5 = testLabel_5.unsqueeze(0)\n",
    "            testLabel_6 = testLabel_6.unsqueeze(0)\n",
    "            testLabel_7 = testLabel_7.unsqueeze(0)\n",
    "            testLabel_8 = testLabel_8.unsqueeze(0)\n",
    "            testLabel_9 = testLabel_9.unsqueeze(0)\n",
    "            testLabel_10 = testLabel_10.unsqueeze(0)\n",
    "            testLabel_11 = testLabel_11.unsqueeze(0)\n",
    "            testLabel_12 = testLabel_12.unsqueeze(0)\n",
    "            Label_1 = torch.cat((Label_1, testLabel_1), dim=0)\n",
    "            Label_2 = torch.cat((Label_2, testLabel_2), dim=0)\n",
    "            Label_3 = torch.cat((Label_3, testLabel_3), dim=0)\n",
    "            Label_4 = torch.cat((Label_4, testLabel_4), dim=0)\n",
    "            Label_5 = torch.cat((Label_5, testLabel_5), dim=0)\n",
    "            Label_6 = torch.cat((Label_6, testLabel_1), dim=0)\n",
    "            Label_7 = torch.cat((Label_7, testLabel_2), dim=0)\n",
    "            Label_8 = torch.cat((Label_8, testLabel_3), dim=0)\n",
    "            Label_9 = torch.cat((Label_9, testLabel_4), dim=0)\n",
    "            Label_10 = torch.cat((Label_10, testLabel_5), dim=0)\n",
    "            Label_11 = torch.cat((Label_11, testLabel_6), dim=0)\n",
    "            Label_12 = torch.cat((Label_12, testLabel_7), dim=0)\n",
    "  \n",
    "    end_test = time.time()\n",
    "\n",
    "    log_string(log, 'testing time: %.1fs' % (end_test - start_test))\n",
    "\n",
    "    return (Pred_1, Pred_2, Pred_3, Pred_4, Pred_5, Pred_6, Pred_7, Pred_8, Pred_9, Pred_10, Pred_11, Pred_12, \n",
    "            Label_1, Label_2, Label_3, Label_4, Label_5, Label_6, Label_7, Label_8, Label_9, Label_10, Label_11, Label_12, \n",
    "            mae_list, mape_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:10.863430Z",
     "iopub.status.busy": "2024-07-01T06:44:10.863310Z",
     "iopub.status.idle": "2024-07-01T06:44:21.497045Z",
     "shell.execute_reply": "2024-07-01T06:44:21.496353Z",
     "shell.execute_reply.started": "2024-07-01T06:44:10.863418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "——loading data——\n",
      "mean head at sensor nodes: 73.0772963793718, std head at sensor nodes: 6.528225559049961\n",
      "mean head : 72.97857368764194, std pressure: 6.515785156814622\n",
      "Adjacency matrix loaded\n",
      "node heights loaded\n",
      "node types loaded\n"
     ]
    }
   ],
   "source": [
    "# Create a description file\n",
    "log = open(log_file, 'w')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log_string(log, f'using device: {device}')\n",
    "\n",
    "log_string(log, '——loading data——')\n",
    "\n",
    "fixed_nodes = ['111', '226', '300', '781']\n",
    "sensor_name = ['1','4','31','54','105','114','163','188','215','229',\n",
    "'288','296','330','340','408','413','427','456','467',\n",
    "'493','504','514','517','547','611','634','642','677',\n",
    "'720','724','738','750','767']\n",
    "sensor_name.extend(fixed_nodes)\n",
    "sensor_index = [int(i)-1 for i in sensor_name]\n",
    "\n",
    "water_head = pd.read_csv(water_head_file, header=0, index_col=None)\n",
    "water_head= torch.from_numpy(water_head.values)\n",
    "std_head = water_head[sensor_index].std()\n",
    "mean_head = water_head[sensor_index].mean()\n",
    "std_head = std_head.to(device)\n",
    "mean_head = mean_head.to(device)\n",
    "log_string(log, f'mean head at sensor nodes: {mean_head}, std head at sensor nodes: {std_head}')\n",
    "std_head_ = water_head.std()\n",
    "mean_head_ = water_head.mean()\n",
    "log_string(log, f'mean head : {mean_head_}, std pressure: {std_head_}')\n",
    "\n",
    "# Adjacency Matrix\n",
    "Adj = pd.read_csv(Adj_file, header=0, index_col=0, sep=',')\n",
    "Adj = torch.from_numpy(Adj.values)\n",
    "Adj = (Adj + torch.eye(Adj.shape[0])).to(dtype=torch.float32)\n",
    "Adj = Adj.to(device)\n",
    "log_string(log, f'Adjacency matrix loaded')\n",
    "\n",
    "# node feature\n",
    "node_heights = pd.read_csv(node_heights, header=None, index_col=0, sep=',')\n",
    "node_heights = torch.from_numpy(node_heights.values).float()\n",
    "node_heights = (node_heights - node_heights.mean()) / (node_heights.std())\n",
    "node_heights = node_heights.unsqueeze(0).expand(args.batch_size, -1, -1)\n",
    "node_heights = node_heights.repeat(1, 1, 12)\n",
    "node_heights = node_heights.transpose(1, 2)\n",
    "node_heights = node_heights.to(device)\n",
    "log_string(log, f'node heights loaded')\n",
    "\n",
    "node_types = pd.read_csv(node_types, header=None, index_col=0, sep=',')\n",
    "node_types = torch.from_numpy(node_types.values).float()\n",
    "node_types = (node_types - node_types.mean()) / (node_types.std())\n",
    "node_types = node_types.unsqueeze(0).expand(args.batch_size, -1, -1)\n",
    "node_types = node_types.repeat(1, 1, 12)\n",
    "node_types = node_types.transpose(1, 2)\n",
    "node_types = node_types.to(device)\n",
    "log_string(log, f'node types loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:21.498308Z",
     "iopub.status.busy": "2024-07-01T06:44:21.498007Z",
     "iopub.status.idle": "2024-07-01T06:44:22.682033Z",
     "shell.execute_reply": "2024-07-01T06:44:22.681413Z",
     "shell.execute_reply.started": "2024-07-01T06:44:21.498294Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——loading model——\n",
      "model loaded!\n",
      "trainable parameters: 1,478,292\n",
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "log_string(log, '——loading model——')\n",
    "model = GGNN(args.num_hp, args.num_vertex, args.hidden_dim,  \n",
    "             args.batch_size, args.K, args.d, args.iter_step, args.bn_decay).to(args.device)\n",
    "model.apply(init_weights)\n",
    "log_string(log, 'model loaded!')\n",
    "\n",
    "loss_criterion = CustomLoss()\n",
    "optimizer = optim.Adam(model.parameters(), args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=args.decay_epoch,\n",
    "                                      gamma=args.gamma)\n",
    "parameters = count_parameters(model)\n",
    "log_string(log, 'trainable parameters: {:,}'.format(parameters))\n",
    "log_string(log, torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:44:22.683271Z",
     "iopub.status.busy": "2024-07-01T06:44:22.682901Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AppData\\Local\\Temp\\ipykernel_34960\\1302657811.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masked_tensor1 = torch.tensor(masked_tensor1, dtype=torch.float32)\n",
      "C:\\AppData\\Local\\Temp\\ipykernel_34960\\1302657811.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masked_tensor2 = torch.tensor(masked_tensor2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 11 in epoch:1, training batch loss:345.0157\n",
      "Training batch: 21 in epoch:1, training batch loss:244.4195\n",
      "Training batch: 31 in epoch:1, training batch loss:163.1194\n",
      "Training batch: 41 in epoch:1, training batch loss:108.5228\n",
      "Training batch: 51 in epoch:1, training batch loss:69.4231\n",
      "Training batch: 61 in epoch:1, training batch loss:44.9714\n",
      "Training batch: 71 in epoch:1, training batch loss:31.9676\n",
      "Training batch: 81 in epoch:1, training batch loss:24.0109\n",
      "Training batch: 91 in epoch:1, training batch loss:19.5300\n",
      "Training batch: 101 in epoch:1, training batch loss:17.1739\n",
      "Training batch: 111 in epoch:1, training batch loss:15.8323\n",
      "Training batch: 121 in epoch:1, training batch loss:14.1766\n",
      "Training batch: 131 in epoch:1, training batch loss:14.0706\n",
      "Training batch: 141 in epoch:1, training batch loss:12.5691\n",
      "Training batch: 151 in epoch:1, training batch loss:11.8852\n",
      "Training batch: 161 in epoch:1, training batch loss:11.8917\n",
      "Training batch: 171 in epoch:1, training batch loss:11.1372\n",
      "Training batch: 181 in epoch:1, training batch loss:11.7606\n",
      "Training batch: 191 in epoch:1, training batch loss:11.5392\n",
      "Training batch: 201 in epoch:1, training batch loss:11.6580\n",
      "Training batch: 211 in epoch:1, training batch loss:11.0573\n",
      "Training batch: 221 in epoch:1, training batch loss:10.5250\n",
      "Training batch: 231 in epoch:1, training batch loss:11.9044\n",
      "Training batch: 241 in epoch:1, training batch loss:11.1395\n",
      "Training batch: 251 in epoch:1, training batch loss:11.0962\n",
      "Training batch: 261 in epoch:1, training batch loss:10.4474\n",
      "Training batch: 271 in epoch:1, training batch loss:10.3802\n",
      "Training batch: 281 in epoch:1, training batch loss:10.1408\n",
      "Training batch: 291 in epoch:1, training batch loss:10.1220\n",
      "Training batch: 301 in epoch:1, training batch loss:10.6377\n",
      "Training batch: 311 in epoch:1, training batch loss:10.2383\n",
      "Training batch: 321 in epoch:1, training batch loss:10.1736\n",
      "Training batch: 331 in epoch:1, training batch loss:9.4226\n",
      "Training batch: 341 in epoch:1, training batch loss:11.0240\n",
      "Training batch: 351 in epoch:1, training batch loss:10.9932\n",
      "Training batch: 361 in epoch:1, training batch loss:9.4312\n",
      "Training batch: 371 in epoch:1, training batch loss:10.2541\n",
      "Training batch: 381 in epoch:1, training batch loss:9.5538\n",
      "Training batch: 391 in epoch:1, training batch loss:10.1095\n",
      "Training batch: 401 in epoch:1, training batch loss:9.5562\n",
      "Training batch: 411 in epoch:1, training batch loss:9.9797\n",
      "Training batch: 421 in epoch:1, training batch loss:8.4329\n",
      "Training batch: 431 in epoch:1, training batch loss:9.6413\n",
      "Training batch: 441 in epoch:1, training batch loss:9.3800\n",
      "Training batch: 451 in epoch:1, training batch loss:9.2727\n",
      "Training batch: 461 in epoch:1, training batch loss:8.5530\n",
      "Training batch: 471 in epoch:1, training batch loss:8.7961\n",
      "Training batch: 481 in epoch:1, training batch loss:9.2147\n",
      "Training batch: 491 in epoch:1, training batch loss:9.0125\n",
      "Training batch: 501 in epoch:1, training batch loss:8.5278\n",
      "Training batch: 511 in epoch:1, training batch loss:9.3317\n",
      "Training batch: 521 in epoch:1, training batch loss:8.9549\n",
      "Training batch: 531 in epoch:1, training batch loss:9.1622\n",
      "Training batch: 541 in epoch:1, training batch loss:8.6338\n",
      "Training batch: 551 in epoch:1, training batch loss:8.3975\n",
      "Training batch: 561 in epoch:1, training batch loss:7.9074\n",
      "Training batch: 571 in epoch:1, training batch loss:8.0484\n",
      "Training batch: 581 in epoch:1, training batch loss:8.4982\n",
      "Training batch: 591 in epoch:1, training batch loss:7.6697\n",
      "Training batch: 601 in epoch:1, training batch loss:7.2294\n",
      "Training batch: 611 in epoch:1, training batch loss:7.3284\n",
      "Training batch: 621 in epoch:1, training batch loss:7.3309\n",
      "Training batch: 631 in epoch:1, training batch loss:8.3138\n",
      "Training batch: 641 in epoch:1, training batch loss:8.3629\n",
      "Training batch: 651 in epoch:1, training batch loss:7.0928\n",
      "Training batch: 661 in epoch:1, training batch loss:7.2806\n",
      "Training batch: 671 in epoch:1, training batch loss:7.2695\n",
      "Training batch: 681 in epoch:1, training batch loss:6.6898\n",
      "Training batch: 691 in epoch:1, training batch loss:7.2094\n",
      "Training batch: 701 in epoch:1, training batch loss:6.7450\n",
      "Training batch: 711 in epoch:1, training batch loss:6.5185\n",
      "Training batch: 721 in epoch:1, training batch loss:6.5792\n",
      "Training batch: 731 in epoch:1, training batch loss:6.5669\n",
      "Training batch: 741 in epoch:1, training batch loss:6.3725\n",
      "Training batch: 751 in epoch:1, training batch loss:6.0911\n",
      "Training batch: 761 in epoch:1, training batch loss:6.7813\n",
      "Training batch: 771 in epoch:1, training batch loss:7.1906\n",
      "Training batch: 781 in epoch:1, training batch loss:5.7557\n",
      "Training batch: 791 in epoch:1, training batch loss:5.9862\n",
      "Training batch: 801 in epoch:1, training batch loss:5.8657\n",
      "Training batch: 811 in epoch:1, training batch loss:5.5182\n",
      "Training batch: 821 in epoch:1, training batch loss:6.2352\n",
      "Training batch: 831 in epoch:1, training batch loss:6.1181\n",
      "Training batch: 841 in epoch:1, training batch loss:5.2095\n",
      "Training batch: 851 in epoch:1, training batch loss:5.5867\n",
      "Training batch: 861 in epoch:1, training batch loss:6.5077\n",
      "Training batch: 871 in epoch:1, training batch loss:6.2585\n",
      "Training batch: 881 in epoch:1, training batch loss:5.3917\n",
      "Training batch: 891 in epoch:1, training batch loss:5.3977\n",
      "Training batch: 901 in epoch:1, training batch loss:5.0701\n",
      "Training batch: 911 in epoch:1, training batch loss:5.1333\n",
      "Training batch: 921 in epoch:1, training batch loss:5.0671\n",
      "Training batch: 931 in epoch:1, training batch loss:4.9552\n",
      "Training batch: 941 in epoch:1, training batch loss:5.2892\n",
      "Training batch: 951 in epoch:1, training batch loss:5.1608\n",
      "Training batch: 961 in epoch:1, training batch loss:4.6705\n",
      "Training batch: 971 in epoch:1, training batch loss:4.9328\n",
      "Training batch: 981 in epoch:1, training batch loss:4.2888\n",
      "Training batch: 991 in epoch:1, training batch loss:4.6162\n",
      "Training batch: 1001 in epoch:1, training batch loss:5.1619\n",
      "Training batch: 1011 in epoch:1, training batch loss:4.3360\n",
      "Training batch: 1021 in epoch:1, training batch loss:4.4888\n",
      "Training batch: 1031 in epoch:1, training batch loss:4.5141\n",
      "Training batch: 1041 in epoch:1, training batch loss:4.3550\n",
      "Training batch: 1051 in epoch:1, training batch loss:4.2094\n",
      "Training batch: 1061 in epoch:1, training batch loss:4.4903\n",
      "Training batch: 1071 in epoch:1, training batch loss:4.2437\n",
      "Training batch: 1081 in epoch:1, training batch loss:4.3798\n",
      "Training batch: 1091 in epoch:1, training batch loss:4.9936\n",
      "Training batch: 1101 in epoch:1, training batch loss:4.4162\n",
      "Training batch: 1111 in epoch:1, training batch loss:4.2355\n",
      "Training batch: 1121 in epoch:1, training batch loss:3.2169\n",
      "Training batch: 1131 in epoch:1, training batch loss:4.4877\n",
      "Training batch: 1141 in epoch:1, training batch loss:4.2514\n",
      "Training batch: 1151 in epoch:1, training batch loss:4.2716\n",
      "Training batch: 1161 in epoch:1, training batch loss:4.5601\n",
      "Training batch: 1171 in epoch:1, training batch loss:3.9606\n",
      "Training batch: 1181 in epoch:1, training batch loss:4.4467\n",
      "Training batch: 1191 in epoch:1, training batch loss:4.0064\n",
      "Training batch: 1201 in epoch:1, training batch loss:3.8647\n",
      "Training batch: 1211 in epoch:1, training batch loss:3.7593\n",
      "Training batch: 1221 in epoch:1, training batch loss:4.8116\n",
      "Training batch: 1231 in epoch:1, training batch loss:4.0026\n",
      "Training batch: 1241 in epoch:1, training batch loss:4.4266\n",
      "Training batch: 1251 in epoch:1, training batch loss:3.9602\n",
      "Training batch: 1261 in epoch:1, training batch loss:3.9554\n",
      "Training batch: 1271 in epoch:1, training batch loss:4.1035\n",
      "Training batch: 1281 in epoch:1, training batch loss:3.9344\n",
      "Training batch: 1291 in epoch:1, training batch loss:3.9300\n",
      "Training batch: 1301 in epoch:1, training batch loss:4.0281\n",
      "Training batch: 1311 in epoch:1, training batch loss:3.8357\n",
      "Training batch: 1321 in epoch:1, training batch loss:4.0473\n",
      "Training batch: 1331 in epoch:1, training batch loss:3.6434\n",
      "Training batch: 1341 in epoch:1, training batch loss:3.9595\n",
      "Training batch: 1351 in epoch:1, training batch loss:4.0340\n",
      "Training batch: 1361 in epoch:1, training batch loss:4.2172\n",
      "Training batch: 1371 in epoch:1, training batch loss:3.8511\n",
      "Training batch: 1381 in epoch:1, training batch loss:4.3156\n",
      "Training batch: 1391 in epoch:1, training batch loss:4.0243\n",
      "Training batch: 1401 in epoch:1, training batch loss:3.9321\n",
      "Training batch: 1411 in epoch:1, training batch loss:3.8742\n",
      "Training batch: 1421 in epoch:1, training batch loss:4.0023\n",
      "Training batch: 1431 in epoch:1, training batch loss:3.9446\n",
      "Training batch: 1441 in epoch:1, training batch loss:4.0584\n",
      "Training batch: 1451 in epoch:1, training batch loss:4.0403\n",
      "Training batch: 1461 in epoch:1, training batch loss:3.6385\n",
      "Training batch: 1471 in epoch:1, training batch loss:3.8745\n",
      "Training batch: 1481 in epoch:1, training batch loss:3.7998\n",
      "Training batch: 1491 in epoch:1, training batch loss:4.5234\n",
      "Training batch: 1501 in epoch:1, training batch loss:4.2411\n",
      "Training batch: 1511 in epoch:1, training batch loss:3.8543\n",
      "Training batch: 1521 in epoch:1, training batch loss:3.7459\n",
      "Training batch: 1531 in epoch:1, training batch loss:3.9452\n",
      "Training batch: 1541 in epoch:1, training batch loss:3.7596\n",
      "Training batch: 1551 in epoch:1, training batch loss:3.9598\n",
      "Training batch: 1561 in epoch:1, training batch loss:4.0206\n",
      "Training batch: 1571 in epoch:1, training batch loss:4.0109\n",
      "Training batch: 1581 in epoch:1, training batch loss:3.6858\n",
      "Training batch: 1591 in epoch:1, training batch loss:3.8166\n",
      "Training batch: 1601 in epoch:1, training batch loss:4.1420\n",
      "Training batch: 1611 in epoch:1, training batch loss:4.1922\n",
      "Training batch: 1621 in epoch:1, training batch loss:4.1292\n",
      "Training batch: 1631 in epoch:1, training batch loss:3.4945\n",
      "Training batch: 1641 in epoch:1, training batch loss:3.6094\n",
      "Training batch: 1651 in epoch:1, training batch loss:3.4697\n",
      "Training batch: 1661 in epoch:1, training batch loss:3.9324\n",
      "Training batch: 1671 in epoch:1, training batch loss:3.8435\n",
      "Training batch: 1681 in epoch:1, training batch loss:3.7053\n",
      "Training batch: 1691 in epoch:1, training batch loss:3.3412\n",
      "Training batch: 1701 in epoch:1, training batch loss:3.6824\n",
      "Training batch: 1711 in epoch:1, training batch loss:3.7233\n",
      "Training batch: 1721 in epoch:1, training batch loss:3.9865\n",
      "Training batch: 1731 in epoch:1, training batch loss:3.3062\n",
      "Training batch: 1741 in epoch:1, training batch loss:3.5688\n",
      "Training batch: 1751 in epoch:1, training batch loss:3.9274\n",
      "Training batch: 1761 in epoch:1, training batch loss:3.9430\n",
      "Training batch: 1771 in epoch:1, training batch loss:3.7203\n",
      "Training batch: 1781 in epoch:1, training batch loss:3.7607\n",
      "Training batch: 1791 in epoch:1, training batch loss:3.8759\n",
      "Training batch: 1801 in epoch:1, training batch loss:3.3108\n",
      "Training batch: 1811 in epoch:1, training batch loss:3.9971\n",
      "Training batch: 1821 in epoch:1, training batch loss:3.7331\n",
      "Training batch: 1831 in epoch:1, training batch loss:3.9721\n",
      "Training batch: 1841 in epoch:1, training batch loss:4.1344\n",
      "Training batch: 1851 in epoch:1, training batch loss:4.1297\n",
      "Training batch: 1861 in epoch:1, training batch loss:3.3685\n",
      "Training batch: 1871 in epoch:1, training batch loss:3.3926\n",
      "Training batch: 1881 in epoch:1, training batch loss:4.0354\n",
      "Training batch: 1891 in epoch:1, training batch loss:3.8136\n",
      "Training batch: 1901 in epoch:1, training batch loss:3.9004\n",
      "Training batch: 1911 in epoch:1, training batch loss:3.5473\n",
      "Training batch: 1921 in epoch:1, training batch loss:3.1269\n",
      "Training batch: 1931 in epoch:1, training batch loss:4.1415\n",
      "Training batch: 1941 in epoch:1, training batch loss:3.7473\n",
      "Training batch: 1951 in epoch:1, training batch loss:3.6818\n",
      "Training batch: 1961 in epoch:1, training batch loss:3.4943\n",
      "Training batch: 1971 in epoch:1, training batch loss:3.5829\n",
      "Training batch: 1981 in epoch:1, training batch loss:3.7666\n",
      "Training batch: 1991 in epoch:1, training batch loss:3.9005\n",
      "Training batch: 2001 in epoch:1, training batch loss:3.7374\n",
      "Training batch: 2011 in epoch:1, training batch loss:3.8653\n",
      "Training batch: 2021 in epoch:1, training batch loss:4.7901\n",
      "Training batch: 2031 in epoch:1, training batch loss:4.2069\n",
      "Training batch: 2041 in epoch:1, training batch loss:3.3327\n",
      "Training batch: 2051 in epoch:1, training batch loss:3.7095\n",
      "Training batch: 2061 in epoch:1, training batch loss:3.6448\n",
      "Training batch: 2071 in epoch:1, training batch loss:3.4980\n",
      "Training batch: 2081 in epoch:1, training batch loss:3.1275\n",
      "Training batch: 2091 in epoch:1, training batch loss:3.7971\n",
      "Training batch: 2101 in epoch:1, training batch loss:3.5906\n",
      "Training batch: 2111 in epoch:1, training batch loss:4.1238\n",
      "Training batch: 2121 in epoch:1, training batch loss:3.6805\n",
      "Training batch: 2131 in epoch:1, training batch loss:3.8029\n",
      "Training batch: 2141 in epoch:1, training batch loss:3.7945\n",
      "Training batch: 2151 in epoch:1, training batch loss:3.9926\n",
      "Training batch: 2161 in epoch:1, training batch loss:3.5752\n",
      "Training batch: 2171 in epoch:1, training batch loss:3.4166\n",
      "Training batch: 2181 in epoch:1, training batch loss:3.6919\n",
      "Training batch: 2191 in epoch:1, training batch loss:3.7533\n",
      "Training batch: 2201 in epoch:1, training batch loss:4.0223\n",
      "Training batch: 2211 in epoch:1, training batch loss:3.8924\n",
      "Training batch: 2221 in epoch:1, training batch loss:3.7097\n",
      "Training batch: 2231 in epoch:1, training batch loss:3.6405\n",
      "Training batch: 2241 in epoch:1, training batch loss:3.4691\n",
      "Training batch: 2251 in epoch:1, training batch loss:3.7410\n",
      "Training batch: 2261 in epoch:1, training batch loss:3.7432\n",
      "Training batch: 2271 in epoch:1, training batch loss:3.4316\n",
      "Training batch: 2281 in epoch:1, training batch loss:3.6160\n",
      "Training batch: 2291 in epoch:1, training batch loss:3.6891\n",
      "2024-09-03 16:49:50 | epoch: 0001/10, training time: 668.6s, inference time: 70.9s\n",
      "train loss: 71.8168, val_loss: 0.8526\n",
      "val loss decrease from inf to 0.8526, saving model to GGNN_33_step12_height_type.pkl\n",
      "Training batch: 11 in epoch:2, training batch loss:3.6778\n",
      "Training batch: 21 in epoch:2, training batch loss:3.4615\n",
      "Training batch: 31 in epoch:2, training batch loss:3.3701\n",
      "Training batch: 41 in epoch:2, training batch loss:3.4906\n",
      "Training batch: 51 in epoch:2, training batch loss:3.6583\n",
      "Training batch: 61 in epoch:2, training batch loss:3.3778\n",
      "Training batch: 71 in epoch:2, training batch loss:3.5127\n",
      "Training batch: 81 in epoch:2, training batch loss:3.3088\n",
      "Training batch: 91 in epoch:2, training batch loss:3.7042\n",
      "Training batch: 101 in epoch:2, training batch loss:3.4550\n",
      "Training batch: 111 in epoch:2, training batch loss:3.4807\n",
      "Training batch: 121 in epoch:2, training batch loss:3.3826\n",
      "Training batch: 131 in epoch:2, training batch loss:3.5824\n",
      "Training batch: 141 in epoch:2, training batch loss:3.5609\n",
      "Training batch: 151 in epoch:2, training batch loss:3.5745\n",
      "Training batch: 161 in epoch:2, training batch loss:3.4749\n",
      "Training batch: 171 in epoch:2, training batch loss:3.7676\n",
      "Training batch: 181 in epoch:2, training batch loss:3.2577\n",
      "Training batch: 191 in epoch:2, training batch loss:3.3735\n",
      "Training batch: 201 in epoch:2, training batch loss:3.3813\n",
      "Training batch: 211 in epoch:2, training batch loss:3.4736\n",
      "Training batch: 221 in epoch:2, training batch loss:3.3628\n",
      "Training batch: 231 in epoch:2, training batch loss:3.0679\n",
      "Training batch: 241 in epoch:2, training batch loss:3.5043\n",
      "Training batch: 251 in epoch:2, training batch loss:3.5024\n",
      "Training batch: 261 in epoch:2, training batch loss:3.2893\n",
      "Training batch: 271 in epoch:2, training batch loss:3.2553\n",
      "Training batch: 281 in epoch:2, training batch loss:3.5114\n",
      "Training batch: 291 in epoch:2, training batch loss:3.5218\n",
      "Training batch: 301 in epoch:2, training batch loss:3.6810\n",
      "Training batch: 311 in epoch:2, training batch loss:3.4146\n",
      "Training batch: 321 in epoch:2, training batch loss:3.6700\n",
      "Training batch: 331 in epoch:2, training batch loss:3.3313\n",
      "Training batch: 341 in epoch:2, training batch loss:3.2767\n",
      "Training batch: 351 in epoch:2, training batch loss:3.4466\n",
      "Training batch: 361 in epoch:2, training batch loss:3.6300\n",
      "Training batch: 371 in epoch:2, training batch loss:3.7255\n",
      "Training batch: 381 in epoch:2, training batch loss:3.7046\n",
      "Training batch: 391 in epoch:2, training batch loss:3.3373\n",
      "Training batch: 401 in epoch:2, training batch loss:3.6767\n",
      "Training batch: 411 in epoch:2, training batch loss:3.3599\n",
      "Training batch: 421 in epoch:2, training batch loss:3.3456\n",
      "Training batch: 431 in epoch:2, training batch loss:3.7336\n",
      "Training batch: 441 in epoch:2, training batch loss:3.1428\n",
      "Training batch: 451 in epoch:2, training batch loss:3.4408\n",
      "Training batch: 461 in epoch:2, training batch loss:3.3066\n",
      "Training batch: 471 in epoch:2, training batch loss:3.3625\n",
      "Training batch: 481 in epoch:2, training batch loss:3.7916\n",
      "Training batch: 491 in epoch:2, training batch loss:3.7393\n",
      "Training batch: 501 in epoch:2, training batch loss:3.2023\n",
      "Training batch: 511 in epoch:2, training batch loss:3.6693\n",
      "Training batch: 521 in epoch:2, training batch loss:3.6321\n",
      "Training batch: 531 in epoch:2, training batch loss:3.4851\n",
      "Training batch: 541 in epoch:2, training batch loss:3.3124\n",
      "Training batch: 551 in epoch:2, training batch loss:3.3309\n",
      "Training batch: 561 in epoch:2, training batch loss:3.4140\n",
      "Training batch: 571 in epoch:2, training batch loss:3.6251\n",
      "Training batch: 581 in epoch:2, training batch loss:3.2808\n",
      "Training batch: 591 in epoch:2, training batch loss:3.5787\n",
      "Training batch: 601 in epoch:2, training batch loss:3.2574\n",
      "Training batch: 611 in epoch:2, training batch loss:3.2790\n",
      "Training batch: 621 in epoch:2, training batch loss:3.2957\n",
      "Training batch: 631 in epoch:2, training batch loss:3.2064\n",
      "Training batch: 641 in epoch:2, training batch loss:3.4598\n",
      "Training batch: 651 in epoch:2, training batch loss:3.5025\n",
      "Training batch: 661 in epoch:2, training batch loss:3.7796\n",
      "Training batch: 671 in epoch:2, training batch loss:3.9284\n",
      "Training batch: 681 in epoch:2, training batch loss:3.7797\n",
      "Training batch: 691 in epoch:2, training batch loss:3.6178\n",
      "Training batch: 701 in epoch:2, training batch loss:3.5280\n",
      "Training batch: 711 in epoch:2, training batch loss:3.2713\n",
      "Training batch: 721 in epoch:2, training batch loss:3.3581\n",
      "Training batch: 731 in epoch:2, training batch loss:3.6408\n",
      "Training batch: 741 in epoch:2, training batch loss:3.7128\n",
      "Training batch: 751 in epoch:2, training batch loss:3.5592\n",
      "Training batch: 761 in epoch:2, training batch loss:3.5275\n",
      "Training batch: 771 in epoch:2, training batch loss:3.3101\n",
      "Training batch: 781 in epoch:2, training batch loss:3.7437\n",
      "Training batch: 791 in epoch:2, training batch loss:3.6606\n",
      "Training batch: 801 in epoch:2, training batch loss:3.5747\n",
      "Training batch: 811 in epoch:2, training batch loss:3.4568\n",
      "Training batch: 821 in epoch:2, training batch loss:3.5078\n",
      "Training batch: 831 in epoch:2, training batch loss:3.2107\n",
      "Training batch: 841 in epoch:2, training batch loss:3.4881\n",
      "Training batch: 851 in epoch:2, training batch loss:3.5278\n",
      "Training batch: 861 in epoch:2, training batch loss:3.6468\n",
      "Training batch: 871 in epoch:2, training batch loss:3.3614\n",
      "Training batch: 881 in epoch:2, training batch loss:3.4045\n",
      "Training batch: 891 in epoch:2, training batch loss:3.4401\n",
      "Training batch: 901 in epoch:2, training batch loss:3.6476\n",
      "Training batch: 911 in epoch:2, training batch loss:3.7460\n",
      "Training batch: 921 in epoch:2, training batch loss:3.3419\n",
      "Training batch: 931 in epoch:2, training batch loss:2.4555\n",
      "Training batch: 941 in epoch:2, training batch loss:3.2006\n",
      "Training batch: 951 in epoch:2, training batch loss:3.3413\n",
      "Training batch: 961 in epoch:2, training batch loss:3.3609\n",
      "Training batch: 971 in epoch:2, training batch loss:3.4312\n",
      "Training batch: 981 in epoch:2, training batch loss:3.1826\n",
      "Training batch: 991 in epoch:2, training batch loss:3.6253\n",
      "Training batch: 1001 in epoch:2, training batch loss:3.3550\n",
      "Training batch: 1011 in epoch:2, training batch loss:3.3420\n",
      "Training batch: 1021 in epoch:2, training batch loss:3.1701\n",
      "Training batch: 1031 in epoch:2, training batch loss:3.3308\n",
      "Training batch: 1041 in epoch:2, training batch loss:3.3688\n",
      "Training batch: 1051 in epoch:2, training batch loss:3.4411\n",
      "Training batch: 1061 in epoch:2, training batch loss:3.7976\n",
      "Training batch: 1071 in epoch:2, training batch loss:3.3575\n",
      "Training batch: 1081 in epoch:2, training batch loss:3.7821\n",
      "Training batch: 1091 in epoch:2, training batch loss:3.5313\n",
      "Training batch: 1101 in epoch:2, training batch loss:3.4064\n",
      "Training batch: 1111 in epoch:2, training batch loss:3.3886\n",
      "Training batch: 1121 in epoch:2, training batch loss:3.1707\n",
      "Training batch: 1131 in epoch:2, training batch loss:3.8268\n",
      "Training batch: 1141 in epoch:2, training batch loss:3.6041\n",
      "Training batch: 1151 in epoch:2, training batch loss:3.3982\n",
      "Training batch: 1161 in epoch:2, training batch loss:3.8748\n",
      "Training batch: 1171 in epoch:2, training batch loss:4.0674\n",
      "Training batch: 1181 in epoch:2, training batch loss:3.3482\n",
      "Training batch: 1191 in epoch:2, training batch loss:3.2046\n",
      "Training batch: 1201 in epoch:2, training batch loss:3.4598\n",
      "Training batch: 1211 in epoch:2, training batch loss:3.2799\n",
      "Training batch: 1221 in epoch:2, training batch loss:3.3874\n",
      "Training batch: 1231 in epoch:2, training batch loss:3.2355\n",
      "Training batch: 1241 in epoch:2, training batch loss:3.1369\n",
      "Training batch: 1251 in epoch:2, training batch loss:3.2494\n",
      "Training batch: 1261 in epoch:2, training batch loss:3.4109\n",
      "Training batch: 1271 in epoch:2, training batch loss:3.4543\n",
      "Training batch: 1281 in epoch:2, training batch loss:3.1948\n",
      "Training batch: 1291 in epoch:2, training batch loss:3.6844\n",
      "Training batch: 1301 in epoch:2, training batch loss:3.3407\n",
      "Training batch: 1311 in epoch:2, training batch loss:3.4459\n",
      "Training batch: 1321 in epoch:2, training batch loss:3.5282\n",
      "Training batch: 1331 in epoch:2, training batch loss:3.8721\n",
      "Training batch: 1341 in epoch:2, training batch loss:3.7041\n",
      "Training batch: 1351 in epoch:2, training batch loss:3.7544\n",
      "Training batch: 1361 in epoch:2, training batch loss:3.5414\n",
      "Training batch: 1371 in epoch:2, training batch loss:3.7790\n",
      "Training batch: 1381 in epoch:2, training batch loss:3.3159\n",
      "Training batch: 1391 in epoch:2, training batch loss:3.4430\n",
      "Training batch: 1401 in epoch:2, training batch loss:3.7128\n",
      "Training batch: 1411 in epoch:2, training batch loss:3.3747\n",
      "Training batch: 1421 in epoch:2, training batch loss:3.3463\n",
      "Training batch: 1431 in epoch:2, training batch loss:3.1481\n",
      "Training batch: 1441 in epoch:2, training batch loss:3.3781\n",
      "Training batch: 1451 in epoch:2, training batch loss:3.4314\n",
      "Training batch: 1461 in epoch:2, training batch loss:3.5436\n",
      "Training batch: 1471 in epoch:2, training batch loss:3.3617\n",
      "Training batch: 1481 in epoch:2, training batch loss:3.3403\n",
      "Training batch: 1491 in epoch:2, training batch loss:3.2720\n",
      "Training batch: 1501 in epoch:2, training batch loss:3.8817\n",
      "Training batch: 1511 in epoch:2, training batch loss:3.1291\n",
      "Training batch: 1521 in epoch:2, training batch loss:3.4849\n",
      "Training batch: 1531 in epoch:2, training batch loss:3.1471\n",
      "Training batch: 1541 in epoch:2, training batch loss:3.5042\n",
      "Training batch: 1551 in epoch:2, training batch loss:3.6567\n",
      "Training batch: 1561 in epoch:2, training batch loss:3.3699\n",
      "Training batch: 1571 in epoch:2, training batch loss:3.5811\n",
      "Training batch: 1581 in epoch:2, training batch loss:3.2442\n",
      "Training batch: 1591 in epoch:2, training batch loss:3.6595\n",
      "Training batch: 1601 in epoch:2, training batch loss:3.2967\n",
      "Training batch: 1611 in epoch:2, training batch loss:3.1790\n",
      "Training batch: 1621 in epoch:2, training batch loss:3.4355\n",
      "Training batch: 1631 in epoch:2, training batch loss:3.4238\n",
      "Training batch: 1641 in epoch:2, training batch loss:3.5790\n",
      "Training batch: 1651 in epoch:2, training batch loss:3.5054\n",
      "Training batch: 1661 in epoch:2, training batch loss:3.8719\n",
      "Training batch: 1671 in epoch:2, training batch loss:3.5368\n",
      "Training batch: 1681 in epoch:2, training batch loss:3.1624\n",
      "Training batch: 1691 in epoch:2, training batch loss:3.0098\n",
      "Training batch: 1701 in epoch:2, training batch loss:3.3738\n",
      "Training batch: 1711 in epoch:2, training batch loss:3.6071\n",
      "Training batch: 1721 in epoch:2, training batch loss:3.3188\n",
      "Training batch: 1731 in epoch:2, training batch loss:3.3353\n",
      "Training batch: 1741 in epoch:2, training batch loss:3.5569\n",
      "Training batch: 1751 in epoch:2, training batch loss:3.7299\n",
      "Training batch: 1761 in epoch:2, training batch loss:3.2014\n",
      "Training batch: 1771 in epoch:2, training batch loss:3.5187\n",
      "Training batch: 1781 in epoch:2, training batch loss:3.5389\n",
      "Training batch: 1791 in epoch:2, training batch loss:3.8594\n",
      "Training batch: 1801 in epoch:2, training batch loss:3.3776\n",
      "Training batch: 1811 in epoch:2, training batch loss:3.8076\n",
      "Training batch: 1821 in epoch:2, training batch loss:2.5807\n",
      "Training batch: 1831 in epoch:2, training batch loss:3.5558\n",
      "Training batch: 1841 in epoch:2, training batch loss:3.8729\n",
      "Training batch: 1851 in epoch:2, training batch loss:3.3487\n",
      "Training batch: 1861 in epoch:2, training batch loss:3.0573\n",
      "Training batch: 1871 in epoch:2, training batch loss:3.5957\n",
      "Training batch: 1881 in epoch:2, training batch loss:3.3815\n",
      "Training batch: 1891 in epoch:2, training batch loss:3.0175\n",
      "Training batch: 1901 in epoch:2, training batch loss:3.1950\n",
      "Training batch: 1911 in epoch:2, training batch loss:3.8369\n",
      "Training batch: 1921 in epoch:2, training batch loss:3.7019\n",
      "Training batch: 1931 in epoch:2, training batch loss:3.3707\n",
      "Training batch: 1941 in epoch:2, training batch loss:3.8192\n",
      "Training batch: 1951 in epoch:2, training batch loss:3.3263\n",
      "Training batch: 1961 in epoch:2, training batch loss:3.2596\n",
      "Training batch: 1971 in epoch:2, training batch loss:3.5933\n",
      "Training batch: 1981 in epoch:2, training batch loss:3.6711\n",
      "Training batch: 1991 in epoch:2, training batch loss:3.2437\n",
      "Training batch: 2001 in epoch:2, training batch loss:3.7515\n",
      "Training batch: 2011 in epoch:2, training batch loss:3.1418\n",
      "Training batch: 2021 in epoch:2, training batch loss:3.0244\n",
      "Training batch: 2031 in epoch:2, training batch loss:3.2507\n",
      "Training batch: 2041 in epoch:2, training batch loss:3.0979\n",
      "Training batch: 2051 in epoch:2, training batch loss:3.5100\n",
      "Training batch: 2061 in epoch:2, training batch loss:3.5915\n",
      "Training batch: 2071 in epoch:2, training batch loss:3.8224\n",
      "Training batch: 2081 in epoch:2, training batch loss:3.6824\n",
      "Training batch: 2091 in epoch:2, training batch loss:3.5804\n",
      "Training batch: 2101 in epoch:2, training batch loss:3.3537\n",
      "Training batch: 2111 in epoch:2, training batch loss:3.3409\n",
      "Training batch: 2121 in epoch:2, training batch loss:2.9854\n",
      "Training batch: 2131 in epoch:2, training batch loss:3.3592\n",
      "Training batch: 2141 in epoch:2, training batch loss:3.2444\n",
      "Training batch: 2151 in epoch:2, training batch loss:3.3826\n",
      "Training batch: 2161 in epoch:2, training batch loss:3.2511\n",
      "Training batch: 2171 in epoch:2, training batch loss:3.4101\n",
      "Training batch: 2181 in epoch:2, training batch loss:3.5496\n",
      "Training batch: 2191 in epoch:2, training batch loss:3.2959\n",
      "Training batch: 2201 in epoch:2, training batch loss:3.4078\n",
      "Training batch: 2211 in epoch:2, training batch loss:3.3247\n",
      "Training batch: 2221 in epoch:2, training batch loss:3.0956\n",
      "Training batch: 2231 in epoch:2, training batch loss:3.1603\n",
      "Training batch: 2241 in epoch:2, training batch loss:3.6953\n",
      "Training batch: 2251 in epoch:2, training batch loss:3.3769\n",
      "Training batch: 2261 in epoch:2, training batch loss:3.2298\n",
      "Training batch: 2271 in epoch:2, training batch loss:3.5847\n",
      "Training batch: 2281 in epoch:2, training batch loss:3.0799\n",
      "Training batch: 2291 in epoch:2, training batch loss:3.7040\n",
      "2024-09-03 17:02:07 | epoch: 0002/10, training time: 665.7s, inference time: 70.4s\n",
      "train loss: 22.4381, val_loss: 0.8528\n",
      "Training batch: 11 in epoch:3, training batch loss:3.1136\n",
      "Training batch: 21 in epoch:3, training batch loss:3.0016\n",
      "Training batch: 31 in epoch:3, training batch loss:3.4160\n",
      "Training batch: 41 in epoch:3, training batch loss:3.2885\n",
      "Training batch: 51 in epoch:3, training batch loss:3.3547\n",
      "Training batch: 61 in epoch:3, training batch loss:3.3908\n",
      "Training batch: 71 in epoch:3, training batch loss:3.6347\n",
      "Training batch: 81 in epoch:3, training batch loss:3.2518\n",
      "Training batch: 91 in epoch:3, training batch loss:3.2394\n",
      "Training batch: 101 in epoch:3, training batch loss:3.1077\n",
      "Training batch: 111 in epoch:3, training batch loss:3.2861\n",
      "Training batch: 121 in epoch:3, training batch loss:3.0805\n",
      "Training batch: 131 in epoch:3, training batch loss:3.2355\n",
      "Training batch: 141 in epoch:3, training batch loss:3.1719\n",
      "Training batch: 151 in epoch:3, training batch loss:3.3029\n",
      "Training batch: 161 in epoch:3, training batch loss:3.1348\n",
      "Training batch: 171 in epoch:3, training batch loss:3.2055\n",
      "Training batch: 181 in epoch:3, training batch loss:3.2773\n",
      "Training batch: 191 in epoch:3, training batch loss:3.0921\n",
      "Training batch: 201 in epoch:3, training batch loss:3.1774\n",
      "Training batch: 211 in epoch:3, training batch loss:3.4343\n",
      "Training batch: 221 in epoch:3, training batch loss:3.1985\n",
      "Training batch: 231 in epoch:3, training batch loss:3.1251\n",
      "Training batch: 241 in epoch:3, training batch loss:3.7006\n",
      "Training batch: 251 in epoch:3, training batch loss:3.1996\n",
      "Training batch: 261 in epoch:3, training batch loss:3.5315\n",
      "Training batch: 271 in epoch:3, training batch loss:3.3131\n",
      "Training batch: 281 in epoch:3, training batch loss:3.3856\n",
      "Training batch: 291 in epoch:3, training batch loss:3.3847\n",
      "Training batch: 301 in epoch:3, training batch loss:3.2399\n",
      "Training batch: 311 in epoch:3, training batch loss:3.0402\n",
      "Training batch: 321 in epoch:3, training batch loss:3.1310\n",
      "Training batch: 331 in epoch:3, training batch loss:3.6088\n",
      "Training batch: 341 in epoch:3, training batch loss:3.3866\n",
      "Training batch: 351 in epoch:3, training batch loss:3.1981\n",
      "Training batch: 361 in epoch:3, training batch loss:3.2208\n",
      "Training batch: 371 in epoch:3, training batch loss:3.7231\n",
      "Training batch: 381 in epoch:3, training batch loss:3.2093\n",
      "Training batch: 391 in epoch:3, training batch loss:3.1934\n",
      "Training batch: 401 in epoch:3, training batch loss:3.0686\n",
      "Training batch: 411 in epoch:3, training batch loss:3.4928\n",
      "Training batch: 421 in epoch:3, training batch loss:3.4065\n",
      "Training batch: 431 in epoch:3, training batch loss:3.3885\n",
      "Training batch: 441 in epoch:3, training batch loss:3.3677\n",
      "Training batch: 451 in epoch:3, training batch loss:3.2992\n",
      "Training batch: 461 in epoch:3, training batch loss:3.2942\n",
      "Training batch: 471 in epoch:3, training batch loss:3.3645\n",
      "Training batch: 481 in epoch:3, training batch loss:3.0793\n",
      "Training batch: 491 in epoch:3, training batch loss:3.0895\n",
      "Training batch: 501 in epoch:3, training batch loss:3.1597\n",
      "Training batch: 511 in epoch:3, training batch loss:2.2553\n",
      "Training batch: 521 in epoch:3, training batch loss:3.4107\n",
      "Training batch: 531 in epoch:3, training batch loss:3.1014\n",
      "Training batch: 541 in epoch:3, training batch loss:2.3801\n",
      "Training batch: 551 in epoch:3, training batch loss:3.2964\n",
      "Training batch: 561 in epoch:3, training batch loss:3.3083\n",
      "Training batch: 571 in epoch:3, training batch loss:3.4777\n",
      "Training batch: 581 in epoch:3, training batch loss:3.1052\n",
      "Training batch: 591 in epoch:3, training batch loss:3.2184\n",
      "Training batch: 601 in epoch:3, training batch loss:3.0994\n",
      "Training batch: 611 in epoch:3, training batch loss:3.0975\n",
      "Training batch: 621 in epoch:3, training batch loss:3.1189\n",
      "Training batch: 631 in epoch:3, training batch loss:3.1914\n",
      "Training batch: 641 in epoch:3, training batch loss:3.3131\n",
      "Training batch: 651 in epoch:3, training batch loss:3.1609\n",
      "Training batch: 661 in epoch:3, training batch loss:2.9993\n",
      "Training batch: 671 in epoch:3, training batch loss:3.1772\n",
      "Training batch: 681 in epoch:3, training batch loss:3.5270\n",
      "Training batch: 691 in epoch:3, training batch loss:3.4588\n",
      "Training batch: 701 in epoch:3, training batch loss:3.0337\n",
      "Training batch: 711 in epoch:3, training batch loss:3.1583\n",
      "Training batch: 721 in epoch:3, training batch loss:3.2170\n",
      "Training batch: 731 in epoch:3, training batch loss:3.3083\n",
      "Training batch: 741 in epoch:3, training batch loss:3.1155\n",
      "Training batch: 751 in epoch:3, training batch loss:3.2651\n",
      "Training batch: 761 in epoch:3, training batch loss:3.1405\n",
      "Training batch: 771 in epoch:3, training batch loss:3.0494\n",
      "Training batch: 781 in epoch:3, training batch loss:3.4461\n",
      "Training batch: 791 in epoch:3, training batch loss:3.1686\n",
      "Training batch: 801 in epoch:3, training batch loss:3.1328\n",
      "Training batch: 811 in epoch:3, training batch loss:3.7376\n",
      "Training batch: 821 in epoch:3, training batch loss:3.0005\n",
      "Training batch: 831 in epoch:3, training batch loss:3.2249\n",
      "Training batch: 841 in epoch:3, training batch loss:3.2560\n",
      "Training batch: 851 in epoch:3, training batch loss:3.2414\n",
      "Training batch: 861 in epoch:3, training batch loss:3.4755\n",
      "Training batch: 871 in epoch:3, training batch loss:3.0242\n",
      "Training batch: 881 in epoch:3, training batch loss:3.4616\n",
      "Training batch: 891 in epoch:3, training batch loss:3.3836\n",
      "Training batch: 901 in epoch:3, training batch loss:3.3274\n",
      "Training batch: 911 in epoch:3, training batch loss:2.6102\n",
      "Training batch: 921 in epoch:3, training batch loss:3.5094\n",
      "Training batch: 931 in epoch:3, training batch loss:3.2544\n",
      "Training batch: 941 in epoch:3, training batch loss:3.3750\n",
      "Training batch: 951 in epoch:3, training batch loss:3.2630\n",
      "Training batch: 961 in epoch:3, training batch loss:3.6466\n",
      "Training batch: 971 in epoch:3, training batch loss:3.2942\n",
      "Training batch: 981 in epoch:3, training batch loss:3.1688\n",
      "Training batch: 991 in epoch:3, training batch loss:3.1470\n",
      "Training batch: 1001 in epoch:3, training batch loss:3.1756\n",
      "Training batch: 1011 in epoch:3, training batch loss:3.7297\n",
      "Training batch: 1021 in epoch:3, training batch loss:3.2274\n",
      "Training batch: 1031 in epoch:3, training batch loss:3.2678\n",
      "Training batch: 1041 in epoch:3, training batch loss:3.5689\n",
      "Training batch: 1051 in epoch:3, training batch loss:3.3742\n",
      "Training batch: 1061 in epoch:3, training batch loss:3.2262\n",
      "Training batch: 1071 in epoch:3, training batch loss:3.1576\n",
      "Training batch: 1081 in epoch:3, training batch loss:3.1985\n",
      "Training batch: 1091 in epoch:3, training batch loss:3.3755\n",
      "Training batch: 1101 in epoch:3, training batch loss:3.5839\n",
      "Training batch: 1111 in epoch:3, training batch loss:2.9074\n",
      "Training batch: 1121 in epoch:3, training batch loss:3.1912\n",
      "Training batch: 1131 in epoch:3, training batch loss:3.2935\n",
      "Training batch: 1141 in epoch:3, training batch loss:3.3672\n",
      "Training batch: 1151 in epoch:3, training batch loss:3.1731\n",
      "Training batch: 1161 in epoch:3, training batch loss:3.1558\n",
      "Training batch: 1171 in epoch:3, training batch loss:2.9979\n",
      "Training batch: 1181 in epoch:3, training batch loss:3.2081\n",
      "Training batch: 1191 in epoch:3, training batch loss:3.4163\n",
      "Training batch: 1201 in epoch:3, training batch loss:3.0858\n",
      "Training batch: 1211 in epoch:3, training batch loss:3.2967\n",
      "Training batch: 1221 in epoch:3, training batch loss:3.1623\n",
      "Training batch: 1231 in epoch:3, training batch loss:3.1376\n",
      "Training batch: 1241 in epoch:3, training batch loss:3.0887\n",
      "Training batch: 1251 in epoch:3, training batch loss:3.4197\n",
      "Training batch: 1261 in epoch:3, training batch loss:3.4651\n",
      "Training batch: 1271 in epoch:3, training batch loss:3.0047\n",
      "Training batch: 1281 in epoch:3, training batch loss:3.0454\n",
      "Training batch: 1291 in epoch:3, training batch loss:3.3634\n",
      "Training batch: 1301 in epoch:3, training batch loss:2.9735\n",
      "Training batch: 1311 in epoch:3, training batch loss:3.4149\n",
      "Training batch: 1321 in epoch:3, training batch loss:2.8940\n",
      "Training batch: 1331 in epoch:3, training batch loss:3.3741\n",
      "Training batch: 1341 in epoch:3, training batch loss:3.2624\n",
      "Training batch: 1351 in epoch:3, training batch loss:3.3501\n",
      "Training batch: 1361 in epoch:3, training batch loss:3.2270\n",
      "Training batch: 1371 in epoch:3, training batch loss:3.2944\n",
      "Training batch: 1381 in epoch:3, training batch loss:3.0698\n",
      "Training batch: 1391 in epoch:3, training batch loss:3.2454\n",
      "Training batch: 1401 in epoch:3, training batch loss:3.2335\n",
      "Training batch: 1411 in epoch:3, training batch loss:3.2253\n",
      "Training batch: 1421 in epoch:3, training batch loss:3.2185\n",
      "Training batch: 1431 in epoch:3, training batch loss:3.0056\n",
      "Training batch: 1441 in epoch:3, training batch loss:3.0830\n",
      "Training batch: 1451 in epoch:3, training batch loss:3.0998\n",
      "Training batch: 1461 in epoch:3, training batch loss:3.1005\n",
      "Training batch: 1471 in epoch:3, training batch loss:3.3461\n",
      "Training batch: 1481 in epoch:3, training batch loss:3.2797\n",
      "Training batch: 1491 in epoch:3, training batch loss:3.5754\n",
      "Training batch: 1501 in epoch:3, training batch loss:3.4122\n",
      "Training batch: 1511 in epoch:3, training batch loss:3.0563\n",
      "Training batch: 1521 in epoch:3, training batch loss:3.3890\n",
      "Training batch: 1531 in epoch:3, training batch loss:3.2853\n",
      "Training batch: 1541 in epoch:3, training batch loss:3.1307\n",
      "Training batch: 1551 in epoch:3, training batch loss:3.6929\n",
      "Training batch: 1561 in epoch:3, training batch loss:3.2738\n",
      "Training batch: 1571 in epoch:3, training batch loss:3.0795\n",
      "Training batch: 1581 in epoch:3, training batch loss:3.7438\n",
      "Training batch: 1591 in epoch:3, training batch loss:3.3746\n",
      "Training batch: 1601 in epoch:3, training batch loss:3.1049\n",
      "Training batch: 1611 in epoch:3, training batch loss:3.2528\n",
      "Training batch: 1621 in epoch:3, training batch loss:3.2797\n",
      "Training batch: 1631 in epoch:3, training batch loss:3.0518\n",
      "Training batch: 1641 in epoch:3, training batch loss:3.0624\n",
      "Training batch: 1651 in epoch:3, training batch loss:3.1776\n",
      "Training batch: 1661 in epoch:3, training batch loss:3.2174\n",
      "Training batch: 1671 in epoch:3, training batch loss:3.2246\n",
      "Training batch: 1681 in epoch:3, training batch loss:3.5080\n",
      "Training batch: 1691 in epoch:3, training batch loss:3.3019\n",
      "Training batch: 1701 in epoch:3, training batch loss:3.3398\n",
      "Training batch: 1711 in epoch:3, training batch loss:3.4657\n",
      "Training batch: 1721 in epoch:3, training batch loss:3.0550\n",
      "Training batch: 1731 in epoch:3, training batch loss:3.5803\n",
      "Training batch: 1741 in epoch:3, training batch loss:3.3562\n",
      "Training batch: 1751 in epoch:3, training batch loss:3.6012\n",
      "Training batch: 1761 in epoch:3, training batch loss:3.1821\n",
      "Training batch: 1771 in epoch:3, training batch loss:3.0984\n",
      "Training batch: 1781 in epoch:3, training batch loss:2.9092\n",
      "Training batch: 1791 in epoch:3, training batch loss:3.1575\n",
      "Training batch: 1801 in epoch:3, training batch loss:3.2011\n",
      "Training batch: 1811 in epoch:3, training batch loss:3.3973\n",
      "Training batch: 1821 in epoch:3, training batch loss:3.5394\n",
      "Training batch: 1831 in epoch:3, training batch loss:3.6064\n",
      "Training batch: 1841 in epoch:3, training batch loss:3.3259\n",
      "Training batch: 1851 in epoch:3, training batch loss:3.3556\n",
      "Training batch: 1861 in epoch:3, training batch loss:3.1174\n",
      "Training batch: 1871 in epoch:3, training batch loss:3.6802\n",
      "Training batch: 1881 in epoch:3, training batch loss:3.2593\n",
      "Training batch: 1891 in epoch:3, training batch loss:3.0245\n",
      "Training batch: 1901 in epoch:3, training batch loss:3.5557\n",
      "Training batch: 1911 in epoch:3, training batch loss:3.5763\n",
      "Training batch: 1921 in epoch:3, training batch loss:3.3458\n",
      "Training batch: 1931 in epoch:3, training batch loss:3.0905\n",
      "Training batch: 1941 in epoch:3, training batch loss:3.3529\n",
      "Training batch: 1951 in epoch:3, training batch loss:3.0218\n",
      "Training batch: 1961 in epoch:3, training batch loss:3.2455\n",
      "Training batch: 1971 in epoch:3, training batch loss:3.4910\n",
      "Training batch: 1981 in epoch:3, training batch loss:3.4264\n",
      "Training batch: 1991 in epoch:3, training batch loss:3.2539\n",
      "Training batch: 2001 in epoch:3, training batch loss:3.1208\n",
      "Training batch: 2011 in epoch:3, training batch loss:2.9650\n",
      "Training batch: 2021 in epoch:3, training batch loss:3.4459\n",
      "Training batch: 2031 in epoch:3, training batch loss:3.0339\n",
      "Training batch: 2041 in epoch:3, training batch loss:3.5417\n",
      "Training batch: 2051 in epoch:3, training batch loss:2.8846\n",
      "Training batch: 2061 in epoch:3, training batch loss:3.2865\n",
      "Training batch: 2071 in epoch:3, training batch loss:3.5333\n",
      "Training batch: 2081 in epoch:3, training batch loss:3.0996\n",
      "Training batch: 2091 in epoch:3, training batch loss:2.9661\n",
      "Training batch: 2101 in epoch:3, training batch loss:3.1061\n",
      "Training batch: 2111 in epoch:3, training batch loss:3.0848\n",
      "Training batch: 2121 in epoch:3, training batch loss:3.1722\n",
      "Training batch: 2131 in epoch:3, training batch loss:3.2790\n",
      "Training batch: 2141 in epoch:3, training batch loss:3.0847\n",
      "Training batch: 2151 in epoch:3, training batch loss:3.1584\n",
      "Training batch: 2161 in epoch:3, training batch loss:3.1899\n",
      "Training batch: 2171 in epoch:3, training batch loss:3.1190\n",
      "Training batch: 2181 in epoch:3, training batch loss:3.3947\n",
      "Training batch: 2191 in epoch:3, training batch loss:3.2008\n",
      "Training batch: 2201 in epoch:3, training batch loss:3.2981\n",
      "Training batch: 2211 in epoch:3, training batch loss:3.4742\n",
      "Training batch: 2221 in epoch:3, training batch loss:3.3023\n",
      "Training batch: 2231 in epoch:3, training batch loss:3.2287\n",
      "Training batch: 2241 in epoch:3, training batch loss:3.2826\n",
      "Training batch: 2251 in epoch:3, training batch loss:3.1502\n",
      "Training batch: 2261 in epoch:3, training batch loss:3.2975\n",
      "Training batch: 2271 in epoch:3, training batch loss:3.3625\n",
      "Training batch: 2281 in epoch:3, training batch loss:3.4164\n",
      "Training batch: 2291 in epoch:3, training batch loss:3.0308\n",
      "2024-09-03 17:14:20 | epoch: 0003/10, training time: 663.0s, inference time: 70.8s\n",
      "train loss: 21.3058, val_loss: 0.8506\n",
      "val loss decrease from 0.8526 to 0.8506, saving model to GGNN_33_step12_height_type.pkl\n",
      "Training batch: 11 in epoch:4, training batch loss:3.1641\n",
      "Training batch: 21 in epoch:4, training batch loss:2.7937\n",
      "Training batch: 31 in epoch:4, training batch loss:3.0231\n",
      "Training batch: 41 in epoch:4, training batch loss:3.0257\n",
      "Training batch: 51 in epoch:4, training batch loss:3.2484\n",
      "Training batch: 61 in epoch:4, training batch loss:3.2374\n",
      "Training batch: 71 in epoch:4, training batch loss:3.2408\n",
      "Training batch: 81 in epoch:4, training batch loss:3.1957\n",
      "Training batch: 91 in epoch:4, training batch loss:3.5126\n",
      "Training batch: 101 in epoch:4, training batch loss:3.1446\n",
      "Training batch: 111 in epoch:4, training batch loss:2.9811\n",
      "Training batch: 121 in epoch:4, training batch loss:2.8529\n",
      "Training batch: 131 in epoch:4, training batch loss:2.9986\n",
      "Training batch: 141 in epoch:4, training batch loss:3.1654\n",
      "Training batch: 151 in epoch:4, training batch loss:3.2764\n",
      "Training batch: 161 in epoch:4, training batch loss:3.2475\n",
      "Training batch: 171 in epoch:4, training batch loss:3.2023\n",
      "Training batch: 181 in epoch:4, training batch loss:3.1308\n",
      "Training batch: 191 in epoch:4, training batch loss:3.0104\n",
      "Training batch: 201 in epoch:4, training batch loss:3.5049\n",
      "Training batch: 211 in epoch:4, training batch loss:3.0743\n",
      "Training batch: 221 in epoch:4, training batch loss:3.0616\n",
      "Training batch: 231 in epoch:4, training batch loss:3.2163\n",
      "Training batch: 241 in epoch:4, training batch loss:2.9662\n",
      "Training batch: 251 in epoch:4, training batch loss:3.6116\n",
      "Training batch: 261 in epoch:4, training batch loss:3.0956\n",
      "Training batch: 271 in epoch:4, training batch loss:2.7791\n",
      "Training batch: 281 in epoch:4, training batch loss:2.9439\n",
      "Training batch: 291 in epoch:4, training batch loss:3.5425\n",
      "Training batch: 301 in epoch:4, training batch loss:3.4642\n",
      "Training batch: 311 in epoch:4, training batch loss:2.9088\n",
      "Training batch: 321 in epoch:4, training batch loss:3.0172\n",
      "Training batch: 331 in epoch:4, training batch loss:3.2586\n",
      "Training batch: 341 in epoch:4, training batch loss:3.1987\n",
      "Training batch: 351 in epoch:4, training batch loss:3.5280\n",
      "Training batch: 361 in epoch:4, training batch loss:2.3764\n",
      "Training batch: 371 in epoch:4, training batch loss:3.3321\n",
      "Training batch: 381 in epoch:4, training batch loss:3.3257\n",
      "Training batch: 391 in epoch:4, training batch loss:3.4810\n",
      "Training batch: 401 in epoch:4, training batch loss:2.8875\n",
      "Training batch: 411 in epoch:4, training batch loss:3.1774\n",
      "Training batch: 421 in epoch:4, training batch loss:3.3463\n",
      "Training batch: 431 in epoch:4, training batch loss:3.1369\n",
      "Training batch: 441 in epoch:4, training batch loss:3.1620\n",
      "Training batch: 451 in epoch:4, training batch loss:3.2286\n",
      "Training batch: 461 in epoch:4, training batch loss:3.0272\n",
      "Training batch: 471 in epoch:4, training batch loss:3.2416\n",
      "Training batch: 481 in epoch:4, training batch loss:3.3761\n",
      "Training batch: 491 in epoch:4, training batch loss:3.0728\n",
      "Training batch: 501 in epoch:4, training batch loss:3.3351\n",
      "Training batch: 511 in epoch:4, training batch loss:3.0134\n",
      "Training batch: 521 in epoch:4, training batch loss:3.0343\n",
      "Training batch: 531 in epoch:4, training batch loss:2.9263\n",
      "Training batch: 541 in epoch:4, training batch loss:3.2253\n",
      "Training batch: 551 in epoch:4, training batch loss:3.0804\n",
      "Training batch: 561 in epoch:4, training batch loss:3.2528\n",
      "Training batch: 571 in epoch:4, training batch loss:3.2373\n",
      "Training batch: 581 in epoch:4, training batch loss:3.1880\n",
      "Training batch: 591 in epoch:4, training batch loss:3.1127\n",
      "Training batch: 601 in epoch:4, training batch loss:3.2746\n",
      "Training batch: 611 in epoch:4, training batch loss:3.3368\n",
      "Training batch: 621 in epoch:4, training batch loss:3.1893\n",
      "Training batch: 631 in epoch:4, training batch loss:3.0946\n",
      "Training batch: 641 in epoch:4, training batch loss:3.3152\n",
      "Training batch: 651 in epoch:4, training batch loss:2.9575\n",
      "Training batch: 661 in epoch:4, training batch loss:3.5237\n",
      "Training batch: 671 in epoch:4, training batch loss:3.0996\n",
      "Training batch: 681 in epoch:4, training batch loss:3.4173\n",
      "Training batch: 691 in epoch:4, training batch loss:2.9717\n",
      "Training batch: 701 in epoch:4, training batch loss:3.1176\n",
      "Training batch: 711 in epoch:4, training batch loss:3.3346\n",
      "Training batch: 721 in epoch:4, training batch loss:3.1312\n",
      "Training batch: 731 in epoch:4, training batch loss:3.0968\n",
      "Training batch: 741 in epoch:4, training batch loss:3.0021\n",
      "Training batch: 751 in epoch:4, training batch loss:3.1233\n",
      "Training batch: 761 in epoch:4, training batch loss:3.0920\n",
      "Training batch: 771 in epoch:4, training batch loss:3.4150\n",
      "Training batch: 781 in epoch:4, training batch loss:2.9312\n",
      "Training batch: 791 in epoch:4, training batch loss:3.3814\n",
      "Training batch: 801 in epoch:4, training batch loss:2.9723\n",
      "Training batch: 811 in epoch:4, training batch loss:3.1592\n",
      "Training batch: 821 in epoch:4, training batch loss:3.1069\n",
      "Training batch: 831 in epoch:4, training batch loss:3.2720\n",
      "Training batch: 841 in epoch:4, training batch loss:2.8467\n",
      "Training batch: 851 in epoch:4, training batch loss:3.1381\n",
      "Training batch: 861 in epoch:4, training batch loss:3.1344\n",
      "Training batch: 871 in epoch:4, training batch loss:3.0773\n",
      "Training batch: 881 in epoch:4, training batch loss:3.1005\n",
      "Training batch: 891 in epoch:4, training batch loss:3.1726\n",
      "Training batch: 901 in epoch:4, training batch loss:3.1890\n",
      "Training batch: 911 in epoch:4, training batch loss:3.3863\n",
      "Training batch: 921 in epoch:4, training batch loss:3.0312\n",
      "Training batch: 931 in epoch:4, training batch loss:3.0034\n",
      "Training batch: 941 in epoch:4, training batch loss:3.2520\n",
      "Training batch: 951 in epoch:4, training batch loss:3.1609\n",
      "Training batch: 961 in epoch:4, training batch loss:3.2042\n",
      "Training batch: 971 in epoch:4, training batch loss:3.0742\n",
      "Training batch: 981 in epoch:4, training batch loss:3.3456\n",
      "Training batch: 991 in epoch:4, training batch loss:3.3326\n",
      "Training batch: 1001 in epoch:4, training batch loss:3.0253\n",
      "Training batch: 1011 in epoch:4, training batch loss:3.4320\n",
      "Training batch: 1021 in epoch:4, training batch loss:3.0717\n",
      "Training batch: 1031 in epoch:4, training batch loss:3.2593\n",
      "Training batch: 1041 in epoch:4, training batch loss:3.2223\n",
      "Training batch: 1051 in epoch:4, training batch loss:3.1207\n",
      "Training batch: 1061 in epoch:4, training batch loss:3.2666\n",
      "Training batch: 1071 in epoch:4, training batch loss:3.4106\n",
      "Training batch: 1081 in epoch:4, training batch loss:3.0245\n",
      "Training batch: 1091 in epoch:4, training batch loss:3.4517\n",
      "Training batch: 1101 in epoch:4, training batch loss:2.8602\n",
      "Training batch: 1111 in epoch:4, training batch loss:3.2737\n",
      "Training batch: 1121 in epoch:4, training batch loss:2.9851\n",
      "Training batch: 1131 in epoch:4, training batch loss:2.9239\n",
      "Training batch: 1141 in epoch:4, training batch loss:3.1567\n",
      "Training batch: 1151 in epoch:4, training batch loss:2.9628\n",
      "Training batch: 1161 in epoch:4, training batch loss:3.2054\n",
      "Training batch: 1171 in epoch:4, training batch loss:3.0780\n",
      "Training batch: 1181 in epoch:4, training batch loss:3.0039\n",
      "Training batch: 1191 in epoch:4, training batch loss:3.2758\n",
      "Training batch: 1201 in epoch:4, training batch loss:3.0904\n",
      "Training batch: 1211 in epoch:4, training batch loss:3.2645\n",
      "Training batch: 1221 in epoch:4, training batch loss:3.0790\n",
      "Training batch: 1231 in epoch:4, training batch loss:3.3163\n",
      "Training batch: 1241 in epoch:4, training batch loss:3.1419\n",
      "Training batch: 1251 in epoch:4, training batch loss:3.1316\n",
      "Training batch: 1261 in epoch:4, training batch loss:2.9069\n",
      "Training batch: 1271 in epoch:4, training batch loss:3.0222\n",
      "Training batch: 1281 in epoch:4, training batch loss:3.0852\n",
      "Training batch: 1291 in epoch:4, training batch loss:3.2235\n",
      "Training batch: 1301 in epoch:4, training batch loss:3.3154\n",
      "Training batch: 1311 in epoch:4, training batch loss:3.0778\n",
      "Training batch: 1321 in epoch:4, training batch loss:3.0774\n",
      "Training batch: 1331 in epoch:4, training batch loss:3.0573\n",
      "Training batch: 1341 in epoch:4, training batch loss:3.0252\n",
      "Training batch: 1351 in epoch:4, training batch loss:3.0837\n",
      "Training batch: 1361 in epoch:4, training batch loss:2.9527\n",
      "Training batch: 1371 in epoch:4, training batch loss:3.0212\n",
      "Training batch: 1381 in epoch:4, training batch loss:3.2179\n",
      "Training batch: 1391 in epoch:4, training batch loss:2.8904\n",
      "Training batch: 1401 in epoch:4, training batch loss:3.2882\n",
      "Training batch: 1411 in epoch:4, training batch loss:3.1117\n",
      "Training batch: 1421 in epoch:4, training batch loss:3.3724\n",
      "Training batch: 1431 in epoch:4, training batch loss:2.9319\n",
      "Training batch: 1441 in epoch:4, training batch loss:3.0218\n",
      "Training batch: 1451 in epoch:4, training batch loss:3.2902\n",
      "Training batch: 1461 in epoch:4, training batch loss:3.1776\n",
      "Training batch: 1471 in epoch:4, training batch loss:2.9736\n",
      "Training batch: 1481 in epoch:4, training batch loss:3.1121\n",
      "Training batch: 1491 in epoch:4, training batch loss:2.9605\n",
      "Training batch: 1501 in epoch:4, training batch loss:2.4884\n",
      "Training batch: 1511 in epoch:4, training batch loss:3.1980\n",
      "Training batch: 1521 in epoch:4, training batch loss:3.5512\n",
      "Training batch: 1531 in epoch:4, training batch loss:2.8808\n",
      "Training batch: 1541 in epoch:4, training batch loss:3.0689\n",
      "Training batch: 1551 in epoch:4, training batch loss:2.9602\n",
      "Training batch: 1561 in epoch:4, training batch loss:2.8668\n",
      "Training batch: 1571 in epoch:4, training batch loss:3.1754\n",
      "Training batch: 1581 in epoch:4, training batch loss:3.0002\n",
      "Training batch: 1591 in epoch:4, training batch loss:2.9527\n",
      "Training batch: 1601 in epoch:4, training batch loss:3.0840\n",
      "Training batch: 1611 in epoch:4, training batch loss:2.9843\n",
      "Training batch: 1621 in epoch:4, training batch loss:3.0813\n",
      "Training batch: 1631 in epoch:4, training batch loss:3.2393\n",
      "Training batch: 1641 in epoch:4, training batch loss:3.4674\n",
      "Training batch: 1651 in epoch:4, training batch loss:2.9186\n",
      "Training batch: 1661 in epoch:4, training batch loss:3.0551\n",
      "Training batch: 1671 in epoch:4, training batch loss:2.9155\n",
      "Training batch: 1681 in epoch:4, training batch loss:3.2798\n",
      "Training batch: 1691 in epoch:4, training batch loss:3.2364\n",
      "Training batch: 1701 in epoch:4, training batch loss:3.3950\n",
      "Training batch: 1711 in epoch:4, training batch loss:3.3137\n",
      "Training batch: 1721 in epoch:4, training batch loss:3.5852\n",
      "Training batch: 1731 in epoch:4, training batch loss:3.0362\n",
      "Training batch: 1741 in epoch:4, training batch loss:3.5751\n",
      "Training batch: 1751 in epoch:4, training batch loss:3.1912\n",
      "Training batch: 1761 in epoch:4, training batch loss:3.3939\n",
      "Training batch: 1771 in epoch:4, training batch loss:3.1499\n",
      "Training batch: 1781 in epoch:4, training batch loss:3.0528\n",
      "Training batch: 1791 in epoch:4, training batch loss:3.0786\n",
      "Training batch: 1801 in epoch:4, training batch loss:3.0358\n",
      "Training batch: 1811 in epoch:4, training batch loss:3.1666\n",
      "Training batch: 1821 in epoch:4, training batch loss:3.2220\n",
      "Training batch: 1831 in epoch:4, training batch loss:3.2989\n",
      "Training batch: 1841 in epoch:4, training batch loss:3.3130\n",
      "Training batch: 1851 in epoch:4, training batch loss:3.1472\n",
      "Training batch: 1861 in epoch:4, training batch loss:3.1661\n",
      "Training batch: 1871 in epoch:4, training batch loss:3.2230\n",
      "Training batch: 1881 in epoch:4, training batch loss:3.1130\n",
      "Training batch: 1891 in epoch:4, training batch loss:3.2797\n",
      "Training batch: 1901 in epoch:4, training batch loss:3.1108\n",
      "Training batch: 1911 in epoch:4, training batch loss:3.2312\n",
      "Training batch: 1921 in epoch:4, training batch loss:3.1697\n",
      "Training batch: 1931 in epoch:4, training batch loss:3.0415\n",
      "Training batch: 1941 in epoch:4, training batch loss:2.9974\n",
      "Training batch: 1951 in epoch:4, training batch loss:3.2588\n",
      "Training batch: 1961 in epoch:4, training batch loss:3.5222\n",
      "Training batch: 1971 in epoch:4, training batch loss:3.1341\n",
      "Training batch: 1981 in epoch:4, training batch loss:3.1919\n",
      "Training batch: 1991 in epoch:4, training batch loss:3.0560\n",
      "Training batch: 2001 in epoch:4, training batch loss:3.0981\n",
      "Training batch: 2011 in epoch:4, training batch loss:2.9208\n",
      "Training batch: 2021 in epoch:4, training batch loss:2.9934\n",
      "Training batch: 2031 in epoch:4, training batch loss:3.0563\n",
      "Training batch: 2041 in epoch:4, training batch loss:3.0961\n",
      "Training batch: 2051 in epoch:4, training batch loss:3.1282\n",
      "Training batch: 2061 in epoch:4, training batch loss:2.7859\n",
      "Training batch: 2071 in epoch:4, training batch loss:3.2903\n",
      "Training batch: 2081 in epoch:4, training batch loss:2.9457\n",
      "Training batch: 2091 in epoch:4, training batch loss:2.9426\n",
      "Training batch: 2101 in epoch:4, training batch loss:3.3862\n",
      "Training batch: 2111 in epoch:4, training batch loss:3.2114\n",
      "Training batch: 2121 in epoch:4, training batch loss:3.0960\n",
      "Training batch: 2131 in epoch:4, training batch loss:3.0882\n",
      "Training batch: 2141 in epoch:4, training batch loss:3.1810\n",
      "Training batch: 2151 in epoch:4, training batch loss:3.1778\n",
      "Training batch: 2161 in epoch:4, training batch loss:3.1258\n",
      "Training batch: 2171 in epoch:4, training batch loss:3.4433\n",
      "Training batch: 2181 in epoch:4, training batch loss:3.1027\n",
      "Training batch: 2191 in epoch:4, training batch loss:3.2513\n",
      "Training batch: 2201 in epoch:4, training batch loss:2.9280\n",
      "Training batch: 2211 in epoch:4, training batch loss:3.4168\n",
      "Training batch: 2221 in epoch:4, training batch loss:3.2747\n",
      "Training batch: 2231 in epoch:4, training batch loss:3.1555\n",
      "Training batch: 2241 in epoch:4, training batch loss:3.1624\n",
      "Training batch: 2251 in epoch:4, training batch loss:3.1496\n",
      "Training batch: 2261 in epoch:4, training batch loss:3.0425\n",
      "Training batch: 2271 in epoch:4, training batch loss:3.3522\n",
      "Training batch: 2281 in epoch:4, training batch loss:3.0641\n",
      "Training batch: 2291 in epoch:4, training batch loss:2.9821\n",
      "2024-09-03 17:26:36 | epoch: 0004/10, training time: 665.2s, inference time: 70.2s\n",
      "train loss: 20.5034, val_loss: 0.8510\n",
      "Training batch: 11 in epoch:5, training batch loss:3.0359\n",
      "Training batch: 21 in epoch:5, training batch loss:3.2057\n",
      "Training batch: 31 in epoch:5, training batch loss:2.9494\n",
      "Training batch: 41 in epoch:5, training batch loss:2.9533\n",
      "Training batch: 51 in epoch:5, training batch loss:3.2423\n",
      "Training batch: 61 in epoch:5, training batch loss:3.1024\n",
      "Training batch: 71 in epoch:5, training batch loss:2.9563\n",
      "Training batch: 81 in epoch:5, training batch loss:3.1032\n",
      "Training batch: 91 in epoch:5, training batch loss:2.8837\n",
      "Training batch: 101 in epoch:5, training batch loss:3.2223\n",
      "Training batch: 111 in epoch:5, training batch loss:3.3024\n",
      "Training batch: 121 in epoch:5, training batch loss:3.1029\n",
      "Training batch: 131 in epoch:5, training batch loss:2.9805\n",
      "Training batch: 141 in epoch:5, training batch loss:2.9998\n",
      "Training batch: 151 in epoch:5, training batch loss:2.9527\n",
      "Training batch: 161 in epoch:5, training batch loss:2.8811\n",
      "Training batch: 171 in epoch:5, training batch loss:3.1765\n",
      "Training batch: 181 in epoch:5, training batch loss:2.8825\n",
      "Training batch: 191 in epoch:5, training batch loss:2.8752\n",
      "Training batch: 201 in epoch:5, training batch loss:3.0272\n",
      "Training batch: 211 in epoch:5, training batch loss:2.9516\n",
      "Training batch: 221 in epoch:5, training batch loss:3.1306\n",
      "Training batch: 231 in epoch:5, training batch loss:2.9398\n",
      "Training batch: 241 in epoch:5, training batch loss:2.9787\n",
      "Training batch: 251 in epoch:5, training batch loss:2.9393\n",
      "Training batch: 261 in epoch:5, training batch loss:3.1909\n",
      "Training batch: 271 in epoch:5, training batch loss:2.9786\n",
      "Training batch: 281 in epoch:5, training batch loss:3.1691\n",
      "Training batch: 291 in epoch:5, training batch loss:3.0388\n",
      "Training batch: 301 in epoch:5, training batch loss:2.7848\n",
      "Training batch: 311 in epoch:5, training batch loss:3.0981\n",
      "Training batch: 321 in epoch:5, training batch loss:3.3118\n",
      "Training batch: 331 in epoch:5, training batch loss:3.1202\n",
      "Training batch: 341 in epoch:5, training batch loss:2.7905\n",
      "Training batch: 351 in epoch:5, training batch loss:2.9307\n",
      "Training batch: 361 in epoch:5, training batch loss:3.1540\n",
      "Training batch: 371 in epoch:5, training batch loss:3.0668\n",
      "Training batch: 381 in epoch:5, training batch loss:2.9859\n",
      "Training batch: 391 in epoch:5, training batch loss:2.8702\n",
      "Training batch: 401 in epoch:5, training batch loss:3.0482\n",
      "Training batch: 411 in epoch:5, training batch loss:3.2152\n",
      "Training batch: 421 in epoch:5, training batch loss:2.9826\n",
      "Training batch: 431 in epoch:5, training batch loss:3.0922\n",
      "Training batch: 441 in epoch:5, training batch loss:3.0821\n",
      "Training batch: 451 in epoch:5, training batch loss:3.0999\n",
      "Training batch: 461 in epoch:5, training batch loss:3.2346\n",
      "Training batch: 471 in epoch:5, training batch loss:3.3111\n",
      "Training batch: 481 in epoch:5, training batch loss:3.3074\n",
      "Training batch: 491 in epoch:5, training batch loss:2.9983\n",
      "Training batch: 501 in epoch:5, training batch loss:3.0627\n",
      "Training batch: 511 in epoch:5, training batch loss:3.1013\n",
      "Training batch: 521 in epoch:5, training batch loss:3.2514\n",
      "Training batch: 531 in epoch:5, training batch loss:2.9901\n",
      "Training batch: 541 in epoch:5, training batch loss:3.0050\n",
      "Training batch: 551 in epoch:5, training batch loss:3.2380\n",
      "Training batch: 561 in epoch:5, training batch loss:2.8053\n",
      "Training batch: 571 in epoch:5, training batch loss:3.3499\n",
      "Training batch: 581 in epoch:5, training batch loss:2.9344\n",
      "Training batch: 591 in epoch:5, training batch loss:3.0209\n",
      "Training batch: 601 in epoch:5, training batch loss:3.2041\n",
      "Training batch: 611 in epoch:5, training batch loss:2.9416\n",
      "Training batch: 621 in epoch:5, training batch loss:3.0482\n",
      "Training batch: 631 in epoch:5, training batch loss:3.0499\n",
      "Training batch: 641 in epoch:5, training batch loss:2.9786\n",
      "Training batch: 651 in epoch:5, training batch loss:2.9745\n",
      "Training batch: 661 in epoch:5, training batch loss:3.0420\n",
      "Training batch: 671 in epoch:5, training batch loss:3.0914\n",
      "Training batch: 681 in epoch:5, training batch loss:2.9656\n",
      "Training batch: 691 in epoch:5, training batch loss:2.9933\n",
      "Training batch: 701 in epoch:5, training batch loss:3.2299\n",
      "Training batch: 711 in epoch:5, training batch loss:2.9042\n",
      "Training batch: 721 in epoch:5, training batch loss:2.2951\n",
      "Training batch: 731 in epoch:5, training batch loss:3.0452\n",
      "Training batch: 741 in epoch:5, training batch loss:2.8720\n",
      "Training batch: 751 in epoch:5, training batch loss:2.9717\n",
      "Training batch: 761 in epoch:5, training batch loss:3.1416\n",
      "Training batch: 771 in epoch:5, training batch loss:2.9499\n",
      "Training batch: 781 in epoch:5, training batch loss:3.0304\n",
      "Training batch: 791 in epoch:5, training batch loss:3.1722\n",
      "Training batch: 801 in epoch:5, training batch loss:2.9787\n",
      "Training batch: 811 in epoch:5, training batch loss:3.1023\n",
      "Training batch: 821 in epoch:5, training batch loss:2.9404\n",
      "Training batch: 831 in epoch:5, training batch loss:2.9002\n",
      "Training batch: 841 in epoch:5, training batch loss:3.1266\n",
      "Training batch: 851 in epoch:5, training batch loss:2.9879\n",
      "Training batch: 861 in epoch:5, training batch loss:2.9376\n",
      "Training batch: 871 in epoch:5, training batch loss:3.0704\n",
      "Training batch: 881 in epoch:5, training batch loss:2.9342\n",
      "Training batch: 891 in epoch:5, training batch loss:3.0771\n",
      "Training batch: 901 in epoch:5, training batch loss:3.3079\n",
      "Training batch: 911 in epoch:5, training batch loss:2.8703\n",
      "Training batch: 921 in epoch:5, training batch loss:3.0251\n",
      "Training batch: 931 in epoch:5, training batch loss:2.9473\n",
      "Training batch: 941 in epoch:5, training batch loss:3.1224\n",
      "Training batch: 951 in epoch:5, training batch loss:3.0520\n",
      "Training batch: 961 in epoch:5, training batch loss:3.0318\n",
      "Training batch: 971 in epoch:5, training batch loss:2.9978\n",
      "Training batch: 981 in epoch:5, training batch loss:3.2669\n",
      "Training batch: 991 in epoch:5, training batch loss:3.0772\n",
      "Training batch: 1001 in epoch:5, training batch loss:2.8879\n",
      "Training batch: 1011 in epoch:5, training batch loss:3.0723\n",
      "Training batch: 1021 in epoch:5, training batch loss:2.9538\n",
      "Training batch: 1031 in epoch:5, training batch loss:2.8943\n",
      "Training batch: 1041 in epoch:5, training batch loss:3.0171\n",
      "Training batch: 1051 in epoch:5, training batch loss:2.8529\n",
      "Training batch: 1061 in epoch:5, training batch loss:2.8892\n",
      "Training batch: 1071 in epoch:5, training batch loss:3.1471\n",
      "Training batch: 1081 in epoch:5, training batch loss:3.2969\n",
      "Training batch: 1091 in epoch:5, training batch loss:3.0619\n",
      "Training batch: 1101 in epoch:5, training batch loss:3.1000\n",
      "Training batch: 1111 in epoch:5, training batch loss:3.0792\n",
      "Training batch: 1121 in epoch:5, training batch loss:2.9966\n",
      "Training batch: 1131 in epoch:5, training batch loss:3.0690\n",
      "Training batch: 1141 in epoch:5, training batch loss:3.0990\n",
      "Training batch: 1151 in epoch:5, training batch loss:3.0926\n",
      "Training batch: 1161 in epoch:5, training batch loss:3.1379\n",
      "Training batch: 1171 in epoch:5, training batch loss:3.0143\n",
      "Training batch: 1181 in epoch:5, training batch loss:3.2011\n",
      "Training batch: 1191 in epoch:5, training batch loss:2.8743\n",
      "Training batch: 1201 in epoch:5, training batch loss:3.3635\n",
      "Training batch: 1211 in epoch:5, training batch loss:2.9408\n",
      "Training batch: 1221 in epoch:5, training batch loss:3.1559\n",
      "Training batch: 1231 in epoch:5, training batch loss:3.2157\n",
      "Training batch: 1241 in epoch:5, training batch loss:2.8640\n",
      "Training batch: 1251 in epoch:5, training batch loss:3.1375\n",
      "Training batch: 1261 in epoch:5, training batch loss:3.1176\n",
      "Training batch: 1271 in epoch:5, training batch loss:2.8896\n",
      "Training batch: 1281 in epoch:5, training batch loss:2.9443\n",
      "Training batch: 1291 in epoch:5, training batch loss:2.9435\n",
      "Training batch: 1301 in epoch:5, training batch loss:3.0907\n",
      "Training batch: 1311 in epoch:5, training batch loss:3.0817\n",
      "Training batch: 1321 in epoch:5, training batch loss:3.1012\n",
      "Training batch: 1331 in epoch:5, training batch loss:2.8833\n",
      "Training batch: 1341 in epoch:5, training batch loss:3.1322\n",
      "Training batch: 1351 in epoch:5, training batch loss:3.0660\n",
      "Training batch: 1361 in epoch:5, training batch loss:3.0744\n",
      "Training batch: 1371 in epoch:5, training batch loss:3.1332\n",
      "Training batch: 1381 in epoch:5, training batch loss:3.4500\n",
      "Training batch: 1391 in epoch:5, training batch loss:2.8980\n",
      "Training batch: 1401 in epoch:5, training batch loss:3.0323\n",
      "Training batch: 1411 in epoch:5, training batch loss:2.3697\n",
      "Training batch: 1421 in epoch:5, training batch loss:3.0175\n",
      "Training batch: 1431 in epoch:5, training batch loss:3.0505\n",
      "Training batch: 1441 in epoch:5, training batch loss:3.2178\n",
      "Training batch: 1451 in epoch:5, training batch loss:2.9298\n",
      "Training batch: 1461 in epoch:5, training batch loss:2.9157\n",
      "Training batch: 1471 in epoch:5, training batch loss:3.0308\n",
      "Training batch: 1481 in epoch:5, training batch loss:3.0062\n",
      "Training batch: 1491 in epoch:5, training batch loss:3.1235\n",
      "Training batch: 1501 in epoch:5, training batch loss:3.2096\n",
      "Training batch: 1511 in epoch:5, training batch loss:2.9639\n",
      "Training batch: 1521 in epoch:5, training batch loss:3.2084\n",
      "Training batch: 1531 in epoch:5, training batch loss:3.1068\n",
      "Training batch: 1541 in epoch:5, training batch loss:3.0258\n",
      "Training batch: 1551 in epoch:5, training batch loss:3.1112\n",
      "Training batch: 1561 in epoch:5, training batch loss:3.1158\n",
      "Training batch: 1571 in epoch:5, training batch loss:3.0758\n",
      "Training batch: 1581 in epoch:5, training batch loss:2.9025\n",
      "Training batch: 1591 in epoch:5, training batch loss:2.8502\n",
      "Training batch: 1601 in epoch:5, training batch loss:3.0088\n",
      "Training batch: 1611 in epoch:5, training batch loss:3.0479\n",
      "Training batch: 1621 in epoch:5, training batch loss:3.0026\n",
      "Training batch: 1631 in epoch:5, training batch loss:2.9390\n",
      "Training batch: 1641 in epoch:5, training batch loss:2.9947\n",
      "Training batch: 1651 in epoch:5, training batch loss:3.2332\n",
      "Training batch: 1661 in epoch:5, training batch loss:3.1366\n",
      "Training batch: 1671 in epoch:5, training batch loss:2.9372\n",
      "Training batch: 1681 in epoch:5, training batch loss:3.0635\n",
      "Training batch: 1691 in epoch:5, training batch loss:3.1106\n",
      "Training batch: 1701 in epoch:5, training batch loss:2.9032\n",
      "Training batch: 1711 in epoch:5, training batch loss:3.2248\n",
      "Training batch: 1721 in epoch:5, training batch loss:3.0389\n",
      "Training batch: 1731 in epoch:5, training batch loss:3.0950\n",
      "Training batch: 1741 in epoch:5, training batch loss:3.1919\n",
      "Training batch: 1751 in epoch:5, training batch loss:3.1238\n",
      "Training batch: 1761 in epoch:5, training batch loss:3.1242\n",
      "Training batch: 1771 in epoch:5, training batch loss:3.2774\n",
      "Training batch: 1781 in epoch:5, training batch loss:2.3579\n",
      "Training batch: 1791 in epoch:5, training batch loss:3.0801\n",
      "Training batch: 1801 in epoch:5, training batch loss:3.2459\n",
      "Training batch: 1811 in epoch:5, training batch loss:3.0408\n",
      "Training batch: 1821 in epoch:5, training batch loss:3.1716\n",
      "Training batch: 1831 in epoch:5, training batch loss:3.0831\n",
      "Training batch: 1841 in epoch:5, training batch loss:3.0328\n",
      "Training batch: 1851 in epoch:5, training batch loss:3.0970\n",
      "Training batch: 1861 in epoch:5, training batch loss:2.9730\n",
      "Training batch: 1871 in epoch:5, training batch loss:3.3303\n",
      "Training batch: 1881 in epoch:5, training batch loss:3.2235\n",
      "Training batch: 1891 in epoch:5, training batch loss:2.9622\n",
      "Training batch: 1901 in epoch:5, training batch loss:3.0555\n",
      "Training batch: 1911 in epoch:5, training batch loss:3.3152\n",
      "Training batch: 1921 in epoch:5, training batch loss:3.1107\n",
      "Training batch: 1931 in epoch:5, training batch loss:3.0230\n",
      "Training batch: 1941 in epoch:5, training batch loss:3.1905\n",
      "Training batch: 1951 in epoch:5, training batch loss:2.9195\n",
      "Training batch: 1961 in epoch:5, training batch loss:2.8478\n",
      "Training batch: 1971 in epoch:5, training batch loss:3.1154\n",
      "Training batch: 1981 in epoch:5, training batch loss:3.0239\n",
      "Training batch: 1991 in epoch:5, training batch loss:3.0185\n",
      "Training batch: 2001 in epoch:5, training batch loss:2.9906\n",
      "Training batch: 2011 in epoch:5, training batch loss:3.0208\n",
      "Training batch: 2021 in epoch:5, training batch loss:3.2989\n",
      "Training batch: 2031 in epoch:5, training batch loss:3.2001\n",
      "Training batch: 2041 in epoch:5, training batch loss:3.0664\n",
      "Training batch: 2051 in epoch:5, training batch loss:3.0294\n",
      "Training batch: 2061 in epoch:5, training batch loss:3.3735\n",
      "Training batch: 2071 in epoch:5, training batch loss:3.1518\n",
      "Training batch: 2081 in epoch:5, training batch loss:2.9719\n",
      "Training batch: 2091 in epoch:5, training batch loss:3.2850\n",
      "Training batch: 2101 in epoch:5, training batch loss:3.1541\n",
      "Training batch: 2111 in epoch:5, training batch loss:3.2503\n",
      "Training batch: 2121 in epoch:5, training batch loss:3.1164\n",
      "Training batch: 2131 in epoch:5, training batch loss:2.9833\n",
      "Training batch: 2141 in epoch:5, training batch loss:3.0125\n",
      "Training batch: 2151 in epoch:5, training batch loss:3.0794\n",
      "Training batch: 2161 in epoch:5, training batch loss:3.0492\n",
      "Training batch: 2171 in epoch:5, training batch loss:3.0189\n",
      "Training batch: 2181 in epoch:5, training batch loss:2.9649\n",
      "Training batch: 2191 in epoch:5, training batch loss:3.0146\n",
      "Training batch: 2201 in epoch:5, training batch loss:3.1561\n",
      "Training batch: 2211 in epoch:5, training batch loss:3.1305\n",
      "Training batch: 2221 in epoch:5, training batch loss:3.0137\n",
      "Training batch: 2231 in epoch:5, training batch loss:3.0407\n",
      "Training batch: 2241 in epoch:5, training batch loss:3.0029\n",
      "Training batch: 2251 in epoch:5, training batch loss:3.0378\n",
      "Training batch: 2261 in epoch:5, training batch loss:3.0525\n",
      "Training batch: 2271 in epoch:5, training batch loss:3.0161\n",
      "Training batch: 2281 in epoch:5, training batch loss:2.9893\n",
      "Training batch: 2291 in epoch:5, training batch loss:2.8508\n",
      "2024-09-03 17:38:53 | epoch: 0005/10, training time: 666.9s, inference time: 70.3s\n",
      "train loss: 19.7782, val_loss: 0.8507\n",
      "Training batch: 11 in epoch:6, training batch loss:2.9764\n",
      "Training batch: 21 in epoch:6, training batch loss:2.7635\n",
      "Training batch: 31 in epoch:6, training batch loss:2.9068\n",
      "Training batch: 41 in epoch:6, training batch loss:3.1547\n",
      "Training batch: 51 in epoch:6, training batch loss:2.8881\n",
      "Training batch: 61 in epoch:6, training batch loss:2.8751\n",
      "Training batch: 71 in epoch:6, training batch loss:2.8074\n",
      "Training batch: 81 in epoch:6, training batch loss:2.9983\n",
      "Training batch: 91 in epoch:6, training batch loss:2.7378\n",
      "Training batch: 101 in epoch:6, training batch loss:2.9156\n",
      "Training batch: 111 in epoch:6, training batch loss:2.8977\n",
      "Training batch: 121 in epoch:6, training batch loss:2.8967\n",
      "Training batch: 131 in epoch:6, training batch loss:2.8523\n",
      "Training batch: 141 in epoch:6, training batch loss:3.0899\n",
      "Training batch: 151 in epoch:6, training batch loss:2.9656\n",
      "Training batch: 161 in epoch:6, training batch loss:3.1297\n",
      "Training batch: 171 in epoch:6, training batch loss:2.7698\n",
      "Training batch: 181 in epoch:6, training batch loss:2.9954\n",
      "Training batch: 191 in epoch:6, training batch loss:2.8605\n",
      "Training batch: 201 in epoch:6, training batch loss:2.9177\n",
      "Training batch: 211 in epoch:6, training batch loss:2.8572\n",
      "Training batch: 221 in epoch:6, training batch loss:2.9072\n",
      "Training batch: 231 in epoch:6, training batch loss:2.9309\n",
      "Training batch: 241 in epoch:6, training batch loss:3.1272\n",
      "Training batch: 251 in epoch:6, training batch loss:2.9164\n",
      "Training batch: 261 in epoch:6, training batch loss:2.8888\n",
      "Training batch: 271 in epoch:6, training batch loss:2.7204\n",
      "Training batch: 281 in epoch:6, training batch loss:3.0067\n",
      "Training batch: 291 in epoch:6, training batch loss:2.7564\n",
      "Training batch: 301 in epoch:6, training batch loss:2.9032\n",
      "Training batch: 311 in epoch:6, training batch loss:2.8870\n",
      "Training batch: 321 in epoch:6, training batch loss:3.0118\n",
      "Training batch: 331 in epoch:6, training batch loss:2.7424\n",
      "Training batch: 341 in epoch:6, training batch loss:2.8857\n",
      "Training batch: 351 in epoch:6, training batch loss:2.8267\n",
      "Training batch: 361 in epoch:6, training batch loss:2.7995\n",
      "Training batch: 371 in epoch:6, training batch loss:2.8790\n",
      "Training batch: 381 in epoch:6, training batch loss:2.8065\n",
      "Training batch: 391 in epoch:6, training batch loss:2.8124\n",
      "Training batch: 401 in epoch:6, training batch loss:3.0382\n",
      "Training batch: 411 in epoch:6, training batch loss:2.8862\n",
      "Training batch: 421 in epoch:6, training batch loss:2.9933\n",
      "Training batch: 431 in epoch:6, training batch loss:2.9589\n",
      "Training batch: 441 in epoch:6, training batch loss:2.8678\n",
      "Training batch: 451 in epoch:6, training batch loss:2.8049\n",
      "Training batch: 461 in epoch:6, training batch loss:2.8409\n",
      "Training batch: 471 in epoch:6, training batch loss:2.8846\n",
      "Training batch: 481 in epoch:6, training batch loss:3.0850\n",
      "Training batch: 491 in epoch:6, training batch loss:2.7246\n",
      "Training batch: 501 in epoch:6, training batch loss:2.8600\n",
      "Training batch: 511 in epoch:6, training batch loss:2.9252\n",
      "Training batch: 521 in epoch:6, training batch loss:2.9043\n",
      "Training batch: 531 in epoch:6, training batch loss:3.0335\n",
      "Training batch: 541 in epoch:6, training batch loss:2.8840\n",
      "Training batch: 551 in epoch:6, training batch loss:2.8341\n",
      "Training batch: 561 in epoch:6, training batch loss:3.0227\n",
      "Training batch: 571 in epoch:6, training batch loss:2.9788\n",
      "Training batch: 581 in epoch:6, training batch loss:2.9917\n",
      "Training batch: 591 in epoch:6, training batch loss:3.0117\n",
      "Training batch: 601 in epoch:6, training batch loss:2.9597\n",
      "Training batch: 611 in epoch:6, training batch loss:2.8741\n",
      "Training batch: 621 in epoch:6, training batch loss:2.9011\n",
      "Training batch: 631 in epoch:6, training batch loss:2.7474\n",
      "Training batch: 641 in epoch:6, training batch loss:2.9794\n",
      "Training batch: 651 in epoch:6, training batch loss:2.7601\n",
      "Training batch: 661 in epoch:6, training batch loss:2.8930\n",
      "Training batch: 671 in epoch:6, training batch loss:2.9859\n",
      "Training batch: 681 in epoch:6, training batch loss:2.9842\n",
      "Training batch: 691 in epoch:6, training batch loss:3.0089\n",
      "Training batch: 701 in epoch:6, training batch loss:2.8168\n",
      "Training batch: 711 in epoch:6, training batch loss:2.8782\n",
      "Training batch: 721 in epoch:6, training batch loss:2.9825\n",
      "Training batch: 731 in epoch:6, training batch loss:2.8020\n",
      "Training batch: 741 in epoch:6, training batch loss:2.9712\n",
      "Training batch: 751 in epoch:6, training batch loss:2.8302\n",
      "Training batch: 761 in epoch:6, training batch loss:3.0226\n",
      "Training batch: 771 in epoch:6, training batch loss:2.9297\n",
      "Training batch: 781 in epoch:6, training batch loss:2.9539\n",
      "Training batch: 791 in epoch:6, training batch loss:3.1493\n",
      "Training batch: 801 in epoch:6, training batch loss:2.8538\n",
      "Training batch: 811 in epoch:6, training batch loss:2.7201\n",
      "Training batch: 821 in epoch:6, training batch loss:2.7760\n",
      "Training batch: 831 in epoch:6, training batch loss:2.8899\n",
      "Training batch: 841 in epoch:6, training batch loss:3.0390\n",
      "Training batch: 851 in epoch:6, training batch loss:3.0858\n",
      "Training batch: 861 in epoch:6, training batch loss:2.8904\n",
      "Training batch: 871 in epoch:6, training batch loss:2.7654\n",
      "Training batch: 881 in epoch:6, training batch loss:2.7435\n",
      "Training batch: 891 in epoch:6, training batch loss:3.0258\n",
      "Training batch: 901 in epoch:6, training batch loss:3.0591\n",
      "Training batch: 911 in epoch:6, training batch loss:2.9770\n",
      "Training batch: 921 in epoch:6, training batch loss:3.0497\n",
      "Training batch: 931 in epoch:6, training batch loss:2.8627\n",
      "Training batch: 941 in epoch:6, training batch loss:2.8824\n",
      "Training batch: 951 in epoch:6, training batch loss:2.7876\n",
      "Training batch: 961 in epoch:6, training batch loss:2.9948\n",
      "Training batch: 971 in epoch:6, training batch loss:2.9008\n",
      "Training batch: 981 in epoch:6, training batch loss:2.9562\n",
      "Training batch: 991 in epoch:6, training batch loss:2.8240\n",
      "Training batch: 1001 in epoch:6, training batch loss:2.9413\n",
      "Training batch: 1011 in epoch:6, training batch loss:2.9056\n",
      "Training batch: 1021 in epoch:6, training batch loss:2.9125\n",
      "Training batch: 1031 in epoch:6, training batch loss:2.6663\n",
      "Training batch: 1041 in epoch:6, training batch loss:3.0901\n",
      "Training batch: 1051 in epoch:6, training batch loss:2.7586\n",
      "Training batch: 1061 in epoch:6, training batch loss:2.9039\n",
      "Training batch: 1071 in epoch:6, training batch loss:2.9258\n",
      "Training batch: 1081 in epoch:6, training batch loss:3.0187\n",
      "Training batch: 1091 in epoch:6, training batch loss:2.8862\n",
      "Training batch: 1101 in epoch:6, training batch loss:3.0350\n",
      "Training batch: 1111 in epoch:6, training batch loss:2.8850\n",
      "Training batch: 1121 in epoch:6, training batch loss:3.0774\n",
      "Training batch: 1131 in epoch:6, training batch loss:3.0078\n",
      "Training batch: 1141 in epoch:6, training batch loss:3.0266\n",
      "Training batch: 1151 in epoch:6, training batch loss:2.8496\n",
      "Training batch: 1161 in epoch:6, training batch loss:2.9023\n",
      "Training batch: 1171 in epoch:6, training batch loss:2.8897\n",
      "Training batch: 1181 in epoch:6, training batch loss:2.9466\n",
      "Training batch: 1191 in epoch:6, training batch loss:3.0106\n",
      "Training batch: 1201 in epoch:6, training batch loss:2.8287\n",
      "Training batch: 1211 in epoch:6, training batch loss:3.0768\n",
      "Training batch: 1221 in epoch:6, training batch loss:2.3756\n",
      "Training batch: 1231 in epoch:6, training batch loss:2.8303\n",
      "Training batch: 1241 in epoch:6, training batch loss:2.7537\n",
      "Training batch: 1251 in epoch:6, training batch loss:2.8372\n",
      "Training batch: 1261 in epoch:6, training batch loss:2.9241\n",
      "Training batch: 1271 in epoch:6, training batch loss:3.0528\n",
      "Training batch: 1281 in epoch:6, training batch loss:2.8364\n",
      "Training batch: 1291 in epoch:6, training batch loss:2.9800\n",
      "Training batch: 1301 in epoch:6, training batch loss:3.0792\n",
      "Training batch: 1311 in epoch:6, training batch loss:2.8416\n",
      "Training batch: 1321 in epoch:6, training batch loss:3.0496\n",
      "Training batch: 1331 in epoch:6, training batch loss:2.9040\n",
      "Training batch: 1341 in epoch:6, training batch loss:3.2944\n",
      "Training batch: 1351 in epoch:6, training batch loss:2.9957\n",
      "Training batch: 1361 in epoch:6, training batch loss:2.9096\n",
      "Training batch: 1371 in epoch:6, training batch loss:2.9457\n",
      "Training batch: 1381 in epoch:6, training batch loss:2.7075\n",
      "Training batch: 1391 in epoch:6, training batch loss:3.1629\n",
      "Training batch: 1401 in epoch:6, training batch loss:2.8641\n",
      "Training batch: 1411 in epoch:6, training batch loss:2.8218\n",
      "Training batch: 1421 in epoch:6, training batch loss:3.0132\n",
      "Training batch: 1431 in epoch:6, training batch loss:2.8884\n",
      "Training batch: 1441 in epoch:6, training batch loss:3.0037\n",
      "Training batch: 1451 in epoch:6, training batch loss:2.8697\n",
      "Training batch: 1461 in epoch:6, training batch loss:2.8851\n",
      "Training batch: 1471 in epoch:6, training batch loss:2.9019\n",
      "Training batch: 1481 in epoch:6, training batch loss:2.9574\n",
      "Training batch: 1491 in epoch:6, training batch loss:3.0452\n",
      "Training batch: 1501 in epoch:6, training batch loss:3.0269\n",
      "Training batch: 1511 in epoch:6, training batch loss:2.9733\n",
      "Training batch: 1521 in epoch:6, training batch loss:2.9603\n",
      "Training batch: 1531 in epoch:6, training batch loss:3.0205\n",
      "Training batch: 1541 in epoch:6, training batch loss:2.9478\n",
      "Training batch: 1551 in epoch:6, training batch loss:2.9326\n",
      "Training batch: 1561 in epoch:6, training batch loss:3.0448\n",
      "Training batch: 1571 in epoch:6, training batch loss:2.9037\n",
      "Training batch: 1581 in epoch:6, training batch loss:2.7610\n",
      "Training batch: 1591 in epoch:6, training batch loss:2.9718\n",
      "Training batch: 1601 in epoch:6, training batch loss:2.9906\n",
      "Training batch: 1611 in epoch:6, training batch loss:2.9839\n",
      "Training batch: 1621 in epoch:6, training batch loss:2.9285\n",
      "Training batch: 1631 in epoch:6, training batch loss:2.9399\n",
      "Training batch: 1641 in epoch:6, training batch loss:2.8539\n",
      "Training batch: 1651 in epoch:6, training batch loss:2.9078\n",
      "Training batch: 1661 in epoch:6, training batch loss:2.8584\n",
      "Training batch: 1671 in epoch:6, training batch loss:2.7823\n",
      "Training batch: 1681 in epoch:6, training batch loss:2.9222\n",
      "Training batch: 1691 in epoch:6, training batch loss:2.9847\n",
      "Training batch: 1701 in epoch:6, training batch loss:2.9222\n",
      "Training batch: 1711 in epoch:6, training batch loss:2.8400\n",
      "Training batch: 1721 in epoch:6, training batch loss:3.1313\n",
      "Training batch: 1731 in epoch:6, training batch loss:3.0329\n",
      "Training batch: 1741 in epoch:6, training batch loss:3.0002\n",
      "Training batch: 1751 in epoch:6, training batch loss:3.0561\n",
      "Training batch: 1761 in epoch:6, training batch loss:3.0342\n",
      "Training batch: 1771 in epoch:6, training batch loss:2.8642\n",
      "Training batch: 1781 in epoch:6, training batch loss:3.1084\n",
      "Training batch: 1791 in epoch:6, training batch loss:2.9323\n",
      "Training batch: 1801 in epoch:6, training batch loss:2.8518\n",
      "Training batch: 1811 in epoch:6, training batch loss:3.0800\n",
      "Training batch: 1821 in epoch:6, training batch loss:3.1370\n",
      "Training batch: 1831 in epoch:6, training batch loss:2.9352\n",
      "Training batch: 1841 in epoch:6, training batch loss:2.8322\n",
      "Training batch: 1851 in epoch:6, training batch loss:2.8017\n",
      "Training batch: 1861 in epoch:6, training batch loss:2.9428\n",
      "Training batch: 1871 in epoch:6, training batch loss:2.7670\n",
      "Training batch: 1881 in epoch:6, training batch loss:3.0516\n",
      "Training batch: 1891 in epoch:6, training batch loss:2.7539\n",
      "Training batch: 1901 in epoch:6, training batch loss:3.0412\n",
      "Training batch: 1911 in epoch:6, training batch loss:2.8145\n",
      "Training batch: 1921 in epoch:6, training batch loss:2.9381\n",
      "Training batch: 1931 in epoch:6, training batch loss:2.9208\n",
      "Training batch: 1941 in epoch:6, training batch loss:3.0396\n",
      "Training batch: 1951 in epoch:6, training batch loss:2.7771\n",
      "Training batch: 1961 in epoch:6, training batch loss:2.8940\n",
      "Training batch: 1971 in epoch:6, training batch loss:2.8673\n",
      "Training batch: 1981 in epoch:6, training batch loss:2.8043\n",
      "Training batch: 1991 in epoch:6, training batch loss:2.9588\n",
      "Training batch: 2001 in epoch:6, training batch loss:3.1243\n",
      "Training batch: 2011 in epoch:6, training batch loss:2.9527\n",
      "Training batch: 2021 in epoch:6, training batch loss:3.0652\n",
      "Training batch: 2031 in epoch:6, training batch loss:3.0048\n",
      "Training batch: 2041 in epoch:6, training batch loss:2.9163\n",
      "Training batch: 2051 in epoch:6, training batch loss:2.9033\n",
      "Training batch: 2061 in epoch:6, training batch loss:2.8851\n",
      "Training batch: 2071 in epoch:6, training batch loss:3.0128\n",
      "Training batch: 2081 in epoch:6, training batch loss:2.9836\n",
      "Training batch: 2091 in epoch:6, training batch loss:2.9430\n",
      "Training batch: 2101 in epoch:6, training batch loss:2.1712\n",
      "Training batch: 2111 in epoch:6, training batch loss:2.9847\n",
      "Training batch: 2121 in epoch:6, training batch loss:2.9856\n",
      "Training batch: 2131 in epoch:6, training batch loss:3.0649\n",
      "Training batch: 2141 in epoch:6, training batch loss:2.8327\n",
      "Training batch: 2151 in epoch:6, training batch loss:2.8969\n",
      "Training batch: 2161 in epoch:6, training batch loss:2.9664\n",
      "Training batch: 2171 in epoch:6, training batch loss:3.1790\n",
      "Training batch: 2181 in epoch:6, training batch loss:2.8382\n",
      "Training batch: 2191 in epoch:6, training batch loss:2.9104\n",
      "Training batch: 2201 in epoch:6, training batch loss:2.9247\n",
      "Training batch: 2211 in epoch:6, training batch loss:3.1159\n",
      "Training batch: 2221 in epoch:6, training batch loss:2.9639\n",
      "Training batch: 2231 in epoch:6, training batch loss:2.9034\n",
      "Training batch: 2241 in epoch:6, training batch loss:2.9210\n",
      "Training batch: 2251 in epoch:6, training batch loss:2.7702\n",
      "Training batch: 2261 in epoch:6, training batch loss:2.9806\n",
      "Training batch: 2271 in epoch:6, training batch loss:2.9327\n",
      "Training batch: 2281 in epoch:6, training batch loss:2.8755\n",
      "Training batch: 2291 in epoch:6, training batch loss:2.9462\n",
      "2024-09-03 17:51:18 | epoch: 0006/10, training time: 673.4s, inference time: 71.1s\n",
      "train loss: 19.1356, val_loss: 0.8507\n",
      "Training batch: 11 in epoch:7, training batch loss:2.9486\n",
      "Training batch: 21 in epoch:7, training batch loss:2.8334\n",
      "Training batch: 31 in epoch:7, training batch loss:2.8158\n",
      "Training batch: 41 in epoch:7, training batch loss:2.8559\n",
      "Training batch: 51 in epoch:7, training batch loss:2.9323\n",
      "Training batch: 61 in epoch:7, training batch loss:2.7419\n",
      "Training batch: 71 in epoch:7, training batch loss:2.8909\n",
      "Training batch: 81 in epoch:7, training batch loss:2.8366\n",
      "Training batch: 91 in epoch:7, training batch loss:2.7851\n",
      "Training batch: 101 in epoch:7, training batch loss:2.7947\n",
      "Training batch: 111 in epoch:7, training batch loss:2.8447\n",
      "Training batch: 121 in epoch:7, training batch loss:2.7922\n",
      "Training batch: 131 in epoch:7, training batch loss:2.9054\n",
      "Training batch: 141 in epoch:7, training batch loss:2.7718\n",
      "Training batch: 151 in epoch:7, training batch loss:2.7488\n",
      "Training batch: 161 in epoch:7, training batch loss:2.7464\n",
      "Training batch: 171 in epoch:7, training batch loss:2.6950\n",
      "Training batch: 181 in epoch:7, training batch loss:2.7777\n",
      "Training batch: 191 in epoch:7, training batch loss:2.7620\n",
      "Training batch: 201 in epoch:7, training batch loss:2.8951\n",
      "Training batch: 211 in epoch:7, training batch loss:2.7482\n",
      "Training batch: 221 in epoch:7, training batch loss:2.7854\n",
      "Training batch: 231 in epoch:7, training batch loss:3.0069\n",
      "Training batch: 241 in epoch:7, training batch loss:2.8895\n",
      "Training batch: 251 in epoch:7, training batch loss:2.7275\n",
      "Training batch: 261 in epoch:7, training batch loss:2.8366\n",
      "Training batch: 271 in epoch:7, training batch loss:2.8612\n",
      "Training batch: 281 in epoch:7, training batch loss:2.7857\n",
      "Training batch: 291 in epoch:7, training batch loss:2.7757\n",
      "Training batch: 301 in epoch:7, training batch loss:2.9766\n",
      "Training batch: 311 in epoch:7, training batch loss:2.7683\n",
      "Training batch: 321 in epoch:7, training batch loss:2.7728\n",
      "Training batch: 331 in epoch:7, training batch loss:2.7507\n",
      "Training batch: 341 in epoch:7, training batch loss:2.9796\n",
      "Training batch: 351 in epoch:7, training batch loss:2.8034\n",
      "Training batch: 361 in epoch:7, training batch loss:2.7521\n",
      "Training batch: 371 in epoch:7, training batch loss:2.8166\n",
      "Training batch: 381 in epoch:7, training batch loss:2.8036\n",
      "Training batch: 391 in epoch:7, training batch loss:2.8556\n",
      "Training batch: 401 in epoch:7, training batch loss:2.7273\n",
      "Training batch: 411 in epoch:7, training batch loss:2.2696\n",
      "Training batch: 421 in epoch:7, training batch loss:2.7940\n",
      "Training batch: 431 in epoch:7, training batch loss:2.7678\n",
      "Training batch: 441 in epoch:7, training batch loss:2.8350\n",
      "Training batch: 451 in epoch:7, training batch loss:2.8691\n",
      "Training batch: 461 in epoch:7, training batch loss:2.9153\n",
      "Training batch: 471 in epoch:7, training batch loss:2.8856\n",
      "Training batch: 481 in epoch:7, training batch loss:2.7052\n",
      "Training batch: 491 in epoch:7, training batch loss:2.7310\n",
      "Training batch: 501 in epoch:7, training batch loss:2.9239\n",
      "Training batch: 511 in epoch:7, training batch loss:2.6013\n",
      "Training batch: 521 in epoch:7, training batch loss:2.8692\n",
      "Training batch: 531 in epoch:7, training batch loss:2.8226\n",
      "Training batch: 541 in epoch:7, training batch loss:2.9500\n",
      "Training batch: 551 in epoch:7, training batch loss:2.9652\n",
      "Training batch: 561 in epoch:7, training batch loss:2.7105\n",
      "Training batch: 571 in epoch:7, training batch loss:2.8561\n",
      "Training batch: 581 in epoch:7, training batch loss:2.8912\n",
      "Training batch: 591 in epoch:7, training batch loss:2.8245\n",
      "Training batch: 601 in epoch:7, training batch loss:2.7341\n",
      "Training batch: 611 in epoch:7, training batch loss:2.8486\n",
      "Training batch: 621 in epoch:7, training batch loss:2.7788\n",
      "Training batch: 631 in epoch:7, training batch loss:2.7809\n",
      "Training batch: 641 in epoch:7, training batch loss:2.7539\n",
      "Training batch: 651 in epoch:7, training batch loss:2.7331\n",
      "Training batch: 661 in epoch:7, training batch loss:2.8669\n",
      "Training batch: 671 in epoch:7, training batch loss:2.8600\n",
      "Training batch: 681 in epoch:7, training batch loss:2.8258\n",
      "Training batch: 691 in epoch:7, training batch loss:2.8572\n",
      "Training batch: 701 in epoch:7, training batch loss:3.0123\n",
      "Training batch: 711 in epoch:7, training batch loss:2.8856\n",
      "Training batch: 721 in epoch:7, training batch loss:2.8689\n",
      "Training batch: 731 in epoch:7, training batch loss:2.7275\n",
      "Training batch: 741 in epoch:7, training batch loss:2.7118\n",
      "Training batch: 751 in epoch:7, training batch loss:2.7762\n",
      "Training batch: 761 in epoch:7, training batch loss:2.8132\n",
      "Training batch: 771 in epoch:7, training batch loss:2.7273\n",
      "Training batch: 781 in epoch:7, training batch loss:2.8392\n",
      "Training batch: 791 in epoch:7, training batch loss:2.6287\n",
      "Training batch: 801 in epoch:7, training batch loss:2.8340\n",
      "Training batch: 811 in epoch:7, training batch loss:2.7437\n",
      "Training batch: 821 in epoch:7, training batch loss:3.0002\n",
      "Training batch: 831 in epoch:7, training batch loss:2.8678\n",
      "Training batch: 841 in epoch:7, training batch loss:2.7299\n",
      "Training batch: 851 in epoch:7, training batch loss:2.9040\n",
      "Training batch: 861 in epoch:7, training batch loss:2.9235\n",
      "Training batch: 871 in epoch:7, training batch loss:2.8690\n",
      "Training batch: 881 in epoch:7, training batch loss:2.6636\n",
      "Training batch: 891 in epoch:7, training batch loss:2.7904\n",
      "Training batch: 901 in epoch:7, training batch loss:2.8136\n",
      "Training batch: 911 in epoch:7, training batch loss:2.8070\n",
      "Training batch: 921 in epoch:7, training batch loss:2.8065\n",
      "Training batch: 931 in epoch:7, training batch loss:2.8268\n",
      "Training batch: 941 in epoch:7, training batch loss:2.8242\n",
      "Training batch: 951 in epoch:7, training batch loss:2.9745\n",
      "Training batch: 961 in epoch:7, training batch loss:2.7537\n",
      "Training batch: 971 in epoch:7, training batch loss:2.8508\n",
      "Training batch: 981 in epoch:7, training batch loss:2.8660\n",
      "Training batch: 991 in epoch:7, training batch loss:2.7356\n",
      "Training batch: 1001 in epoch:7, training batch loss:2.8563\n",
      "Training batch: 1011 in epoch:7, training batch loss:2.7681\n",
      "Training batch: 1021 in epoch:7, training batch loss:2.8219\n",
      "Training batch: 1031 in epoch:7, training batch loss:2.8534\n",
      "Training batch: 1041 in epoch:7, training batch loss:3.0093\n",
      "Training batch: 1051 in epoch:7, training batch loss:2.7776\n",
      "Training batch: 1061 in epoch:7, training batch loss:2.8292\n",
      "Training batch: 1071 in epoch:7, training batch loss:2.8149\n",
      "Training batch: 1081 in epoch:7, training batch loss:2.6378\n",
      "Training batch: 1091 in epoch:7, training batch loss:2.9857\n",
      "Training batch: 1101 in epoch:7, training batch loss:2.9225\n",
      "Training batch: 1111 in epoch:7, training batch loss:2.9568\n",
      "Training batch: 1121 in epoch:7, training batch loss:2.6592\n",
      "Training batch: 1131 in epoch:7, training batch loss:2.7668\n",
      "Training batch: 1141 in epoch:7, training batch loss:2.8717\n",
      "Training batch: 1151 in epoch:7, training batch loss:2.7969\n",
      "Training batch: 1161 in epoch:7, training batch loss:2.8387\n",
      "Training batch: 1171 in epoch:7, training batch loss:2.8589\n",
      "Training batch: 1181 in epoch:7, training batch loss:2.8621\n",
      "Training batch: 1191 in epoch:7, training batch loss:2.8024\n",
      "Training batch: 1201 in epoch:7, training batch loss:2.8584\n",
      "Training batch: 1211 in epoch:7, training batch loss:2.8341\n",
      "Training batch: 1221 in epoch:7, training batch loss:2.8709\n",
      "Training batch: 1231 in epoch:7, training batch loss:2.7287\n",
      "Training batch: 1241 in epoch:7, training batch loss:2.8006\n",
      "Training batch: 1251 in epoch:7, training batch loss:2.9231\n",
      "Training batch: 1261 in epoch:7, training batch loss:2.7933\n",
      "Training batch: 1271 in epoch:7, training batch loss:2.6913\n",
      "Training batch: 1281 in epoch:7, training batch loss:2.7748\n",
      "Training batch: 1291 in epoch:7, training batch loss:2.7637\n",
      "Training batch: 1301 in epoch:7, training batch loss:3.0982\n",
      "Training batch: 1311 in epoch:7, training batch loss:2.7973\n",
      "Training batch: 1321 in epoch:7, training batch loss:2.7554\n",
      "Training batch: 1331 in epoch:7, training batch loss:2.8360\n",
      "Training batch: 1341 in epoch:7, training batch loss:2.8422\n",
      "Training batch: 1351 in epoch:7, training batch loss:2.8551\n",
      "Training batch: 1361 in epoch:7, training batch loss:2.7638\n",
      "Training batch: 1371 in epoch:7, training batch loss:2.8762\n",
      "Training batch: 1381 in epoch:7, training batch loss:2.8583\n",
      "Training batch: 1391 in epoch:7, training batch loss:2.8549\n",
      "Training batch: 1401 in epoch:7, training batch loss:2.8394\n",
      "Training batch: 1411 in epoch:7, training batch loss:2.8891\n",
      "Training batch: 1421 in epoch:7, training batch loss:2.7699\n",
      "Training batch: 1431 in epoch:7, training batch loss:2.8167\n",
      "Training batch: 1441 in epoch:7, training batch loss:2.8269\n",
      "Training batch: 1451 in epoch:7, training batch loss:2.7642\n",
      "Training batch: 1461 in epoch:7, training batch loss:2.8621\n",
      "Training batch: 1471 in epoch:7, training batch loss:2.7827\n",
      "Training batch: 1481 in epoch:7, training batch loss:2.7722\n",
      "Training batch: 1491 in epoch:7, training batch loss:2.8832\n",
      "Training batch: 1501 in epoch:7, training batch loss:2.7623\n",
      "Training batch: 1511 in epoch:7, training batch loss:2.7818\n",
      "Training batch: 1521 in epoch:7, training batch loss:2.8101\n",
      "Training batch: 1531 in epoch:7, training batch loss:2.7295\n",
      "Training batch: 1541 in epoch:7, training batch loss:2.8185\n",
      "Training batch: 1551 in epoch:7, training batch loss:2.8513\n",
      "Training batch: 1561 in epoch:7, training batch loss:2.7358\n",
      "Training batch: 1571 in epoch:7, training batch loss:3.0509\n",
      "Training batch: 1581 in epoch:7, training batch loss:2.8083\n",
      "Training batch: 1591 in epoch:7, training batch loss:2.6895\n",
      "Training batch: 1601 in epoch:7, training batch loss:2.9223\n",
      "Training batch: 1611 in epoch:7, training batch loss:2.8768\n",
      "Training batch: 1621 in epoch:7, training batch loss:2.8802\n",
      "Training batch: 1631 in epoch:7, training batch loss:2.8635\n",
      "Training batch: 1641 in epoch:7, training batch loss:2.7717\n",
      "Training batch: 1651 in epoch:7, training batch loss:2.8208\n",
      "Training batch: 1661 in epoch:7, training batch loss:2.8503\n",
      "Training batch: 1671 in epoch:7, training batch loss:2.9683\n",
      "Training batch: 1681 in epoch:7, training batch loss:2.8539\n",
      "Training batch: 1691 in epoch:7, training batch loss:2.9650\n",
      "Training batch: 1701 in epoch:7, training batch loss:2.8285\n",
      "Training batch: 1711 in epoch:7, training batch loss:3.0302\n",
      "Training batch: 1721 in epoch:7, training batch loss:2.9639\n",
      "Training batch: 1731 in epoch:7, training batch loss:2.8734\n",
      "Training batch: 1741 in epoch:7, training batch loss:2.7731\n",
      "Training batch: 1751 in epoch:7, training batch loss:2.8113\n",
      "Training batch: 1761 in epoch:7, training batch loss:2.8073\n",
      "Training batch: 1771 in epoch:7, training batch loss:2.8926\n",
      "Training batch: 1781 in epoch:7, training batch loss:2.8184\n",
      "Training batch: 1791 in epoch:7, training batch loss:2.6437\n",
      "Training batch: 1801 in epoch:7, training batch loss:2.8605\n",
      "Training batch: 1811 in epoch:7, training batch loss:2.8427\n",
      "Training batch: 1821 in epoch:7, training batch loss:2.7952\n",
      "Training batch: 1831 in epoch:7, training batch loss:2.9055\n",
      "Training batch: 1841 in epoch:7, training batch loss:2.8270\n",
      "Training batch: 1851 in epoch:7, training batch loss:2.9063\n",
      "Training batch: 1861 in epoch:7, training batch loss:2.8279\n",
      "Training batch: 1871 in epoch:7, training batch loss:2.8712\n",
      "Training batch: 1881 in epoch:7, training batch loss:2.9889\n",
      "Training batch: 1891 in epoch:7, training batch loss:2.7764\n",
      "Training batch: 1901 in epoch:7, training batch loss:2.8358\n",
      "Training batch: 1911 in epoch:7, training batch loss:2.8248\n",
      "Training batch: 1921 in epoch:7, training batch loss:2.8481\n",
      "Training batch: 1931 in epoch:7, training batch loss:2.7536\n",
      "Training batch: 1941 in epoch:7, training batch loss:2.9689\n",
      "Training batch: 1951 in epoch:7, training batch loss:2.9186\n",
      "Training batch: 1961 in epoch:7, training batch loss:2.8834\n",
      "Training batch: 1971 in epoch:7, training batch loss:3.1260\n",
      "Training batch: 1981 in epoch:7, training batch loss:2.7692\n",
      "Training batch: 1991 in epoch:7, training batch loss:2.7620\n",
      "Training batch: 2001 in epoch:7, training batch loss:2.7862\n",
      "Training batch: 2011 in epoch:7, training batch loss:2.7051\n",
      "Training batch: 2021 in epoch:7, training batch loss:2.9257\n",
      "Training batch: 2031 in epoch:7, training batch loss:2.9807\n",
      "Training batch: 2041 in epoch:7, training batch loss:2.9709\n",
      "Training batch: 2051 in epoch:7, training batch loss:2.8706\n",
      "Training batch: 2061 in epoch:7, training batch loss:3.0136\n",
      "Training batch: 2071 in epoch:7, training batch loss:2.7282\n",
      "Training batch: 2081 in epoch:7, training batch loss:2.8509\n",
      "Training batch: 2091 in epoch:7, training batch loss:2.9127\n",
      "Training batch: 2101 in epoch:7, training batch loss:2.7938\n",
      "Training batch: 2111 in epoch:7, training batch loss:2.9504\n",
      "Training batch: 2121 in epoch:7, training batch loss:2.9153\n",
      "Training batch: 2131 in epoch:7, training batch loss:2.8806\n",
      "Training batch: 2141 in epoch:7, training batch loss:2.7256\n",
      "Training batch: 2151 in epoch:7, training batch loss:3.0198\n",
      "Training batch: 2161 in epoch:7, training batch loss:2.9569\n",
      "Training batch: 2171 in epoch:7, training batch loss:3.0301\n",
      "Training batch: 2181 in epoch:7, training batch loss:2.8344\n",
      "Training batch: 2191 in epoch:7, training batch loss:2.9341\n",
      "Training batch: 2201 in epoch:7, training batch loss:2.9880\n",
      "Training batch: 2211 in epoch:7, training batch loss:2.7850\n",
      "Training batch: 2221 in epoch:7, training batch loss:2.8338\n",
      "Training batch: 2231 in epoch:7, training batch loss:2.8022\n",
      "Training batch: 2241 in epoch:7, training batch loss:2.7366\n",
      "Training batch: 2251 in epoch:7, training batch loss:2.9156\n",
      "Training batch: 2261 in epoch:7, training batch loss:2.8569\n",
      "Training batch: 2271 in epoch:7, training batch loss:2.9211\n",
      "Training batch: 2281 in epoch:7, training batch loss:2.8571\n",
      "Training batch: 2291 in epoch:7, training batch loss:2.7459\n",
      "2024-09-03 18:03:36 | epoch: 0007/10, training time: 668.0s, inference time: 70.3s\n",
      "train loss: 18.4193, val_loss: 0.8510\n",
      "Training batch: 11 in epoch:8, training batch loss:2.7264\n",
      "Training batch: 21 in epoch:8, training batch loss:2.7533\n",
      "Training batch: 31 in epoch:8, training batch loss:2.6910\n",
      "Training batch: 41 in epoch:8, training batch loss:2.8539\n",
      "Training batch: 51 in epoch:8, training batch loss:2.7602\n",
      "Training batch: 61 in epoch:8, training batch loss:2.5927\n",
      "Training batch: 71 in epoch:8, training batch loss:2.7259\n",
      "Training batch: 81 in epoch:8, training batch loss:2.7759\n",
      "Training batch: 91 in epoch:8, training batch loss:2.7583\n",
      "Training batch: 101 in epoch:8, training batch loss:2.7709\n",
      "Training batch: 111 in epoch:8, training batch loss:2.7417\n",
      "Training batch: 121 in epoch:8, training batch loss:2.6195\n",
      "Training batch: 131 in epoch:8, training batch loss:2.7599\n",
      "Training batch: 141 in epoch:8, training batch loss:2.7417\n",
      "Training batch: 151 in epoch:8, training batch loss:2.6843\n",
      "Training batch: 161 in epoch:8, training batch loss:2.6272\n",
      "Training batch: 171 in epoch:8, training batch loss:2.7814\n",
      "Training batch: 181 in epoch:8, training batch loss:2.7180\n",
      "Training batch: 191 in epoch:8, training batch loss:2.7871\n",
      "Training batch: 201 in epoch:8, training batch loss:2.6494\n",
      "Training batch: 211 in epoch:8, training batch loss:2.6833\n",
      "Training batch: 221 in epoch:8, training batch loss:2.7785\n",
      "Training batch: 231 in epoch:8, training batch loss:2.7716\n",
      "Training batch: 241 in epoch:8, training batch loss:2.6422\n",
      "Training batch: 251 in epoch:8, training batch loss:2.7436\n",
      "Training batch: 261 in epoch:8, training batch loss:2.6637\n",
      "Training batch: 271 in epoch:8, training batch loss:2.6667\n",
      "Training batch: 281 in epoch:8, training batch loss:2.7424\n",
      "Training batch: 291 in epoch:8, training batch loss:2.7410\n",
      "Training batch: 301 in epoch:8, training batch loss:2.6673\n",
      "Training batch: 311 in epoch:8, training batch loss:2.7400\n",
      "Training batch: 321 in epoch:8, training batch loss:2.6038\n",
      "Training batch: 331 in epoch:8, training batch loss:2.6463\n",
      "Training batch: 341 in epoch:8, training batch loss:2.6263\n",
      "Training batch: 351 in epoch:8, training batch loss:2.8088\n",
      "Training batch: 361 in epoch:8, training batch loss:2.6184\n",
      "Training batch: 371 in epoch:8, training batch loss:2.6880\n",
      "Training batch: 381 in epoch:8, training batch loss:2.7303\n",
      "Training batch: 391 in epoch:8, training batch loss:2.6744\n",
      "Training batch: 401 in epoch:8, training batch loss:2.7720\n",
      "Training batch: 411 in epoch:8, training batch loss:2.6937\n",
      "Training batch: 421 in epoch:8, training batch loss:2.7091\n",
      "Training batch: 431 in epoch:8, training batch loss:2.7535\n",
      "Training batch: 441 in epoch:8, training batch loss:2.6530\n",
      "Training batch: 451 in epoch:8, training batch loss:2.5979\n",
      "Training batch: 461 in epoch:8, training batch loss:2.7023\n",
      "Training batch: 471 in epoch:8, training batch loss:2.7550\n",
      "Training batch: 481 in epoch:8, training batch loss:2.7018\n",
      "Training batch: 491 in epoch:8, training batch loss:2.6544\n",
      "Training batch: 501 in epoch:8, training batch loss:2.5932\n",
      "Training batch: 511 in epoch:8, training batch loss:2.6725\n",
      "Training batch: 521 in epoch:8, training batch loss:2.7165\n",
      "Training batch: 531 in epoch:8, training batch loss:2.6989\n",
      "Training batch: 541 in epoch:8, training batch loss:2.6890\n",
      "Training batch: 551 in epoch:8, training batch loss:2.7479\n",
      "Training batch: 561 in epoch:8, training batch loss:2.6659\n",
      "Training batch: 571 in epoch:8, training batch loss:2.7360\n",
      "Training batch: 581 in epoch:8, training batch loss:2.8079\n",
      "Training batch: 591 in epoch:8, training batch loss:2.6394\n",
      "Training batch: 601 in epoch:8, training batch loss:2.7442\n",
      "Training batch: 611 in epoch:8, training batch loss:2.6712\n",
      "Training batch: 621 in epoch:8, training batch loss:2.7405\n",
      "Training batch: 631 in epoch:8, training batch loss:2.6876\n",
      "Training batch: 641 in epoch:8, training batch loss:2.7719\n",
      "Training batch: 651 in epoch:8, training batch loss:2.6658\n",
      "Training batch: 661 in epoch:8, training batch loss:2.7513\n",
      "Training batch: 671 in epoch:8, training batch loss:2.7036\n",
      "Training batch: 681 in epoch:8, training batch loss:2.7558\n",
      "Training batch: 691 in epoch:8, training batch loss:2.5185\n",
      "Training batch: 701 in epoch:8, training batch loss:2.7356\n",
      "Training batch: 711 in epoch:8, training batch loss:2.7093\n",
      "Training batch: 721 in epoch:8, training batch loss:2.8310\n",
      "Training batch: 731 in epoch:8, training batch loss:2.7690\n",
      "Training batch: 741 in epoch:8, training batch loss:2.6320\n",
      "Training batch: 751 in epoch:8, training batch loss:2.6530\n",
      "Training batch: 761 in epoch:8, training batch loss:2.8259\n",
      "Training batch: 771 in epoch:8, training batch loss:2.6475\n",
      "Training batch: 781 in epoch:8, training batch loss:2.6817\n",
      "Training batch: 791 in epoch:8, training batch loss:2.6317\n",
      "Training batch: 801 in epoch:8, training batch loss:2.7225\n",
      "Training batch: 811 in epoch:8, training batch loss:2.7225\n",
      "Training batch: 821 in epoch:8, training batch loss:2.6968\n",
      "Training batch: 831 in epoch:8, training batch loss:2.6156\n",
      "Training batch: 841 in epoch:8, training batch loss:2.6631\n",
      "Training batch: 851 in epoch:8, training batch loss:2.7267\n",
      "Training batch: 861 in epoch:8, training batch loss:2.6831\n",
      "Training batch: 871 in epoch:8, training batch loss:2.6617\n",
      "Training batch: 881 in epoch:8, training batch loss:2.8128\n",
      "Training batch: 891 in epoch:8, training batch loss:2.8057\n",
      "Training batch: 901 in epoch:8, training batch loss:2.6622\n",
      "Training batch: 911 in epoch:8, training batch loss:2.6132\n",
      "Training batch: 921 in epoch:8, training batch loss:2.6755\n",
      "Training batch: 931 in epoch:8, training batch loss:2.6500\n",
      "Training batch: 941 in epoch:8, training batch loss:2.6432\n",
      "Training batch: 951 in epoch:8, training batch loss:2.7473\n",
      "Training batch: 961 in epoch:8, training batch loss:2.7019\n",
      "Training batch: 971 in epoch:8, training batch loss:2.7294\n",
      "Training batch: 981 in epoch:8, training batch loss:2.7650\n",
      "Training batch: 991 in epoch:8, training batch loss:2.6579\n",
      "Training batch: 1001 in epoch:8, training batch loss:2.6923\n",
      "Training batch: 1011 in epoch:8, training batch loss:2.7403\n",
      "Training batch: 1021 in epoch:8, training batch loss:2.7157\n",
      "Training batch: 1031 in epoch:8, training batch loss:2.6606\n",
      "Training batch: 1041 in epoch:8, training batch loss:2.7062\n",
      "Training batch: 1051 in epoch:8, training batch loss:2.7319\n",
      "Training batch: 1061 in epoch:8, training batch loss:2.6763\n",
      "Training batch: 1071 in epoch:8, training batch loss:2.7739\n",
      "Training batch: 1081 in epoch:8, training batch loss:2.7931\n",
      "Training batch: 1091 in epoch:8, training batch loss:2.7388\n",
      "Training batch: 1101 in epoch:8, training batch loss:2.6866\n",
      "Training batch: 1111 in epoch:8, training batch loss:2.6264\n",
      "Training batch: 1121 in epoch:8, training batch loss:2.7100\n",
      "Training batch: 1131 in epoch:8, training batch loss:2.7863\n",
      "Training batch: 1141 in epoch:8, training batch loss:2.6518\n",
      "Training batch: 1151 in epoch:8, training batch loss:2.6948\n",
      "Training batch: 1161 in epoch:8, training batch loss:2.7140\n",
      "Training batch: 1171 in epoch:8, training batch loss:2.5899\n",
      "Training batch: 1181 in epoch:8, training batch loss:2.7225\n",
      "Training batch: 1191 in epoch:8, training batch loss:2.7568\n",
      "Training batch: 1201 in epoch:8, training batch loss:2.7678\n",
      "Training batch: 1211 in epoch:8, training batch loss:2.7615\n",
      "Training batch: 1221 in epoch:8, training batch loss:2.8026\n",
      "Training batch: 1231 in epoch:8, training batch loss:2.7841\n",
      "Training batch: 1241 in epoch:8, training batch loss:2.6969\n",
      "Training batch: 1251 in epoch:8, training batch loss:2.7015\n",
      "Training batch: 1261 in epoch:8, training batch loss:2.7841\n",
      "Training batch: 1271 in epoch:8, training batch loss:2.8108\n",
      "Training batch: 1281 in epoch:8, training batch loss:2.7007\n",
      "Training batch: 1291 in epoch:8, training batch loss:2.6233\n",
      "Training batch: 1301 in epoch:8, training batch loss:2.6773\n",
      "Training batch: 1311 in epoch:8, training batch loss:2.7238\n",
      "Training batch: 1321 in epoch:8, training batch loss:2.7458\n",
      "Training batch: 1331 in epoch:8, training batch loss:2.6167\n",
      "Training batch: 1341 in epoch:8, training batch loss:2.7811\n",
      "Training batch: 1351 in epoch:8, training batch loss:2.6147\n",
      "Training batch: 1361 in epoch:8, training batch loss:2.6715\n",
      "Training batch: 1371 in epoch:8, training batch loss:2.7652\n",
      "Training batch: 1381 in epoch:8, training batch loss:2.7043\n",
      "Training batch: 1391 in epoch:8, training batch loss:2.7141\n",
      "Training batch: 1401 in epoch:8, training batch loss:2.7098\n",
      "Training batch: 1411 in epoch:8, training batch loss:2.7103\n",
      "Training batch: 1421 in epoch:8, training batch loss:2.6870\n",
      "Training batch: 1431 in epoch:8, training batch loss:2.7011\n",
      "Training batch: 1441 in epoch:8, training batch loss:2.6392\n",
      "Training batch: 1451 in epoch:8, training batch loss:2.7188\n",
      "Training batch: 1461 in epoch:8, training batch loss:2.7266\n",
      "Training batch: 1471 in epoch:8, training batch loss:2.7732\n",
      "Training batch: 1481 in epoch:8, training batch loss:2.8020\n",
      "Training batch: 1491 in epoch:8, training batch loss:2.7818\n",
      "Training batch: 1501 in epoch:8, training batch loss:2.7784\n",
      "Training batch: 1511 in epoch:8, training batch loss:2.6835\n",
      "Training batch: 1521 in epoch:8, training batch loss:2.7761\n",
      "Training batch: 1531 in epoch:8, training batch loss:2.7297\n",
      "Training batch: 1541 in epoch:8, training batch loss:2.8167\n",
      "Training batch: 1551 in epoch:8, training batch loss:2.7173\n",
      "Training batch: 1561 in epoch:8, training batch loss:2.6666\n",
      "Training batch: 1571 in epoch:8, training batch loss:2.5851\n",
      "Training batch: 1581 in epoch:8, training batch loss:2.7422\n",
      "Training batch: 1591 in epoch:8, training batch loss:2.6971\n",
      "Training batch: 1601 in epoch:8, training batch loss:2.6804\n",
      "Training batch: 1611 in epoch:8, training batch loss:2.7474\n",
      "Training batch: 1621 in epoch:8, training batch loss:2.8336\n",
      "Training batch: 1631 in epoch:8, training batch loss:2.7929\n",
      "Training batch: 1641 in epoch:8, training batch loss:2.6833\n",
      "Training batch: 1651 in epoch:8, training batch loss:2.6694\n",
      "Training batch: 1661 in epoch:8, training batch loss:2.7714\n",
      "Training batch: 1671 in epoch:8, training batch loss:2.6639\n",
      "Training batch: 1681 in epoch:8, training batch loss:2.8436\n",
      "Training batch: 1691 in epoch:8, training batch loss:2.6664\n",
      "Training batch: 1701 in epoch:8, training batch loss:2.6469\n",
      "Training batch: 1711 in epoch:8, training batch loss:2.7239\n",
      "Training batch: 1721 in epoch:8, training batch loss:2.8979\n",
      "Training batch: 1731 in epoch:8, training batch loss:2.7706\n",
      "Training batch: 1741 in epoch:8, training batch loss:2.7006\n",
      "Training batch: 1751 in epoch:8, training batch loss:2.6330\n",
      "Training batch: 1761 in epoch:8, training batch loss:2.7210\n",
      "Training batch: 1771 in epoch:8, training batch loss:2.7333\n",
      "Training batch: 1781 in epoch:8, training batch loss:2.6445\n",
      "Training batch: 1791 in epoch:8, training batch loss:2.7968\n",
      "Training batch: 1801 in epoch:8, training batch loss:2.6759\n",
      "Training batch: 1811 in epoch:8, training batch loss:2.7210\n",
      "Training batch: 1821 in epoch:8, training batch loss:2.6913\n",
      "Training batch: 1831 in epoch:8, training batch loss:2.7895\n",
      "Training batch: 1841 in epoch:8, training batch loss:2.7108\n",
      "Training batch: 1851 in epoch:8, training batch loss:2.7018\n",
      "Training batch: 1861 in epoch:8, training batch loss:2.6829\n",
      "Training batch: 1871 in epoch:8, training batch loss:2.7196\n",
      "Training batch: 1881 in epoch:8, training batch loss:2.7355\n",
      "Training batch: 1891 in epoch:8, training batch loss:2.7417\n",
      "Training batch: 1901 in epoch:8, training batch loss:2.8872\n",
      "Training batch: 1911 in epoch:8, training batch loss:2.6337\n",
      "Training batch: 1921 in epoch:8, training batch loss:2.7744\n",
      "Training batch: 1931 in epoch:8, training batch loss:2.8200\n",
      "Training batch: 1941 in epoch:8, training batch loss:2.6943\n",
      "Training batch: 1951 in epoch:8, training batch loss:2.6843\n",
      "Training batch: 1961 in epoch:8, training batch loss:2.6364\n",
      "Training batch: 1971 in epoch:8, training batch loss:2.7010\n",
      "Training batch: 1981 in epoch:8, training batch loss:2.8019\n",
      "Training batch: 1991 in epoch:8, training batch loss:2.6715\n",
      "Training batch: 2001 in epoch:8, training batch loss:2.5432\n",
      "Training batch: 2011 in epoch:8, training batch loss:2.6974\n",
      "Training batch: 2021 in epoch:8, training batch loss:2.7641\n",
      "Training batch: 2031 in epoch:8, training batch loss:2.6269\n",
      "Training batch: 2041 in epoch:8, training batch loss:2.6969\n",
      "Training batch: 2051 in epoch:8, training batch loss:2.7798\n",
      "Training batch: 2061 in epoch:8, training batch loss:2.6562\n",
      "Training batch: 2071 in epoch:8, training batch loss:2.7382\n",
      "Training batch: 2081 in epoch:8, training batch loss:2.6502\n",
      "Training batch: 2091 in epoch:8, training batch loss:2.6683\n",
      "Training batch: 2101 in epoch:8, training batch loss:2.6765\n",
      "Training batch: 2111 in epoch:8, training batch loss:2.6382\n",
      "Training batch: 2121 in epoch:8, training batch loss:2.7599\n",
      "Training batch: 2131 in epoch:8, training batch loss:2.7536\n",
      "Training batch: 2141 in epoch:8, training batch loss:2.7099\n",
      "Training batch: 2151 in epoch:8, training batch loss:2.6759\n",
      "Training batch: 2161 in epoch:8, training batch loss:2.8786\n",
      "Training batch: 2171 in epoch:8, training batch loss:2.7057\n",
      "Training batch: 2181 in epoch:8, training batch loss:2.6105\n",
      "Training batch: 2191 in epoch:8, training batch loss:2.7838\n",
      "Training batch: 2201 in epoch:8, training batch loss:2.7358\n",
      "Training batch: 2211 in epoch:8, training batch loss:2.7206\n",
      "Training batch: 2221 in epoch:8, training batch loss:2.7011\n",
      "Training batch: 2231 in epoch:8, training batch loss:2.7020\n",
      "Training batch: 2241 in epoch:8, training batch loss:2.6243\n",
      "Training batch: 2251 in epoch:8, training batch loss:2.7139\n",
      "Training batch: 2261 in epoch:8, training batch loss:2.7098\n",
      "Training batch: 2271 in epoch:8, training batch loss:2.7530\n",
      "Training batch: 2281 in epoch:8, training batch loss:2.6796\n",
      "Training batch: 2291 in epoch:8, training batch loss:2.7806\n",
      "2024-09-03 18:15:55 | epoch: 0008/10, training time: 667.8s, inference time: 70.9s\n",
      "train loss: 17.7499, val_loss: 0.8505\n",
      "val loss decrease from 0.8506 to 0.8505, saving model to GGNN_33_step12_height_type.pkl\n",
      "Training batch: 11 in epoch:9, training batch loss:2.6314\n",
      "Training batch: 21 in epoch:9, training batch loss:2.7151\n",
      "Training batch: 31 in epoch:9, training batch loss:2.6193\n",
      "Training batch: 41 in epoch:9, training batch loss:2.6971\n",
      "Training batch: 51 in epoch:9, training batch loss:2.6842\n",
      "Training batch: 61 in epoch:9, training batch loss:2.6108\n",
      "Training batch: 71 in epoch:9, training batch loss:2.6847\n",
      "Training batch: 81 in epoch:9, training batch loss:2.5429\n",
      "Training batch: 91 in epoch:9, training batch loss:2.6370\n",
      "Training batch: 101 in epoch:9, training batch loss:2.6170\n",
      "Training batch: 111 in epoch:9, training batch loss:2.6279\n",
      "Training batch: 121 in epoch:9, training batch loss:2.5702\n",
      "Training batch: 131 in epoch:9, training batch loss:2.6599\n",
      "Training batch: 141 in epoch:9, training batch loss:2.6684\n",
      "Training batch: 151 in epoch:9, training batch loss:2.6036\n",
      "Training batch: 161 in epoch:9, training batch loss:2.6099\n",
      "Training batch: 171 in epoch:9, training batch loss:2.6092\n",
      "Training batch: 181 in epoch:9, training batch loss:2.6279\n",
      "Training batch: 191 in epoch:9, training batch loss:2.7056\n",
      "Training batch: 201 in epoch:9, training batch loss:2.6848\n",
      "Training batch: 211 in epoch:9, training batch loss:2.5661\n",
      "Training batch: 221 in epoch:9, training batch loss:2.6251\n",
      "Training batch: 231 in epoch:9, training batch loss:2.6126\n",
      "Training batch: 241 in epoch:9, training batch loss:2.6526\n",
      "Training batch: 251 in epoch:9, training batch loss:2.6156\n",
      "Training batch: 261 in epoch:9, training batch loss:2.6266\n",
      "Training batch: 271 in epoch:9, training batch loss:2.6754\n",
      "Training batch: 281 in epoch:9, training batch loss:2.6411\n",
      "Training batch: 291 in epoch:9, training batch loss:2.5632\n",
      "Training batch: 301 in epoch:9, training batch loss:2.5544\n",
      "Training batch: 311 in epoch:9, training batch loss:2.6561\n",
      "Training batch: 321 in epoch:9, training batch loss:2.5913\n",
      "Training batch: 331 in epoch:9, training batch loss:2.6445\n",
      "Training batch: 341 in epoch:9, training batch loss:2.6679\n",
      "Training batch: 351 in epoch:9, training batch loss:2.5021\n",
      "Training batch: 361 in epoch:9, training batch loss:2.7234\n",
      "Training batch: 371 in epoch:9, training batch loss:2.5627\n",
      "Training batch: 381 in epoch:9, training batch loss:2.6338\n",
      "Training batch: 391 in epoch:9, training batch loss:2.5951\n",
      "Training batch: 401 in epoch:9, training batch loss:2.6500\n",
      "Training batch: 411 in epoch:9, training batch loss:2.6327\n",
      "Training batch: 421 in epoch:9, training batch loss:2.5747\n",
      "Training batch: 431 in epoch:9, training batch loss:2.6284\n",
      "Training batch: 441 in epoch:9, training batch loss:2.6290\n",
      "Training batch: 451 in epoch:9, training batch loss:2.6990\n",
      "Training batch: 461 in epoch:9, training batch loss:2.6824\n",
      "Training batch: 471 in epoch:9, training batch loss:2.6608\n",
      "Training batch: 481 in epoch:9, training batch loss:2.6341\n",
      "Training batch: 491 in epoch:9, training batch loss:2.5247\n",
      "Training batch: 501 in epoch:9, training batch loss:2.6699\n",
      "Training batch: 511 in epoch:9, training batch loss:2.5958\n",
      "Training batch: 521 in epoch:9, training batch loss:2.6056\n",
      "Training batch: 531 in epoch:9, training batch loss:2.5830\n",
      "Training batch: 541 in epoch:9, training batch loss:2.6769\n",
      "Training batch: 551 in epoch:9, training batch loss:2.6433\n",
      "Training batch: 561 in epoch:9, training batch loss:2.5893\n",
      "Training batch: 571 in epoch:9, training batch loss:2.6477\n",
      "Training batch: 581 in epoch:9, training batch loss:2.7066\n",
      "Training batch: 591 in epoch:9, training batch loss:2.5839\n",
      "Training batch: 601 in epoch:9, training batch loss:2.5984\n",
      "Training batch: 611 in epoch:9, training batch loss:2.5567\n",
      "Training batch: 621 in epoch:9, training batch loss:2.0182\n",
      "Training batch: 631 in epoch:9, training batch loss:2.7186\n",
      "Training batch: 641 in epoch:9, training batch loss:2.5982\n",
      "Training batch: 651 in epoch:9, training batch loss:2.5842\n",
      "Training batch: 661 in epoch:9, training batch loss:2.5506\n",
      "Training batch: 671 in epoch:9, training batch loss:2.6983\n",
      "Training batch: 681 in epoch:9, training batch loss:2.6785\n",
      "Training batch: 691 in epoch:9, training batch loss:2.5874\n",
      "Training batch: 701 in epoch:9, training batch loss:2.5916\n",
      "Training batch: 711 in epoch:9, training batch loss:2.6503\n",
      "Training batch: 721 in epoch:9, training batch loss:2.6264\n",
      "Training batch: 731 in epoch:9, training batch loss:2.6130\n",
      "Training batch: 741 in epoch:9, training batch loss:2.6314\n",
      "Training batch: 751 in epoch:9, training batch loss:2.6260\n",
      "Training batch: 761 in epoch:9, training batch loss:2.6482\n",
      "Training batch: 771 in epoch:9, training batch loss:2.6131\n",
      "Training batch: 781 in epoch:9, training batch loss:2.6725\n",
      "Training batch: 791 in epoch:9, training batch loss:2.6810\n",
      "Training batch: 801 in epoch:9, training batch loss:2.5628\n",
      "Training batch: 811 in epoch:9, training batch loss:2.6395\n",
      "Training batch: 821 in epoch:9, training batch loss:2.7268\n",
      "Training batch: 831 in epoch:9, training batch loss:2.5488\n",
      "Training batch: 841 in epoch:9, training batch loss:2.6603\n",
      "Training batch: 851 in epoch:9, training batch loss:2.5950\n",
      "Training batch: 861 in epoch:9, training batch loss:2.6485\n",
      "Training batch: 871 in epoch:9, training batch loss:2.6330\n",
      "Training batch: 881 in epoch:9, training batch loss:2.6630\n",
      "Training batch: 891 in epoch:9, training batch loss:2.5554\n",
      "Training batch: 901 in epoch:9, training batch loss:2.7480\n",
      "Training batch: 911 in epoch:9, training batch loss:2.6554\n",
      "Training batch: 921 in epoch:9, training batch loss:2.5726\n",
      "Training batch: 931 in epoch:9, training batch loss:2.5909\n",
      "Training batch: 941 in epoch:9, training batch loss:2.6753\n",
      "Training batch: 951 in epoch:9, training batch loss:2.7072\n",
      "Training batch: 961 in epoch:9, training batch loss:2.5363\n",
      "Training batch: 971 in epoch:9, training batch loss:2.5788\n",
      "Training batch: 981 in epoch:9, training batch loss:2.5518\n",
      "Training batch: 991 in epoch:9, training batch loss:2.6658\n",
      "Training batch: 1001 in epoch:9, training batch loss:2.5986\n",
      "Training batch: 1011 in epoch:9, training batch loss:2.5965\n",
      "Training batch: 1021 in epoch:9, training batch loss:2.5791\n",
      "Training batch: 1031 in epoch:9, training batch loss:2.6655\n",
      "Training batch: 1041 in epoch:9, training batch loss:2.7379\n",
      "Training batch: 1051 in epoch:9, training batch loss:2.0072\n",
      "Training batch: 1061 in epoch:9, training batch loss:2.6536\n",
      "Training batch: 1071 in epoch:9, training batch loss:2.6703\n",
      "Training batch: 1081 in epoch:9, training batch loss:2.7606\n",
      "Training batch: 1091 in epoch:9, training batch loss:2.6088\n",
      "Training batch: 1101 in epoch:9, training batch loss:2.6307\n",
      "Training batch: 1111 in epoch:9, training batch loss:2.6691\n",
      "Training batch: 1121 in epoch:9, training batch loss:2.6465\n",
      "Training batch: 1131 in epoch:9, training batch loss:2.5694\n",
      "Training batch: 1141 in epoch:9, training batch loss:2.6140\n",
      "Training batch: 1151 in epoch:9, training batch loss:2.6565\n",
      "Training batch: 1161 in epoch:9, training batch loss:2.6903\n",
      "Training batch: 1171 in epoch:9, training batch loss:2.6307\n",
      "Training batch: 1181 in epoch:9, training batch loss:2.5660\n",
      "Training batch: 1191 in epoch:9, training batch loss:2.6864\n",
      "Training batch: 1201 in epoch:9, training batch loss:2.6060\n",
      "Training batch: 1211 in epoch:9, training batch loss:2.6339\n",
      "Training batch: 1221 in epoch:9, training batch loss:2.5315\n",
      "Training batch: 1231 in epoch:9, training batch loss:2.5352\n",
      "Training batch: 1241 in epoch:9, training batch loss:2.6407\n",
      "Training batch: 1251 in epoch:9, training batch loss:2.7525\n",
      "Training batch: 1261 in epoch:9, training batch loss:2.6840\n",
      "Training batch: 1271 in epoch:9, training batch loss:2.6301\n",
      "Training batch: 1281 in epoch:9, training batch loss:2.6386\n",
      "Training batch: 1291 in epoch:9, training batch loss:2.5752\n",
      "Training batch: 1301 in epoch:9, training batch loss:2.6541\n",
      "Training batch: 1311 in epoch:9, training batch loss:2.6104\n",
      "Training batch: 1321 in epoch:9, training batch loss:2.4684\n",
      "Training batch: 1331 in epoch:9, training batch loss:2.6197\n",
      "Training batch: 1341 in epoch:9, training batch loss:2.6544\n",
      "Training batch: 1351 in epoch:9, training batch loss:2.6758\n",
      "Training batch: 1361 in epoch:9, training batch loss:2.6606\n",
      "Training batch: 1371 in epoch:9, training batch loss:2.5465\n",
      "Training batch: 1381 in epoch:9, training batch loss:2.6235\n",
      "Training batch: 1391 in epoch:9, training batch loss:2.6937\n",
      "Training batch: 1401 in epoch:9, training batch loss:2.6393\n",
      "Training batch: 1411 in epoch:9, training batch loss:2.5170\n",
      "Training batch: 1421 in epoch:9, training batch loss:2.5676\n",
      "Training batch: 1431 in epoch:9, training batch loss:2.6857\n",
      "Training batch: 1441 in epoch:9, training batch loss:2.6121\n",
      "Training batch: 1451 in epoch:9, training batch loss:2.6041\n",
      "Training batch: 1461 in epoch:9, training batch loss:2.6568\n",
      "Training batch: 1471 in epoch:9, training batch loss:2.6620\n",
      "Training batch: 1481 in epoch:9, training batch loss:2.6344\n",
      "Training batch: 1491 in epoch:9, training batch loss:2.6964\n",
      "Training batch: 1501 in epoch:9, training batch loss:2.6381\n",
      "Training batch: 1511 in epoch:9, training batch loss:2.5646\n",
      "Training batch: 1521 in epoch:9, training batch loss:2.6152\n",
      "Training batch: 1531 in epoch:9, training batch loss:2.5820\n",
      "Training batch: 1541 in epoch:9, training batch loss:2.6549\n",
      "Training batch: 1551 in epoch:9, training batch loss:2.6211\n",
      "Training batch: 1561 in epoch:9, training batch loss:2.6130\n",
      "Training batch: 1571 in epoch:9, training batch loss:2.6577\n",
      "Training batch: 1581 in epoch:9, training batch loss:2.7030\n",
      "Training batch: 1591 in epoch:9, training batch loss:2.5470\n",
      "Training batch: 1601 in epoch:9, training batch loss:2.5843\n",
      "Training batch: 1611 in epoch:9, training batch loss:2.6420\n",
      "Training batch: 1621 in epoch:9, training batch loss:2.5977\n",
      "Training batch: 1631 in epoch:9, training batch loss:2.6611\n",
      "Training batch: 1641 in epoch:9, training batch loss:2.6530\n",
      "Training batch: 1651 in epoch:9, training batch loss:2.5695\n",
      "Training batch: 1661 in epoch:9, training batch loss:2.6969\n",
      "Training batch: 1671 in epoch:9, training batch loss:2.7056\n",
      "Training batch: 1681 in epoch:9, training batch loss:2.6583\n",
      "Training batch: 1691 in epoch:9, training batch loss:2.6199\n",
      "Training batch: 1701 in epoch:9, training batch loss:2.6884\n",
      "Training batch: 1711 in epoch:9, training batch loss:2.6150\n",
      "Training batch: 1721 in epoch:9, training batch loss:2.7363\n",
      "Training batch: 1731 in epoch:9, training batch loss:2.5714\n",
      "Training batch: 1741 in epoch:9, training batch loss:2.7026\n",
      "Training batch: 1751 in epoch:9, training batch loss:2.7045\n",
      "Training batch: 1761 in epoch:9, training batch loss:2.6494\n",
      "Training batch: 1771 in epoch:9, training batch loss:2.5936\n",
      "Training batch: 1781 in epoch:9, training batch loss:2.6967\n",
      "Training batch: 1791 in epoch:9, training batch loss:2.6872\n",
      "Training batch: 1801 in epoch:9, training batch loss:2.6693\n",
      "Training batch: 1811 in epoch:9, training batch loss:2.6142\n",
      "Training batch: 1821 in epoch:9, training batch loss:2.6901\n",
      "Training batch: 1831 in epoch:9, training batch loss:2.7827\n",
      "Training batch: 1841 in epoch:9, training batch loss:2.6395\n",
      "Training batch: 1851 in epoch:9, training batch loss:2.6485\n",
      "Training batch: 1861 in epoch:9, training batch loss:2.6831\n",
      "Training batch: 1871 in epoch:9, training batch loss:2.6829\n",
      "Training batch: 1881 in epoch:9, training batch loss:2.6508\n",
      "Training batch: 1891 in epoch:9, training batch loss:2.6014\n",
      "Training batch: 1901 in epoch:9, training batch loss:2.7290\n",
      "Training batch: 1911 in epoch:9, training batch loss:2.7002\n",
      "Training batch: 1921 in epoch:9, training batch loss:2.6397\n",
      "Training batch: 1931 in epoch:9, training batch loss:2.7104\n",
      "Training batch: 1941 in epoch:9, training batch loss:2.6404\n",
      "Training batch: 1951 in epoch:9, training batch loss:2.6547\n",
      "Training batch: 1961 in epoch:9, training batch loss:2.6271\n",
      "Training batch: 1971 in epoch:9, training batch loss:2.7052\n",
      "Training batch: 1981 in epoch:9, training batch loss:2.5606\n",
      "Training batch: 1991 in epoch:9, training batch loss:2.7149\n",
      "Training batch: 2001 in epoch:9, training batch loss:2.5848\n",
      "Training batch: 2011 in epoch:9, training batch loss:2.6690\n",
      "Training batch: 2021 in epoch:9, training batch loss:2.6492\n",
      "Training batch: 2031 in epoch:9, training batch loss:2.7016\n",
      "Training batch: 2041 in epoch:9, training batch loss:2.5377\n",
      "Training batch: 2051 in epoch:9, training batch loss:2.6446\n",
      "Training batch: 2061 in epoch:9, training batch loss:2.6719\n",
      "Training batch: 2071 in epoch:9, training batch loss:2.6719\n",
      "Training batch: 2081 in epoch:9, training batch loss:2.6968\n",
      "Training batch: 2091 in epoch:9, training batch loss:2.6683\n",
      "Training batch: 2101 in epoch:9, training batch loss:2.5350\n",
      "Training batch: 2111 in epoch:9, training batch loss:2.7176\n",
      "Training batch: 2121 in epoch:9, training batch loss:2.6312\n",
      "Training batch: 2131 in epoch:9, training batch loss:2.5970\n",
      "Training batch: 2141 in epoch:9, training batch loss:2.5751\n",
      "Training batch: 2151 in epoch:9, training batch loss:2.6662\n",
      "Training batch: 2161 in epoch:9, training batch loss:2.6193\n",
      "Training batch: 2171 in epoch:9, training batch loss:2.6622\n",
      "Training batch: 2181 in epoch:9, training batch loss:2.6240\n",
      "Training batch: 2191 in epoch:9, training batch loss:2.6141\n",
      "Training batch: 2201 in epoch:9, training batch loss:2.6224\n",
      "Training batch: 2211 in epoch:9, training batch loss:2.6990\n",
      "Training batch: 2221 in epoch:9, training batch loss:2.6965\n",
      "Training batch: 2231 in epoch:9, training batch loss:2.6356\n",
      "Training batch: 2241 in epoch:9, training batch loss:2.5037\n",
      "Training batch: 2251 in epoch:9, training batch loss:2.6722\n",
      "Training batch: 2261 in epoch:9, training batch loss:2.7511\n",
      "Training batch: 2271 in epoch:9, training batch loss:2.7077\n",
      "Training batch: 2281 in epoch:9, training batch loss:2.6271\n",
      "Training batch: 2291 in epoch:9, training batch loss:2.7021\n",
      "2024-09-03 18:28:08 | epoch: 0009/10, training time: 662.7s, inference time: 70.5s\n",
      "train loss: 17.1339, val_loss: 0.8508\n",
      "Training batch: 11 in epoch:10, training batch loss:2.5794\n",
      "Training batch: 21 in epoch:10, training batch loss:2.5578\n",
      "Training batch: 31 in epoch:10, training batch loss:2.5146\n",
      "Training batch: 41 in epoch:10, training batch loss:2.4945\n",
      "Training batch: 51 in epoch:10, training batch loss:2.4954\n",
      "Training batch: 61 in epoch:10, training batch loss:2.5828\n",
      "Training batch: 71 in epoch:10, training batch loss:2.5390\n",
      "Training batch: 81 in epoch:10, training batch loss:2.5237\n",
      "Training batch: 91 in epoch:10, training batch loss:2.5941\n",
      "Training batch: 101 in epoch:10, training batch loss:2.5042\n",
      "Training batch: 111 in epoch:10, training batch loss:2.4802\n",
      "Training batch: 121 in epoch:10, training batch loss:2.5503\n",
      "Training batch: 131 in epoch:10, training batch loss:2.5272\n",
      "Training batch: 141 in epoch:10, training batch loss:2.5722\n",
      "Training batch: 151 in epoch:10, training batch loss:2.5857\n",
      "Training batch: 161 in epoch:10, training batch loss:2.5348\n",
      "Training batch: 171 in epoch:10, training batch loss:2.4367\n",
      "Training batch: 181 in epoch:10, training batch loss:2.4979\n",
      "Training batch: 191 in epoch:10, training batch loss:2.5847\n",
      "Training batch: 201 in epoch:10, training batch loss:2.4756\n",
      "Training batch: 211 in epoch:10, training batch loss:2.5679\n",
      "Training batch: 221 in epoch:10, training batch loss:2.5652\n",
      "Training batch: 231 in epoch:10, training batch loss:2.5501\n",
      "Training batch: 241 in epoch:10, training batch loss:2.5458\n",
      "Training batch: 251 in epoch:10, training batch loss:2.5119\n",
      "Training batch: 261 in epoch:10, training batch loss:2.4827\n",
      "Training batch: 271 in epoch:10, training batch loss:2.5929\n",
      "Training batch: 281 in epoch:10, training batch loss:2.5529\n",
      "Training batch: 291 in epoch:10, training batch loss:2.5269\n",
      "Training batch: 301 in epoch:10, training batch loss:2.5711\n",
      "Training batch: 311 in epoch:10, training batch loss:2.5087\n",
      "Training batch: 321 in epoch:10, training batch loss:2.4887\n",
      "Training batch: 331 in epoch:10, training batch loss:2.5464\n",
      "Training batch: 341 in epoch:10, training batch loss:2.4993\n",
      "Training batch: 351 in epoch:10, training batch loss:2.5145\n",
      "Training batch: 361 in epoch:10, training batch loss:2.5116\n",
      "Training batch: 371 in epoch:10, training batch loss:2.5443\n",
      "Training batch: 381 in epoch:10, training batch loss:2.5463\n",
      "Training batch: 391 in epoch:10, training batch loss:2.6027\n",
      "Training batch: 401 in epoch:10, training batch loss:2.4691\n",
      "Training batch: 411 in epoch:10, training batch loss:2.5582\n",
      "Training batch: 421 in epoch:10, training batch loss:2.5317\n",
      "Training batch: 431 in epoch:10, training batch loss:2.5648\n",
      "Training batch: 441 in epoch:10, training batch loss:2.5100\n",
      "Training batch: 451 in epoch:10, training batch loss:2.5952\n",
      "Training batch: 461 in epoch:10, training batch loss:2.4918\n",
      "Training batch: 471 in epoch:10, training batch loss:2.5179\n",
      "Training batch: 481 in epoch:10, training batch loss:2.5091\n",
      "Training batch: 491 in epoch:10, training batch loss:2.5618\n",
      "Training batch: 501 in epoch:10, training batch loss:2.5743\n",
      "Training batch: 511 in epoch:10, training batch loss:2.5342\n",
      "Training batch: 521 in epoch:10, training batch loss:2.0011\n",
      "Training batch: 531 in epoch:10, training batch loss:2.5008\n",
      "Training batch: 541 in epoch:10, training batch loss:2.5508\n",
      "Training batch: 551 in epoch:10, training batch loss:2.5335\n",
      "Training batch: 561 in epoch:10, training batch loss:2.4834\n",
      "Training batch: 571 in epoch:10, training batch loss:2.6129\n",
      "Training batch: 581 in epoch:10, training batch loss:2.5729\n",
      "Training batch: 591 in epoch:10, training batch loss:2.5242\n",
      "Training batch: 601 in epoch:10, training batch loss:2.4669\n",
      "Training batch: 611 in epoch:10, training batch loss:2.5030\n",
      "Training batch: 621 in epoch:10, training batch loss:2.4619\n",
      "Training batch: 631 in epoch:10, training batch loss:2.5013\n",
      "Training batch: 641 in epoch:10, training batch loss:2.5341\n",
      "Training batch: 651 in epoch:10, training batch loss:2.4795\n",
      "Training batch: 661 in epoch:10, training batch loss:2.5005\n",
      "Training batch: 671 in epoch:10, training batch loss:2.5007\n",
      "Training batch: 681 in epoch:10, training batch loss:2.5147\n",
      "Training batch: 691 in epoch:10, training batch loss:2.5469\n",
      "Training batch: 701 in epoch:10, training batch loss:2.5263\n",
      "Training batch: 711 in epoch:10, training batch loss:2.5272\n",
      "Training batch: 721 in epoch:10, training batch loss:2.4802\n",
      "Training batch: 731 in epoch:10, training batch loss:2.6379\n",
      "Training batch: 741 in epoch:10, training batch loss:2.5227\n",
      "Training batch: 751 in epoch:10, training batch loss:2.4518\n",
      "Training batch: 761 in epoch:10, training batch loss:2.5654\n",
      "Training batch: 771 in epoch:10, training batch loss:2.5603\n",
      "Training batch: 781 in epoch:10, training batch loss:2.5750\n",
      "Training batch: 791 in epoch:10, training batch loss:2.5437\n",
      "Training batch: 801 in epoch:10, training batch loss:2.4827\n",
      "Training batch: 811 in epoch:10, training batch loss:2.5943\n",
      "Training batch: 821 in epoch:10, training batch loss:2.5596\n",
      "Training batch: 831 in epoch:10, training batch loss:2.4831\n",
      "Training batch: 841 in epoch:10, training batch loss:2.4464\n",
      "Training batch: 851 in epoch:10, training batch loss:2.6220\n",
      "Training batch: 861 in epoch:10, training batch loss:2.5259\n",
      "Training batch: 871 in epoch:10, training batch loss:2.5682\n",
      "Training batch: 881 in epoch:10, training batch loss:2.5382\n",
      "Training batch: 891 in epoch:10, training batch loss:2.5439\n",
      "Training batch: 901 in epoch:10, training batch loss:2.6023\n",
      "Training batch: 911 in epoch:10, training batch loss:2.5476\n",
      "Training batch: 921 in epoch:10, training batch loss:2.4957\n",
      "Training batch: 931 in epoch:10, training batch loss:2.5108\n",
      "Training batch: 941 in epoch:10, training batch loss:2.5947\n",
      "Training batch: 951 in epoch:10, training batch loss:2.4994\n",
      "Training batch: 961 in epoch:10, training batch loss:2.5322\n",
      "Training batch: 971 in epoch:10, training batch loss:2.5305\n",
      "Training batch: 981 in epoch:10, training batch loss:2.5402\n",
      "Training batch: 991 in epoch:10, training batch loss:2.5362\n",
      "Training batch: 1001 in epoch:10, training batch loss:2.5620\n",
      "Training batch: 1011 in epoch:10, training batch loss:2.5797\n",
      "Training batch: 1021 in epoch:10, training batch loss:2.4843\n",
      "Training batch: 1031 in epoch:10, training batch loss:2.5305\n",
      "Training batch: 1041 in epoch:10, training batch loss:2.5288\n",
      "Training batch: 1051 in epoch:10, training batch loss:2.4812\n",
      "Training batch: 1061 in epoch:10, training batch loss:2.4989\n",
      "Training batch: 1071 in epoch:10, training batch loss:2.5445\n",
      "Training batch: 1081 in epoch:10, training batch loss:2.5490\n",
      "Training batch: 1091 in epoch:10, training batch loss:2.5761\n",
      "Training batch: 1101 in epoch:10, training batch loss:2.5237\n",
      "Training batch: 1111 in epoch:10, training batch loss:2.4944\n",
      "Training batch: 1121 in epoch:10, training batch loss:2.5846\n",
      "Training batch: 1131 in epoch:10, training batch loss:2.5490\n",
      "Training batch: 1141 in epoch:10, training batch loss:2.4233\n",
      "Training batch: 1151 in epoch:10, training batch loss:2.5123\n",
      "Training batch: 1161 in epoch:10, training batch loss:2.5224\n",
      "Training batch: 1171 in epoch:10, training batch loss:2.5712\n",
      "Training batch: 1181 in epoch:10, training batch loss:2.5687\n",
      "Training batch: 1191 in epoch:10, training batch loss:2.4328\n",
      "Training batch: 1201 in epoch:10, training batch loss:2.5699\n",
      "Training batch: 1211 in epoch:10, training batch loss:2.6256\n",
      "Training batch: 1221 in epoch:10, training batch loss:2.5049\n",
      "Training batch: 1231 in epoch:10, training batch loss:2.4926\n",
      "Training batch: 1241 in epoch:10, training batch loss:2.5363\n",
      "Training batch: 1251 in epoch:10, training batch loss:2.6177\n",
      "Training batch: 1261 in epoch:10, training batch loss:2.4900\n",
      "Training batch: 1271 in epoch:10, training batch loss:2.5465\n",
      "Training batch: 1281 in epoch:10, training batch loss:2.4898\n",
      "Training batch: 1291 in epoch:10, training batch loss:2.5972\n",
      "Training batch: 1301 in epoch:10, training batch loss:2.5882\n",
      "Training batch: 1311 in epoch:10, training batch loss:2.5189\n",
      "Training batch: 1321 in epoch:10, training batch loss:2.4949\n",
      "Training batch: 1331 in epoch:10, training batch loss:2.5710\n",
      "Training batch: 1341 in epoch:10, training batch loss:2.5246\n",
      "Training batch: 1351 in epoch:10, training batch loss:2.5232\n",
      "Training batch: 1361 in epoch:10, training batch loss:2.5413\n",
      "Training batch: 1371 in epoch:10, training batch loss:2.0296\n",
      "Training batch: 1381 in epoch:10, training batch loss:2.5291\n",
      "Training batch: 1391 in epoch:10, training batch loss:2.6103\n",
      "Training batch: 1401 in epoch:10, training batch loss:2.5390\n",
      "Training batch: 1411 in epoch:10, training batch loss:2.4957\n",
      "Training batch: 1421 in epoch:10, training batch loss:2.5178\n",
      "Training batch: 1431 in epoch:10, training batch loss:2.5804\n",
      "Training batch: 1441 in epoch:10, training batch loss:2.4596\n",
      "Training batch: 1451 in epoch:10, training batch loss:2.5081\n",
      "Training batch: 1461 in epoch:10, training batch loss:2.6313\n",
      "Training batch: 1471 in epoch:10, training batch loss:2.4690\n",
      "Training batch: 1481 in epoch:10, training batch loss:2.5763\n",
      "Training batch: 1491 in epoch:10, training batch loss:2.5868\n",
      "Training batch: 1501 in epoch:10, training batch loss:2.5909\n",
      "Training batch: 1511 in epoch:10, training batch loss:2.4983\n",
      "Training batch: 1521 in epoch:10, training batch loss:2.6039\n",
      "Training batch: 1531 in epoch:10, training batch loss:2.6165\n",
      "Training batch: 1541 in epoch:10, training batch loss:2.6033\n",
      "Training batch: 1551 in epoch:10, training batch loss:2.5533\n",
      "Training batch: 1561 in epoch:10, training batch loss:2.5257\n",
      "Training batch: 1571 in epoch:10, training batch loss:2.5698\n",
      "Training batch: 1581 in epoch:10, training batch loss:2.5880\n",
      "Training batch: 1591 in epoch:10, training batch loss:2.5028\n",
      "Training batch: 1601 in epoch:10, training batch loss:2.5146\n",
      "Training batch: 1611 in epoch:10, training batch loss:2.5523\n",
      "Training batch: 1621 in epoch:10, training batch loss:2.5598\n",
      "Training batch: 1631 in epoch:10, training batch loss:2.5265\n",
      "Training batch: 1641 in epoch:10, training batch loss:2.6043\n",
      "Training batch: 1651 in epoch:10, training batch loss:2.5163\n",
      "Training batch: 1661 in epoch:10, training batch loss:2.5587\n",
      "Training batch: 1671 in epoch:10, training batch loss:2.5843\n",
      "Training batch: 1681 in epoch:10, training batch loss:2.5517\n",
      "Training batch: 1691 in epoch:10, training batch loss:2.4897\n",
      "Training batch: 1701 in epoch:10, training batch loss:2.5867\n",
      "Training batch: 1711 in epoch:10, training batch loss:2.4903\n",
      "Training batch: 1721 in epoch:10, training batch loss:2.5277\n",
      "Training batch: 1731 in epoch:10, training batch loss:2.5095\n",
      "Training batch: 1741 in epoch:10, training batch loss:2.5328\n",
      "Training batch: 1751 in epoch:10, training batch loss:2.5678\n",
      "Training batch: 1761 in epoch:10, training batch loss:2.6041\n",
      "Training batch: 1771 in epoch:10, training batch loss:2.5003\n",
      "Training batch: 1781 in epoch:10, training batch loss:2.5607\n",
      "Training batch: 1791 in epoch:10, training batch loss:2.5313\n",
      "Training batch: 1801 in epoch:10, training batch loss:2.5329\n",
      "Training batch: 1811 in epoch:10, training batch loss:2.6081\n",
      "Training batch: 1821 in epoch:10, training batch loss:2.5281\n",
      "Training batch: 1831 in epoch:10, training batch loss:2.4783\n",
      "Training batch: 1841 in epoch:10, training batch loss:2.5675\n",
      "Training batch: 1851 in epoch:10, training batch loss:2.5871\n",
      "Training batch: 1861 in epoch:10, training batch loss:2.5948\n",
      "Training batch: 1871 in epoch:10, training batch loss:2.5223\n",
      "Training batch: 1881 in epoch:10, training batch loss:2.5844\n",
      "Training batch: 1891 in epoch:10, training batch loss:2.5679\n",
      "Training batch: 1901 in epoch:10, training batch loss:2.5392\n",
      "Training batch: 1911 in epoch:10, training batch loss:2.4994\n",
      "Training batch: 1921 in epoch:10, training batch loss:2.5428\n",
      "Training batch: 1931 in epoch:10, training batch loss:2.6373\n",
      "Training batch: 1941 in epoch:10, training batch loss:2.6113\n",
      "Training batch: 1951 in epoch:10, training batch loss:2.4517\n",
      "Training batch: 1961 in epoch:10, training batch loss:2.5084\n",
      "Training batch: 1971 in epoch:10, training batch loss:2.6368\n",
      "Training batch: 1981 in epoch:10, training batch loss:2.5749\n",
      "Training batch: 1991 in epoch:10, training batch loss:2.4815\n",
      "Training batch: 2001 in epoch:10, training batch loss:2.4977\n",
      "Training batch: 2011 in epoch:10, training batch loss:2.5145\n",
      "Training batch: 2021 in epoch:10, training batch loss:2.6018\n",
      "Training batch: 2031 in epoch:10, training batch loss:2.5530\n",
      "Training batch: 2041 in epoch:10, training batch loss:2.5792\n",
      "Training batch: 2051 in epoch:10, training batch loss:2.5370\n",
      "Training batch: 2061 in epoch:10, training batch loss:2.5407\n",
      "Training batch: 2071 in epoch:10, training batch loss:2.5223\n",
      "Training batch: 2081 in epoch:10, training batch loss:2.5398\n",
      "Training batch: 2091 in epoch:10, training batch loss:2.5498\n",
      "Training batch: 2101 in epoch:10, training batch loss:2.5324\n",
      "Training batch: 2111 in epoch:10, training batch loss:2.5655\n",
      "Training batch: 2121 in epoch:10, training batch loss:2.5743\n",
      "Training batch: 2131 in epoch:10, training batch loss:2.5122\n",
      "Training batch: 2141 in epoch:10, training batch loss:2.5325\n",
      "Training batch: 2151 in epoch:10, training batch loss:2.5102\n",
      "Training batch: 2161 in epoch:10, training batch loss:2.5714\n",
      "Training batch: 2171 in epoch:10, training batch loss:2.5732\n",
      "Training batch: 2181 in epoch:10, training batch loss:2.5820\n",
      "Training batch: 2191 in epoch:10, training batch loss:2.4769\n",
      "Training batch: 2201 in epoch:10, training batch loss:2.5713\n",
      "Training batch: 2211 in epoch:10, training batch loss:2.5611\n",
      "Training batch: 2221 in epoch:10, training batch loss:2.5473\n",
      "Training batch: 2231 in epoch:10, training batch loss:2.5190\n",
      "Training batch: 2241 in epoch:10, training batch loss:2.5573\n",
      "Training batch: 2251 in epoch:10, training batch loss:2.5649\n",
      "Training batch: 2261 in epoch:10, training batch loss:2.5954\n",
      "Training batch: 2271 in epoch:10, training batch loss:2.5768\n",
      "Training batch: 2281 in epoch:10, training batch loss:2.5253\n",
      "Training batch: 2291 in epoch:10, training batch loss:2.5633\n",
      "2024-09-03 18:40:44 | epoch: 0010/10, training time: 685.3s, inference time: 70.5s\n",
      "train loss: 16.5878, val_loss: 0.8508\n",
      "Training and validation are completed, and model has been stored as GGNN_33_step12_height_type.pkl\n",
      "**** testing model ****\n",
      "loading model from GGNN_33_step12_height_type.pkl\n",
      "model restored!\n",
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AppData\\Local\\Temp\\ipykernel_34960\\1302657811.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masked_tensor1 = torch.tensor(masked_tensor1, dtype=torch.float32)\n",
      "C:\\AppData\\Local\\Temp\\ipykernel_34960\\1302657811.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masked_tensor2 = torch.tensor(masked_tensor2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing time: 36.1s\n",
      "total time: 124.7min\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "\n",
    "    loss_train, loss_val = train(args, std_head, mean_head, model, model_file, log, loss_criterion, optimizer, scheduler)\n",
    "\n",
    "    (testPred_1, testPred_2, testPred_3, testPred_4, testPred_5, testPred_6, testPred_7, testPred_8, testPred_9, testPred_10, testPred_11, testPred_12, \n",
    "    testLabel_1, testLabel_2, testLabel_3, testLabel_4, testLabel_5, testLabel_6, testLabel_7, testLabel_8, testLabel_9, testLabel_10, testLabel_11, testLabel_12, \n",
    "    mae_list, mape_list) = test(args, std_head, mean_head, model_file, log)\n",
    "\n",
    "    end = time.time()\n",
    "    log_string(log, 'total time: %.1fmin' % ((end - start) / 60))\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 forecasting steps\n",
    "save_test_cpu = testPred_1.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"1_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_2.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"2_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_3.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"3_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_4.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"4_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_5.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"5_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_6.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"6_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_7.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"7_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_8.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"8_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_9.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"9_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_10.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"10_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_11.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"11_test_height_type.csv\", index=False)\n",
    "\n",
    "save_test_cpu = testPred_12.cpu() \n",
    "save_test_numpy = save_test_cpu.numpy()\n",
    "save_test_DF = pd.DataFrame(save_test_numpy)\n",
    "save_test_DF = save_test_DF.iloc[:-12, : ]\n",
    "save_test_DF.to_csv(\"12_test_height_type.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label output\n",
    "save_label_cpu = testLabel_1.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"1_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_2.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"2_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_3.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"3_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_4.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"4_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_5.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"5_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_6.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"6_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_7.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"7_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_8.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"8_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_9.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"9_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_10.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"10_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_11.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"11_label_height_type.csv\", index=False)\n",
    "\n",
    "save_label_cpu = testLabel_12.cpu() \n",
    "save_label_numpy = save_label_cpu.numpy()\n",
    "save_label_DF = pd.DataFrame(save_label_numpy)\n",
    "save_label_DF = save_label_DF.iloc[:-12, :]\n",
    "save_label_DF.to_csv(\"12_label_height_type.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae_list, mape_list\n",
    "mae_list = pd.DataFrame(mae_list)\n",
    "mae_list.to_csv(\"Time_mae_list_height_type.csv\", index=False)\n",
    "\n",
    "mape_list = pd.DataFrame(mape_list)\n",
    "mape_list.to_csv(\"Time_mape_list_height_type.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
